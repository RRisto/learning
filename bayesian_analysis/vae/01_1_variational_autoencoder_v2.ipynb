{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e40736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "\n",
    "import data\n",
    "import flow\n",
    "import argparse\n",
    "import pathlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f1bea7",
   "metadata": {},
   "source": [
    "## Example from here https://jaan.io/what-is-variational-autoencoder-vae-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7692434",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dec2b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    \"\"\"Variational autoencoder, parameterized by a generative network.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_size, data_size):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"p_z_loc\", torch.zeros(latent_size))\n",
    "        self.register_buffer(\"p_z_scale\", torch.ones(latent_size))\n",
    "        self.log_p_z = NormalLogProb()\n",
    "        self.log_p_x = BernoulliLogProb()\n",
    "        self.generative_network = NeuralNetwork(\n",
    "            input_size=latent_size, output_size=data_size, hidden_size=latent_size * 2\n",
    "        )\n",
    "\n",
    "    def forward(self, z, x):\n",
    "        \"\"\"Return log probability of model.\"\"\"\n",
    "        log_p_z = self.log_p_z(self.p_z_loc, self.p_z_scale, z).sum(-1, keepdim=True)\n",
    "        logits = self.generative_network(z)\n",
    "        # unsqueeze sample dimension\n",
    "        logits, x = torch.broadcast_tensors(logits, x.unsqueeze(1))\n",
    "        #basically reconstruction loss?\n",
    "        log_p_x = self.log_p_x(logits, x).sum(-1, keepdim=True)\n",
    "        return log_p_z + log_p_x\n",
    "\n",
    "    \n",
    "class VariationalMeanField(nn.Module):\n",
    "    \"\"\"Approximate posterior parameterized by an inference network.\"\"\"\n",
    "\n",
    "    def __init__(self, latent_size, data_size):\n",
    "        super().__init__()\n",
    "        self.inference_network = NeuralNetwork(\n",
    "            input_size=data_size,\n",
    "            output_size=latent_size * 2,\n",
    "            hidden_size=latent_size * 2,\n",
    "        )\n",
    "        self.log_q_z = NormalLogProb()\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "    def forward(self, x, n_samples=1):\n",
    "        \"\"\"Return sample of latent variable and log prob.\"\"\"\n",
    "        loc, scale_arg = torch.chunk(\n",
    "            self.inference_network(x).unsqueeze(1), chunks=2, dim=-1\n",
    "        )\n",
    "        scale = self.softplus(scale_arg)\n",
    "        eps = torch.randn((loc.shape[0], n_samples, loc.shape[-1]), device=loc.device)\n",
    "        z = loc + scale * eps  # reparameterization\n",
    "        log_q_z = self.log_q_z(loc, scale, z).sum(-1, keepdim=True)\n",
    "        return z, log_q_z\n",
    "\n",
    "    \n",
    "class VariationalFlow(nn.Module):\n",
    "    \"\"\"Approximate posterior parameterized by a flow (https://arxiv.org/abs/1606.04934).\"\"\"\n",
    "\n",
    "    def __init__(self, latent_size, data_size, flow_depth):\n",
    "        super().__init__()\n",
    "        hidden_size = latent_size * 2\n",
    "        self.inference_network = NeuralNetwork(\n",
    "            input_size=data_size,\n",
    "            # loc, scale, and context\n",
    "            output_size=latent_size * 3,\n",
    "            hidden_size=hidden_size,\n",
    "        )\n",
    "        modules = []\n",
    "        for _ in range(flow_depth):\n",
    "            modules.append(\n",
    "                flow.InverseAutoregressiveFlow(\n",
    "                    num_input=latent_size,\n",
    "                    num_hidden=hidden_size,\n",
    "                    num_context=latent_size,\n",
    "                )\n",
    "            )\n",
    "            modules.append(flow.Reverse(latent_size))\n",
    "        self.q_z_flow = flow.FlowSequential(*modules)\n",
    "        self.log_q_z_0 = NormalLogProb()\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self, x, n_samples=1):\n",
    "        \"\"\"Return sample of latent variable and log prob.\"\"\"\n",
    "        loc, scale_arg, h = torch.chunk(\n",
    "            self.inference_network(x).unsqueeze(1), chunks=3, dim=-1\n",
    "        )\n",
    "        scale = self.softplus(scale_arg)\n",
    "        eps = torch.randn((loc.shape[0], n_samples, loc.shape[-1]), device=loc.device)\n",
    "        z_0 = loc + scale * eps  # reparameterization\n",
    "        log_q_z_0 = self.log_q_z_0(loc, scale, z_0)\n",
    "        z_T, log_q_z_flow = self.q_z_flow(z_0, context=h)\n",
    "        log_q_z = (log_q_z_0 + log_q_z_flow).sum(-1, keepdim=True)\n",
    "        return z_T, log_q_z\n",
    "    \n",
    "    \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size):\n",
    "        super().__init__()\n",
    "        modules = [\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size),\n",
    "        ]\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.net(input)\n",
    "\n",
    "\n",
    "class NormalLogProb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, loc, scale, z):\n",
    "        #calculates probability that samples z are taken from normal distribution with mean \n",
    "        #loc and variance (str) var\n",
    "        var = torch.pow(scale, 2)\n",
    "        return -0.5 * torch.log(2 * np.pi * var) - torch.pow(z - loc, 2) / (2 * var)\n",
    "\n",
    "\n",
    "class BernoulliLogProb(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.bce_with_logits = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        # bernoulli log prob is equivalent to negative binary cross entropy\n",
    "        return -self.bce_with_logits(logits, target)\n",
    "\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "            \n",
    "@torch.no_grad()\n",
    "def evaluate(n_samples, model, variational, eval_data):\n",
    "    model.eval()\n",
    "    total_log_p_x = 0.0\n",
    "    total_elbo = 0.0\n",
    "    for batch in eval_data:\n",
    "        x = batch[0].to(next(model.parameters()).device)\n",
    "        z, log_q_z = variational(x, n_samples)\n",
    "        log_p_x_and_z = model(z, x)\n",
    "        # importance sampling of approximate marginal likelihood with q(z)\n",
    "        # as the proposal, and logsumexp in the sample dimension\n",
    "        elbo = log_p_x_and_z - log_q_z\n",
    "        log_p_x = torch.logsumexp(elbo, dim=1) - np.log(n_samples)\n",
    "        # average over sample dimension, sum over minibatch\n",
    "        total_elbo += elbo.cpu().numpy().mean(1).sum()\n",
    "        # sum over minibatch\n",
    "        total_log_p_x += log_p_x.cpu().numpy().sum()\n",
    "    n_data = len(eval_data.dataset)\n",
    "    return total_elbo / n_data, total_log_p_x / n_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af3bd2f",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13e070f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    latent_size=128\n",
    "    variational=\"mean-field\"\n",
    "    flow_depth=2\n",
    "    data_size=784\n",
    "    learning_rate=0.001\n",
    "    batch_size=128\n",
    "    test_batch_size=512\n",
    "    max_iterations=30000\n",
    "    log_interval=10000\n",
    "    n_samples=1000\n",
    "    seed=582838\n",
    "    train_dir=\"train/\"\n",
    "    data_dir=\"data/\"\n",
    "    use_gpu=False\n",
    "\n",
    "parser=Parser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419dc80a",
   "metadata": {},
   "source": [
    "## Init model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21b1c2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dfa1985",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VariationalMeanField(\n",
       "  (inference_network): NeuralNetwork(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=784, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=256, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (log_q_z): NormalLogProb()\n",
       "  (softplus): Softplus(beta=1, threshold=20)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if cfg.use_gpu else \"cpu\")\n",
    "model = Model(latent_size=cfg.latent_size, data_size=cfg.data_size)\n",
    "\n",
    "if cfg.variational == \"flow\":\n",
    "    variational = VariationalFlow(\n",
    "        latent_size=cfg.latent_size,\n",
    "        data_size=cfg.data_size,\n",
    "        flow_depth=cfg.flow_depth)\n",
    "elif cfg.variational == \"mean-field\":\n",
    "    variational = VariationalMeanField(\n",
    "        latent_size=cfg.latent_size, data_size=cfg.data_size)\n",
    "else:\n",
    "    raise ValueError(\n",
    "        \"Variational distribution not implemented: %s\" % cfg.variational)\n",
    "    \n",
    "model.to(device)\n",
    "variational.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b7d9c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (log_p_z): NormalLogProb()\n",
       "  (log_p_x): BernoulliLogProb(\n",
       "    (bce_with_logits): BCEWithLogitsLoss()\n",
       "  )\n",
       "  (generative_network): NeuralNetwork(\n",
       "    (net): Sequential(\n",
       "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=256, out_features=784, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b29b2a",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690d48a1",
   "metadata": {},
   "source": [
    "#### Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "543c8520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading binary MNIST data...\n",
      "Saved binary MNIST data to: data\\binary_mnist.h5\n"
     ]
    }
   ],
   "source": [
    "fname = pathlib.Path(cfg.data_dir + \"binary_mnist.h5\")\n",
    "if not fname.exists():\n",
    "    print(\"Downloading binary MNIST data...\")\n",
    "    data.download_binary_mnist(fname)\n",
    "train_data, valid_data, test_data = data.load_binary_mnist(\n",
    "    fname, cfg.batch_size, cfg.test_batch_size, cfg.use_gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2243f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b=next(iter(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a089d900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad243589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a69359d",
   "metadata": {},
   "source": [
    "#### Version 2 - if you want to use get also validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2aba6e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2dc3f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} \n",
    "dataset_path='data'\n",
    "train_dataset = MNIST(dataset_path, transform=mnist_transform, train=True, download=True)\n",
    "test_dataset  = MNIST(dataset_path, transform=mnist_transform, train=False, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=cfg.batch_size, shuffle=True, **kwargs)\n",
    "test_loader  = DataLoader(dataset=test_dataset,  batch_size=cfg.batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98cb2676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "           )"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98f5d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b2=next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "521d9cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 784])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#but then have to change shape during the training\n",
    "b2[0].view(cfg.batch_size, cfg.data_size).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185b130e",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1478b88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.RMSprop(\n",
    "    list(model.parameters()) + list(variational.parameters()),\n",
    "    lr=cfg.learning_rate,\n",
    "    centered=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d873d",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a93818a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0         \tTrain ELBO estimate: -383.830\tValidation ELBO estimate: -267.057\tValidation log p(x) estimate: -241.069\tSpeed: 6.45e+07 examples/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [32]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39melbo\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     24\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 25\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m cfg\u001b[38;5;241m.\u001b[39mlog_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     28\u001b[0m     t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\rmsprop.py:141\u001b[0m, in \u001b[0;36mRMSprop.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    136\u001b[0m             grad_avgs\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad_avg\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    138\u001b[0m         state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 141\u001b[0m     \u001b[43mrmsprop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m            \u001b[49m\u001b[43msquare_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgrad_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43malpha\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmomentum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcentered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcentered\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\rmsprop.py:188\u001b[0m, in \u001b[0;36mrmsprop\u001b[1;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, foreach, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    186\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_rmsprop\n\u001b[1;32m--> 188\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m     \u001b[49m\u001b[43msquare_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum_buffer_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m     \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcentered\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcentered\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\optim\\rmsprop.py:235\u001b[0m, in \u001b[0;36m_single_tensor_rmsprop\u001b[1;34m(params, grads, square_avgs, grad_avgs, momentum_buffer_list, lr, alpha, eps, weight_decay, momentum, centered)\u001b[0m\n\u001b[0;32m    233\u001b[0m     param\u001b[38;5;241m.\u001b[39madd_(buf, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mlr)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 235\u001b[0m     \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcdiv_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "      \n",
    "    torch.manual_seed(cfg.seed)\n",
    "    np.random.seed(cfg.seed)\n",
    "    random.seed(cfg.seed)\n",
    "\n",
    "    best_valid_elbo = -np.inf\n",
    "    num_no_improvement = 0\n",
    "    train_ds = cycle(train_data)\n",
    "    t0 = time.time()\n",
    "\n",
    "    for step in range(cfg.max_iterations):\n",
    "        batch = next(train_ds)\n",
    "        x = batch[0].to(device)\n",
    "        model.zero_grad()\n",
    "        variational.zero_grad()\n",
    "        z, log_q_z = variational(x, n_samples=1)\n",
    "        log_p_x_and_z = model(z, x)\n",
    "        # average over sample dimension\n",
    "        # elbo = (kl - recon_loss), in our case other way around? recon_loss - kl? is from here: \n",
    "        #elbo = exptected reconstruction error - kl divergence between apporx posterios and prior\n",
    "        #https://towardsdatascience.com/variational-autoencoder-demystified-with-pytorch-implementation-3a06bee395ed\n",
    "        ##more on elbo: https://mbernste.github.io/posts/elbo/ and here: https://www.youtube.com/watch?v=IXsA5Rpp25w\n",
    "        elbo = (log_p_x_and_z - log_q_z).mean(1)\n",
    "        # sum over batch dimension\n",
    "        loss = -elbo.sum(0)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % cfg.log_interval == 0:\n",
    "            t1 = time.time()\n",
    "            examples_per_sec = cfg.log_interval * cfg.batch_size / (t1 - t0)\n",
    "            with torch.no_grad():\n",
    "                valid_elbo, valid_log_p_x = evaluate(\n",
    "                    cfg.n_samples, model, variational, valid_data\n",
    "                )\n",
    "            print(\n",
    "                f\"Step {step:<10d}\\t\"\n",
    "                f\"Train ELBO estimate: {elbo.detach().cpu().numpy().mean():<5.3f}\\t\"\n",
    "                f\"Validation ELBO estimate: {valid_elbo:<5.3f}\\t\"\n",
    "                f\"Validation log p(x) estimate: {valid_log_p_x:<5.3f}\\t\"\n",
    "                f\"Speed: {examples_per_sec:<5.2e} examples/s\"\n",
    "            )\n",
    "            if valid_elbo > best_valid_elbo:\n",
    "                num_no_improvement = 0\n",
    "                best_valid_elbo = valid_elbo\n",
    "                states = {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"variational\": variational.state_dict(),\n",
    "                }\n",
    "                torch.save(states, str(cfg.train_dir) + \"best_state_dict\")\n",
    "            t0 = t1\n",
    "\n",
    "    checkpoint = torch.load(cfg.train_dir + \"best_state_dict\")\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    variational.load_state_dict(checkpoint[\"variational\"])\n",
    "    test_elbo, test_log_p_x = evaluate(cfg.n_samples, model, variational, test_data)\n",
    "    print(\n",
    "        f\"Step {step:<10d}\\t\"\n",
    "        f\"Test ELBO estimate: {test_elbo:<5.3f}\\t\"\n",
    "        f\"Test log p(x) estimate: {test_log_p_x:<5.3f}\\t\"\n",
    "    )\n",
    "\n",
    "    print(f\"Total time: {(time.time() - start_time) / 60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8198095a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
