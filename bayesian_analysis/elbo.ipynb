{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4938abdd",
   "metadata": {},
   "source": [
    "Source: https://www.youtube.com/watch?v=IXsA5Rpp25w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b440665",
   "metadata": {},
   "source": [
    "We start with Bayes formula, where we have variable X (our data/observations) and want to estimate Z given our data that would have similar properties. Z is latent variable:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ea80b",
   "metadata": {},
   "source": [
    "$$\n",
    "    p_{\\Theta }(Z|X)=\\frac{p_{\\Theta }(X|Z)p_{\\Theta }(Z)}{p_{\\Theta }(X)}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $\n",
    "p_{\\Theta }(Z|X) $  is posterior\n",
    "\n",
    "- $\n",
    "p_{\\Theta }(X|Z) $ is likelihood\n",
    "\n",
    "- $\n",
    "p_{\\Theta }(Z) $ is prior\n",
    "\n",
    "- $\n",
    "p_{\\Theta }(X) $ is normalizing constant, evidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7805e7fa",
   "metadata": {},
   "source": [
    "Problem lies in the denominator (which represents continuous random variable) which leads to intractable integral:\n",
    "$\n",
    "\\int p_{\\Theta }(x|z)p_{\\Theta }(z)dz\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7036e3",
   "metadata": {},
   "source": [
    "To get around this problem is to approximate posterior with some other distribution\n",
    "$ q_{\\Theta }(Z|X) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84d4034",
   "metadata": {},
   "source": [
    "This distribution should be more well-behaved: we can sample from it and calculate posterior $ p_{\\Theta }(Z|X) \\approx q_{\\Theta }(Z|X) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768544e2",
   "metadata": {},
   "source": [
    "But we have to learn parameters for $ q_{\\Theta } $. To learn we need to compute dissimilarity with $ p_{\\Theta } $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a3b5f",
   "metadata": {},
   "source": [
    "We are going to use KL divergence for that:\n",
    "$$\n",
    "D_{KL}(q_{\\Theta }|| p_{\\Theta })=\\mathbb{E}_{q_{\\Theta}}[log\\frac{q_{\\Theta}(Z|X)}{p_{\\Theta}(Z|X)}]    \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58096fc1",
   "metadata": {},
   "source": [
    "Our goal is to minimize KL divergence between p and q. But we can't compute it directly, because of the denominator, which true but intractable posterior. This is a thing we want to approximate!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22685a3",
   "metadata": {},
   "source": [
    "To tackle this problem, let's first divide problem into pieces and see solve the pieces. First make divison into subtraction because we are using log function: \n",
    "$$\n",
    "\\mathbb{E}_{q_{\\Theta}}[ \\log q_{\\Theta}(Z|X)]- \\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(Z|X)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bc5ab3",
   "metadata": {},
   "source": [
    "1. Rewrite second component using Bayes' rule:\n",
    "$$\n",
    "\\mathbb{E}_{q_{\\Theta}}[ \\log q_{\\Theta}(Z|X)]-\\mathbb{E}_{q_{\\Theta}}[\\log \\frac{p_{\\Theta}(Z,X)}{p_{\\Theta}(X)}]\n",
    "$$\n",
    "A little refresher of the Bayes' rule:\n",
    "$\n",
    " p_{\\Theta }(Z|X)=\\frac{p_{\\Theta }(X|Z)p_{\\Theta }(Z)}{p_{\\Theta }(X)} = \\frac{p_{\\Theta }(Z, X)}{p_{\\Theta }(X)}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e18776",
   "metadata": {},
   "source": [
    "2. Split second component further:\n",
    "$\n",
    "\\mathbb{E}_{q_{\\Theta}}[ \\log q_{\\Theta}(Z|X)]-\\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(Z,X)]+\n",
    "\\mathbb{E}_{q_{\\Theta}}[p_{\\Theta}(X)]\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645835f4",
   "metadata": {},
   "source": [
    "3. focus on third term, this is expectation with respect to Z and could expand it to integral form like this:\n",
    "    $$\n",
    "    \\mathbb{E}_{q_{\\Theta}}[ \\log q_{\\Theta}(Z|X)]-\\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(Z,X)]+\n",
    "\\int q_{\\Theta}(Z|X) \\log p_{\\Theta}(X) dx\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e818a36",
   "metadata": {},
   "source": [
    "4. move the part that does not have  Z  outside of integral:\n",
    "    $$\n",
    "    \\mathbb{E}_{q_{\\Theta}}[ \\log q_{\\Theta}(Z|X)]-\\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(Z,X)]+\n",
    "\\log p_{\\Theta}(X) \\int q_{\\Theta}(Z|X)  dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d509ba68",
   "metadata": {},
   "source": [
    "5. final simplification, this is possible because integrating any density function would equate to 1 (area under pdf is 1):\n",
    "    $$\n",
    "    D_{KL}(q_{\\Theta }|| p_{\\Theta }) = \\mathbb{E}_{q_{\\Theta}}[ \\log q_{\\Theta}(Z|X)]-\\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(Z,X)]+\n",
    "\\log p_{\\Theta}(X)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a084069f",
   "metadata": {},
   "source": [
    "Now we have revealed something:\n",
    " - third component is marginal log likelihood/log evidence (total probability taking into account all hiden variables and parameters): $\\log p_{\\Theta}(X)$ We cannot computet it directly because we don't have its analytical form.\n",
    "\n",
    "But this equation shows that there is a relationship between log-likelihood ka KL divergence. It is interesting because most of the time we are doing log-likelihood estimation:\n",
    "\n",
    "$$\n",
    " \\log p_{\\Theta}(X)  = - \\mathbb{E}_{q_{\\Theta}}[ \\log q_{\\Theta}(Z|X)]+\\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(Z,X)]\n",
    "+ D_{KL}(q_{\\Theta }|| p_{\\Theta })\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad2ed38",
   "metadata": {},
   "source": [
    "We cannot compute KL divergence. But there is an interesting property that KL divergence possess: it can't be negative.\n",
    "Let's rewrite previous equation and get rid of KL divergence. At minimum we have to get rid of equality:\n",
    "$$\n",
    " \\log p_{\\Theta}(X)  \\geq  - \\mathbb{E}_{q_{\\Theta}}[ \\log q_{\\Theta}(Z|X)]+\\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(Z,X)]\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ade6b83",
   "metadata": {},
   "source": [
    "Right side of the equation we call __ELBO__ (evidence lower bound), lower bound on the evidence. Now how could we just get rid of KL divergence? Because by maximizing KL divergence we are indirectly minimizing KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0deacb",
   "metadata": {},
   "source": [
    "Let's do final simplification of ELBO $$ ELBO = - \\mathbb{E}_{q_{\\Theta}}[ \\log q_{\\Theta}(Z|X)]+\\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(Z,X)] $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2ac21",
   "metadata": {},
   "source": [
    "$$\n",
    "ELBO = - \\mathbb{E}_{q_{\\Theta}}[ \\log q_{\\Theta}(Z|X)]+\\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(X|Z)]+\n",
    "\\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(Z)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d5b9c2",
   "metadata": {},
   "source": [
    "Second term is joint probability of X and latent variable Z can be expanded into two factors. Because we are in log scale we are doing summation instead of multiplication. I have applied prduct rule of probability on joint distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f919d795",
   "metadata": {},
   "source": [
    "Now move terms around:\n",
    "   $$\n",
    "   ELBO = \\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(X|Z)]\n",
    "- \\mathbb{E}_{q_{\\Theta}}[ \\log q_{\\Theta}(Z|X)]+\n",
    "\\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(Z)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ad6a5d",
   "metadata": {},
   "source": [
    "And we'll get a final formula:\n",
    "$$\n",
    "ELBO = \\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(X|Z)]\n",
    "- \\mathbb{E}_{q_{\\Theta}}[ \\log \\frac{q_{\\Theta}(Z|X)}{p_{\\Theta}(Z)}]\n",
    "$$\n",
    "\n",
    "where:\n",
    " - $ \\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(X|Z)] $ is expected reconstruction error (we are reconstructing X given Z)\n",
    " - $ \\mathbb{E}_{q_{\\Theta}}[ \\log \\frac{q_{\\Theta}(Z|X)}{p_{\\Theta}(Z)}] $ is KL divergence between approximate posterior and prior (don't confuse it with KL divergence we started with, that was KL divergence between approximate posterior and true posterior. We know the prior and we will learn the approximate posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89561eac",
   "metadata": {},
   "source": [
    "Notwe that we chose Q so that it was conditional distribution (it was for Z but conditioned on X). But we couold have picked Q or Z so that it doesn't have to condition. All the derviations still work:\n",
    "$$\n",
    "ELBO = \\mathbb{E}_{q_{\\Theta}}[\\log p_{\\Theta}(X|Z)]\n",
    "- \\mathbb{E}_{q_{\\Theta}}[ \\log \\frac{q_{\\Theta}(Z)}{p_{\\Theta}(Z)}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c49940",
   "metadata": {},
   "source": [
    "Now we ought to find these:\n",
    "- $ q_{\\Theta}(Z|X) $\n",
    "- $ p_{\\Theta}(X|Z) $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c560c8",
   "metadata": {},
   "source": [
    "There are two algorithms for finding values:\n",
    "- expectation maximization\n",
    "- variational autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2ca1f8",
   "metadata": {},
   "source": [
    "Convert to pdf:  jupyter nbconvert --to webpdf --allow-chromium-download\n",
    " .\\elbo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781f6de4",
   "metadata": {},
   "source": [
    "more info: https://alpopkes.com/posts/machine_learning/kl_divergence/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5513e15e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
