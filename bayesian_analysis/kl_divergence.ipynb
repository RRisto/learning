{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dde1fd39",
   "metadata": {},
   "source": [
    "Source: https://www.youtube.com/watch?v=9_eZHt2qJs4&t=16s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8949a1",
   "metadata": {},
   "source": [
    "KL divergence is measure how one probability distribution is different from second, reference distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5a234",
   "metadata": {},
   "source": [
    "X is random variable and small x-s are the states it could take:\n",
    "    $\n",
    "    X=\\{x_{1},x_{2},x_{3}, ..., x_{n}\\}\n",
    "    $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac40f9c",
   "metadata": {},
   "source": [
    "We want to compare different probability distribution $ \\log p_{\\theta}x_{1} $ with some other probability distribution $ \\log q_{\\phi }x_{1} $. They dont have to be same type of distributions (usually q is simpler to use distribution for modelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aab1e0a",
   "metadata": {},
   "source": [
    "One way to calculate difference is use subtraction, which because we are using logs is division:\n",
    "$$\n",
    "\\log p_{\\theta}x_{1} - \\log q_{\\phi }x_{1} = \\log[\\frac{p_{\\theta}x_{1}}{q_{\\phi }x_{1}}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a067e1",
   "metadata": {},
   "source": [
    "This division is called log likelihood ratio. But currently we are calculatiing difference between one sample. We would like to calculate average difference between p and q. In random variables we say what is expected value of the random vairable or what is its central tendency.\n",
    "\n",
    "For random variables exptected value is weighted average of instances of random variable. Each variable and all states have probability of occurence and average should reflect that. Samples that have higher probability should contribute more to the average.\n",
    "$$\n",
    "\\mathbb{E}_{p_{\\theta }}[X]=\\sum_{i=1}^{\\infty}x_{i}p_{\\theta }(x_{i})\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_{i}$ is state of the random variable\n",
    "- $p_{\\theta }(x_{i})$ is weight of the random variable\n",
    "\n",
    "\n",
    "Here we showed that calculation should be done for very large number of samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0661a0",
   "metadata": {},
   "source": [
    "Here is same formula more in general, instead of random variable we calculate weighted average of a function of random variables \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{p_{\\theta }}[h(X)]=\\sum_{i=1}^{\\infty}h(x_{i})p_{\\theta }(x_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d252b7d",
   "metadata": {},
   "source": [
    "So far we have looked at discrete random variables. For continous random variable we would calculate exptected value as such (summation has been replaced by integral):\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{p_{\\theta }}[h(X)]=\\int_{\\mathbb{R}}^{}h(x_{i})p_{\\theta }(x_{i})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450aa4b1",
   "metadata": {},
   "source": [
    "Our log likelihood ratio is nothig but a function of random variable and since we are interested in average of this function we should be able to use expectation. So we need weights and compute the sums:\n",
    "\n",
    "$$\n",
    "\\sum_{i=1}^{\\infty}p_{\\theta }(x_{i})\\log[\\frac{p_{\\theta}x_{i}}{q_{\\phi }x_{i}}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2fd41",
   "metadata": {},
   "source": [
    "To be exact we are calculating exptected value of log likelihood ratio. This is __KL divergence__. We can express it using expectation symbol:\n",
    "\n",
    "$$\n",
    "\\mathbb{E_{p}}[\\log[\\frac{p_{\\theta}x_{i}}{q_{\\phi }x_{i}}]=\\sum_{i=i}^{\\infty}p_{\\theta }(x_{i})\\log[\\frac{p_{\\theta}x_{i}}{q_{\\phi }x_{i}}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb67fe1e",
   "metadata": {},
   "source": [
    "Previous example was for discrete random variable for continuous random variable we replace summation with an integral:\n",
    "$$\n",
    "\\mathbb{E_{p}}[\\log[\\frac{p_{\\theta}x_{i}}{q_{\\phi }x_{i}}]=\\int_{\\mathbb{R}}p_{\\theta }(x_{i})\\log[\\frac{p_{\\theta}x_{i}}{q_{\\phi }x_{i}}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3392d4e1",
   "metadata": {},
   "source": [
    "We have one problem. Integral and summation both go from minus infinity to infinity. We could get help from the __law of large numbers__: \"as a sample size grows, its mean gets closer to the average of the whole population\" (https://www.investopedia.com/terms/l/lawoflargenumbers.asp). So we can revrite KL divergence as a mean provided we use many samples:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N}\\sum_{i=1}^{N}\\log[\\frac{p_{\\theta}x_{i}}{q_{\\phi }x_{i}}]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dafb88",
   "metadata": {},
   "source": [
    "Antoher notation used is:\n",
    "    $$\n",
    "    DD_{KL}(p_{\\theta }||q_{\\phi })=\\int_{\\mathbb{R}}p_{\\theta }(x)log[\\frac{p_{\\theta}(x)}{q_{\\phi }(x)}]dx\n",
    "    $$\n",
    "    \n",
    "   This on is called __forward KL__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273d2bcd",
   "metadata": {},
   "source": [
    "If we would like to use q instead of p for weighting:\n",
    "\n",
    "$$\n",
    "D_{KL}(q_{\\phi }||p_{\\theta })=\\int_{\\mathbb{R}}q_{\\phi }(x)log[\\frac{q_{\\phi}(x)}{p_{\\theta }(x)}]dx\n",
    "$$\n",
    "\n",
    "This on is called __reverse KL__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717b3f60",
   "metadata": {},
   "source": [
    "These formulations are going to give different values. This is a reason why this is not called metric, but a distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a587119",
   "metadata": {},
   "source": [
    "Generally we use p for reference distribution and q for approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f901d",
   "metadata": {},
   "source": [
    "Which one to use? It depends. Forward KL has mean-seeking behavior as reverse KL has mode seeking behavior.\n",
    "\n",
    "![image](kl.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25821044",
   "metadata": {},
   "source": [
    "Here we can see that revrse has picked up mean mode of the distribution. Most of the time when we are doing density estimation and using variatinal inference we use reverse scale. Forward KL is being used a lot in machine learning but you don't see name forward KL a lot. It is being used indirectly. For example when we are using cross-entropy loss in classificatin loss we are using KL indirectly (at least a  component of KL divergence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ffcd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
