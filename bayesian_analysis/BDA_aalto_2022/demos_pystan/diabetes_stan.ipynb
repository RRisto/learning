{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression with PyStan\n",
    "\n",
    "TODO: Work in progress \n",
    "\n",
    "Authors: Jonah Gabry, Ben Goodrich, Aki Vehtari, Tuomas Sivula\n",
    "\n",
    "The introduction to Bayesian logistic regression is from a [CRAN vignette](https://cran.r-project.org/web/packages/rstanarm/vignettes/binomial.html) by Jonah Gabry and Ben Goodrich. CRAN vignette was modified to a [R notebook](https://github.com/avehtari/BDA_R_demos/blob/master/demos_rstan/diabetes.Rmd) by Aki Vehtari.  Instead of wells data in CRAN vignette, Pima Indians data is used. The end of the notebook differs significantly from the CRAN vignette.  The R notebook was ported to this Python notebook by Aki Vehtari and Tuomas Sivula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This vignette explains how to estimate generalized linear models (GLMs) for binary (Bernoulli) response variables using PyStan.\n",
    "\n",
    "The four steps of a Bayesian analysis are\n",
    "\n",
    "1. Specify a joint distribution for the outcome(s) and all the unknowns, which typically takes the form of a marginal prior distribution for the unknowns multiplied by a likelihood for the outcome(s) conditional on the unknowns. This joint distribution is proportional to a posterior distribution of the unknowns conditional on the observed data\n",
    "2. Draw from posterior distribution using Markov Chain Monte Carlo (MCMC).\n",
    "3. Evaluate how well the model fits the data and possibly revise the model.\n",
    "4. Draw from the posterior predictive distribution of the outcome(s) given interesting values of the predictors in order to visualize how a manipulation of a predictor affects (a function of) the outcome(s).\n",
    "This notebook demonstrates Steps 1-3 when the likelihood is the product of conditionally independent binomial distributions (possibly with only one trial per observation).\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "For a binomial GLM the likelihood for one observation $y$ can be written as a conditionally binomial PMF $$\\binom{n}{y} \\pi^{y} (1 - \\pi)^{n - y},$$ where $n$ is the known number of trials, $\\pi = g^{-1}(\\eta)$ is the probability of success and $\\eta = \\alpha + \\mathbf{x}^\\top \\boldsymbol{\\beta}$ is a linear predictor. For a sample of size $N$, the likelihood of the entire sample is the product of $N$ individual likelihood contributions.\n",
    "\n",
    "Because $\\pi$ is a probability, for a binomial model the link function $g$ maps between the unit interval (the support of $\\pi$) and the set of all real numbers $\\mathbb{R}$. When applied to a linear predictor $\\eta$ with values in $\\mathbb{R}$, the inverse link function $g^{-1}(\\eta)$ therefore returns a valid probability between 0 and 1.\n",
    "\n",
    "The two most common link functions used for binomial GLMs are the logit and probit functions. With the logit (or log-odds) link function $g(x) = \\ln{\\left(\\frac{x}{1-x}\\right)}$, the likelihood for a single observation becomes\n",
    "\n",
    "$$\\binom{n}{y}\\left(\\text{logit}^{-1}(\\eta)\\right)^y \\left(1 - \\text{logit}^{-1}(\\eta)\\right)^{n-y} = \\binom{n}{y} \\left(\\frac{e^{\\eta}}{1 + e^{\\eta}}\\right)^{y} \\left(\\frac{1}{1 + e^{\\eta}}\\right)^{n - y}$$\n",
    "\n",
    "and the probit link function $g(x) = \\Phi^{-1}(x)$ yields the likelihood\n",
    "\n",
    "$$\\binom{n}{y} \\left(\\Phi(\\eta)\\right)^{y} \\left(1 - \\Phi(\\eta)\\right)^{n - y},$$\n",
    "\n",
    "where $\\Phi$ is the CDF of the standard normal distribution. The differences between the logit and probit functions are minor and -- if, as rstanarm does by default, the probit is scaled so its slope at the origin matches the logit's -- the two link functions should yield similar results. Unless the user has a specific reason to prefer the probit link, we recommend the logit simply because it will be slightly faster and more numerically stable.\n",
    "\n",
    "In theory, there are infinitely many possible link functions, although in practice only a few are typically used. \n",
    "\n",
    "\n",
    "### Priors\n",
    "\n",
    "A full Bayesian analysis requires specifying prior distributions $f(\\alpha)$ and $f(\\boldsymbol{\\beta})$ for the intercept and vector of regression coefficients. \n",
    "\n",
    "As an example, suppose we have $K$ predictors and believe --- prior to seeing the data --- that $\\alpha, \\beta_1, \\dots, \\beta_K$ are as likely to be positive as they are to be negative, but are highly unlikely to be far from zero. These beliefs can be represented by normal distributions with mean zero and a small scale (standard deviation).\n",
    "\n",
    "If, on the other hand, we have less a priori confidence that the parameters will be close to zero then we could use a larger scale for the normal distribution and/or a distribution with heavier tails than the normal like the Student's $t$ distribution.\n",
    "\n",
    "### Posterior\n",
    "\n",
    "With independent prior distributions, the joint posterior distribution for $\\alpha$ and $\\boldsymbol{\\beta}$ is proportional to the product of the priors and the $N$ likelihood contributions:\n",
    "\n",
    "$$f\\left(\\alpha,\\boldsymbol{\\beta} | \\mathbf{y},\\mathbf{X}\\right) \\propto f\\left(\\alpha\\right) \\times \\prod_{k=1}^K f\\left(\\beta_k\\right) \\times \\prod_{i=1}^N { g^{-1}\\left(\\eta_i\\right)^{y_i} \\left(1 - g^{-1}\\left(\\eta_i\\right)\\right)^{n_i-y_i}}.$$\n",
    "\n",
    "This is posterior distribution that PyStan will draw from when using MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Example\n",
    "\n",
    "When the logit link function is used the model is often referred to as a logistic regression model (the inverse logit function is the CDF of the standard logistic distribution). As an example, here we will show how to carry out a analysis for Pima Indians data set similar to analysis from Chapter 5.4 of Gelman and Hill (2007) using PyStan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import stan interface\n",
    "import stan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add utilities directory to path\n",
    "import os, sys\n",
    "util_path = os.path.abspath(os.path.join(os.path.pardir, 'utilities_and_data'))\n",
    "if util_path not in sys.path and os.path.exists(util_path):\n",
    "    sys.path.insert(0, util_path)\n",
    "\n",
    "# import from utilities\n",
    "import stan_v3_utility\n",
    "import psis  # pareto smoothed importance sampling\n",
    "import plot_tools\n",
    "import arviz as az # For visualization and loo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#needed for notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "First we load and pre-process data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 768 entries, 0 to 767\n",
      "Data columns (total 9 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Pregnancies               768 non-null    int64  \n",
      " 1   Glucose                   768 non-null    int64  \n",
      " 2   BloodPressure             768 non-null    int64  \n",
      " 3   SkinThickness             768 non-null    int64  \n",
      " 4   Insulin                   768 non-null    int64  \n",
      " 5   BMI                       768 non-null    float64\n",
      " 6   DiabetesPedigreeFunction  768 non-null    float64\n",
      " 7   Age                       768 non-null    int64  \n",
      " 8   Outcome                   768 non-null    int64  \n",
      "dtypes: float64(2), int64(7)\n",
      "memory usage: 54.1 KB\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "data_path = os.path.abspath(\n",
    "    os.path.join(\n",
    "        os.path.pardir,\n",
    "        'utilities_and_data',\n",
    "        'diabetes.csv'\n",
    "    )\n",
    ")\n",
    "data = pd.read_csv(data_path)\n",
    "# print some basic info()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pregnancies  Glucose  BloodPressure  SkinThickness  Insulin   BMI  \\\n",
       "0            6      148             72             35        0  33.6   \n",
       "1            1       85             66             29        0  26.6   \n",
       "2            8      183             64              0        0  23.3   \n",
       "3            1       89             66             23       94  28.1   \n",
       "4            0      137             40             35      168  43.1   \n",
       "\n",
       "   DiabetesPedigreeFunction  Age  Outcome  \n",
       "0                     0.627   50        1  \n",
       "1                     0.351   31        0  \n",
       "2                     0.672   32        1  \n",
       "3                     0.167   21        0  \n",
       "4                     2.288   33        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview some first rows\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pregnancies</th>\n",
       "      <th>Glucose</th>\n",
       "      <th>BloodPressure</th>\n",
       "      <th>SkinThickness</th>\n",
       "      <th>Insulin</th>\n",
       "      <th>BMI</th>\n",
       "      <th>DiabetesPedigreeFunction</th>\n",
       "      <th>Age</th>\n",
       "      <th>Outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "      <td>768.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.845052</td>\n",
       "      <td>120.894531</td>\n",
       "      <td>69.105469</td>\n",
       "      <td>20.536458</td>\n",
       "      <td>79.799479</td>\n",
       "      <td>31.992578</td>\n",
       "      <td>0.471876</td>\n",
       "      <td>33.240885</td>\n",
       "      <td>0.348958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.369578</td>\n",
       "      <td>31.972618</td>\n",
       "      <td>19.355807</td>\n",
       "      <td>15.952218</td>\n",
       "      <td>115.244002</td>\n",
       "      <td>7.884160</td>\n",
       "      <td>0.331329</td>\n",
       "      <td>11.760232</td>\n",
       "      <td>0.476951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>27.300000</td>\n",
       "      <td>0.243750</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>117.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>30.500000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.372500</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>140.250000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>127.250000</td>\n",
       "      <td>36.600000</td>\n",
       "      <td>0.626250</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>199.000000</td>\n",
       "      <td>122.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>846.000000</td>\n",
       "      <td>67.100000</td>\n",
       "      <td>2.420000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Pregnancies     Glucose  BloodPressure  SkinThickness     Insulin  \\\n",
       "count   768.000000  768.000000     768.000000     768.000000  768.000000   \n",
       "mean      3.845052  120.894531      69.105469      20.536458   79.799479   \n",
       "std       3.369578   31.972618      19.355807      15.952218  115.244002   \n",
       "min       0.000000    0.000000       0.000000       0.000000    0.000000   \n",
       "25%       1.000000   99.000000      62.000000       0.000000    0.000000   \n",
       "50%       3.000000  117.000000      72.000000      23.000000   30.500000   \n",
       "75%       6.000000  140.250000      80.000000      32.000000  127.250000   \n",
       "max      17.000000  199.000000     122.000000      99.000000  846.000000   \n",
       "\n",
       "              BMI  DiabetesPedigreeFunction         Age     Outcome  \n",
       "count  768.000000                768.000000  768.000000  768.000000  \n",
       "mean    31.992578                  0.471876   33.240885    0.348958  \n",
       "std      7.884160                  0.331329   11.760232    0.476951  \n",
       "min      0.000000                  0.078000   21.000000    0.000000  \n",
       "25%     27.300000                  0.243750   24.000000    0.000000  \n",
       "50%     32.000000                  0.372500   29.000000    0.000000  \n",
       "75%     36.600000                  0.626250   41.000000    1.000000  \n",
       "max     67.100000                  2.420000   81.000000    1.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# some summary\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# modify the data column names slightly for easier typing\n",
    "# rename DiabetesPedigreeFunction to dpf\n",
    "data.rename(columns={'DiabetesPedigreeFunction': 'dpf'}, inplace=True)\n",
    "# make lower\n",
    "data.rename(columns=lambda old_name: old_name.lower(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# removing those observation rows with 0 in selected variables\n",
    "normed_predictors = [\n",
    "    'glucose',\n",
    "    'bloodpressure',\n",
    "    'skinthickness',\n",
    "    'insulin',\n",
    "    'bmi'\n",
    "]\n",
    "data = data[(data[normed_predictors] != 0).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# scale the covariates for easier comparison of coefficient posteriors\n",
    "# N.B. int columns turn into floats\n",
    "data.iloc[:,:-1] -= data.iloc[:,:-1].mean()\n",
    "data.iloc[:,:-1] /= data.iloc[:,:-1].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnancies</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bloodpressure</th>\n",
       "      <th>skinthickness</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>dpf</th>\n",
       "      <th>age</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.716511</td>\n",
       "      <td>-1.089653</td>\n",
       "      <td>-0.373178</td>\n",
       "      <td>-0.584363</td>\n",
       "      <td>-0.522175</td>\n",
       "      <td>-0.709514</td>\n",
       "      <td>-1.030559</td>\n",
       "      <td>-0.967063</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.027899</td>\n",
       "      <td>0.465719</td>\n",
       "      <td>-2.453828</td>\n",
       "      <td>0.556709</td>\n",
       "      <td>0.100502</td>\n",
       "      <td>1.424909</td>\n",
       "      <td>5.108582</td>\n",
       "      <td>0.209318</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.093734</td>\n",
       "      <td>-1.446093</td>\n",
       "      <td>-1.653578</td>\n",
       "      <td>0.271441</td>\n",
       "      <td>-0.572662</td>\n",
       "      <td>-0.296859</td>\n",
       "      <td>-0.796108</td>\n",
       "      <td>-0.476904</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.405123</td>\n",
       "      <td>2.409934</td>\n",
       "      <td>-0.053078</td>\n",
       "      <td>1.507603</td>\n",
       "      <td>3.255961</td>\n",
       "      <td>-0.368007</td>\n",
       "      <td>-1.056609</td>\n",
       "      <td>2.169953</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.716511</td>\n",
       "      <td>2.150705</td>\n",
       "      <td>-0.853328</td>\n",
       "      <td>-0.584363</td>\n",
       "      <td>5.805571</td>\n",
       "      <td>-0.424924</td>\n",
       "      <td>-0.361940</td>\n",
       "      <td>2.758143</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    pregnancies   glucose  bloodpressure  skinthickness   insulin       bmi  \\\n",
       "3     -0.716511 -1.089653      -0.373178      -0.584363 -0.522175 -0.709514   \n",
       "4     -1.027899  0.465719      -2.453828       0.556709  0.100502  1.424909   \n",
       "6     -0.093734 -1.446093      -1.653578       0.271441 -0.572662 -0.296859   \n",
       "8     -0.405123  2.409934      -0.053078       1.507603  3.255961 -0.368007   \n",
       "13    -0.716511  2.150705      -0.853328      -0.584363  5.805571 -0.424924   \n",
       "\n",
       "         dpf       age  outcome  \n",
       "3  -1.030559 -0.967063        0  \n",
       "4   5.108582  0.209318        1  \n",
       "6  -0.796108 -0.476904        1  \n",
       "8  -1.056609  2.169953        1  \n",
       "13 -0.361940  2.758143        1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preview some first rows againg\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preparing the inputs\n",
    "X = data.iloc[:,:-1]\n",
    "y = data.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of observations = 392\n",
      "number of predictors = 8\n"
     ]
    }
   ],
   "source": [
    "# get shape into variables\n",
    "n, p = X.shape\n",
    "print('number of observations = {}'.format(n))\n",
    "print('number of predictors = {}'.format(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stan model code for logistic regression\n",
    "\n",
    "Logistic regression with Student's $t$ prior as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/**\n",
      " * Logistic regression t-prior\n",
      " *\n",
      " * Priors:\n",
      " *     weights - student t\n",
      " *     intercept - student t\n",
      " */\n",
      "data {\n",
      "    int<lower=0> n;               // number of data points\n",
      "    int<lower=1> d;               // explanatory variable dimension\n",
      "    matrix[n, d] X;               // explanatory variable\n",
      "    int<lower=0,upper=1> y[n];    // response variable\n",
      "    int<lower=1> p_alpha_df;      // prior degrees of freedom for alpha\n",
      "    real p_alpha_loc;             // prior location for alpha\n",
      "    real<lower=0> p_alpha_scale;  // prior scale for alpha\n",
      "    int<lower=1> p_beta_df;       // prior degrees of freedom for beta\n",
      "    real p_beta_loc;              // prior location for beta\n",
      "    real<lower=0> p_beta_scale;   // prior scale for beta\n",
      "}\n",
      "parameters {\n",
      "    real alpha;      // intercept\n",
      "    vector[d] beta;  // explanatory variable weights\n",
      "}\n",
      "transformed parameters {\n",
      "    // linear predictor\n",
      "    vector[n] eta;\n",
      "    eta = alpha + X * beta;\n",
      "}\n",
      "model {\n",
      "    alpha ~ student_t(p_alpha_df, p_alpha_loc, p_alpha_scale);\n",
      "    beta ~ student_t(p_beta_df, p_beta_loc, p_beta_scale);\n",
      "    y ~ bernoulli_logit(eta);\n",
      "}\n",
      "generated quantities {\n",
      "    vector[n] log_lik;\n",
      "    for (i in 1:n)\n",
      "        log_lik[i] = bernoulli_logit_lpmf(y[i] | eta[i]);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('logistic_t.stan') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set priors and sample from the posterior\n",
    "\n",
    "Here we'll use a Student t prior with 7 degrees of freedom and a scale of 2.5, which, as discussed above, is a reasonable default prior when coefficients should be close to zero but have some chance of being large. PyStan  returns the posterior distribution for the parameters describing the uncertainty related to unknown parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    }
   ],
   "source": [
    "# model = stan_utility.compile_model('logistic_t.stan')\n",
    "data1 = dict(\n",
    "    n=n,\n",
    "    d=p,\n",
    "    X=X.to_numpy(),\n",
    "    y=y.to_list(),\n",
    "    p_alpha_df=7,\n",
    "    p_alpha_loc=0,\n",
    "    p_alpha_scale=2.5,\n",
    "    p_beta_df=7,\n",
    "    p_beta_loc=0,\n",
    "    p_beta_scale=2.5\n",
    ")\n",
    "\n",
    "model = stan_v3_utility.compile_model('logistic_t.stan', data=data1, random_seed=74749)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mSampling:\u001b[0m   0%\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  25% (2000/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  50% (4000/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  75% (6000/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m 100% (8000/8000)\n",
      "\u001b[1A\u001b[0J\u001b[32mSampling:\u001b[0m 100% (8000/8000), done.\n",
      "\u001b[36mMessages received during sampling:\u001b[0m\n",
      "  Gradient evaluation took 0.000454 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 4.54 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 7.1e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.71 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 7.7e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.77 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Gradient evaluation took 6e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.6 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    }
   ],
   "source": [
    "fit1 = model.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data1 = dict(\n",
    "#     n=n,\n",
    "#     d=p,\n",
    "#     X=X,\n",
    "#     y=y,\n",
    "#     p_alpha_df=7,\n",
    "#     p_alpha_loc=0,\n",
    "#     p_alpha_scale=2.5,\n",
    "#     p_beta_df=7,\n",
    "#     p_beta_loc=0,\n",
    "#     p_beta_scale=2.5\n",
    "# )\n",
    "# fit1 = model.sampling(data=data1, seed=74749)\n",
    "# samples1 = fit1.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the resulting posterior\n",
    "\n",
    "Check n_effs and Rhats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parameters</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>-1.016862</td>\n",
       "      <td>0.144546</td>\n",
       "      <td>-1.578147</td>\n",
       "      <td>-1.311535</td>\n",
       "      <td>-1.112018</td>\n",
       "      <td>-1.014672</td>\n",
       "      <td>-0.918253</td>\n",
       "      <td>-0.740157</td>\n",
       "      <td>-0.487295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.1</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.264932</td>\n",
       "      <td>0.178422</td>\n",
       "      <td>-0.426140</td>\n",
       "      <td>-0.089818</td>\n",
       "      <td>0.142755</td>\n",
       "      <td>0.266089</td>\n",
       "      <td>0.388841</td>\n",
       "      <td>0.605866</td>\n",
       "      <td>0.903036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.2</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.210818</td>\n",
       "      <td>0.176717</td>\n",
       "      <td>0.611739</td>\n",
       "      <td>0.865590</td>\n",
       "      <td>1.087157</td>\n",
       "      <td>1.208064</td>\n",
       "      <td>1.329381</td>\n",
       "      <td>1.563500</td>\n",
       "      <td>1.811100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.3</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>-0.013565</td>\n",
       "      <td>0.149726</td>\n",
       "      <td>-0.497163</td>\n",
       "      <td>-0.297799</td>\n",
       "      <td>-0.114905</td>\n",
       "      <td>-0.012731</td>\n",
       "      <td>0.088033</td>\n",
       "      <td>0.278724</td>\n",
       "      <td>0.451196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.4</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.124018</td>\n",
       "      <td>0.176936</td>\n",
       "      <td>-0.519438</td>\n",
       "      <td>-0.218820</td>\n",
       "      <td>0.009998</td>\n",
       "      <td>0.123276</td>\n",
       "      <td>0.242995</td>\n",
       "      <td>0.471641</td>\n",
       "      <td>0.761128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.5</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>-0.095054</td>\n",
       "      <td>0.153800</td>\n",
       "      <td>-0.629984</td>\n",
       "      <td>-0.389173</td>\n",
       "      <td>-0.199717</td>\n",
       "      <td>-0.097883</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>0.212003</td>\n",
       "      <td>0.464952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.6</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.504239</td>\n",
       "      <td>0.190303</td>\n",
       "      <td>-0.199331</td>\n",
       "      <td>0.140657</td>\n",
       "      <td>0.372355</td>\n",
       "      <td>0.500615</td>\n",
       "      <td>0.636858</td>\n",
       "      <td>0.865561</td>\n",
       "      <td>1.206584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.7</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.407154</td>\n",
       "      <td>0.151684</td>\n",
       "      <td>-0.129616</td>\n",
       "      <td>0.110761</td>\n",
       "      <td>0.304798</td>\n",
       "      <td>0.405008</td>\n",
       "      <td>0.507456</td>\n",
       "      <td>0.708236</td>\n",
       "      <td>0.927356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.8</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.360972</td>\n",
       "      <td>0.189553</td>\n",
       "      <td>-0.361327</td>\n",
       "      <td>0.007722</td>\n",
       "      <td>0.231943</td>\n",
       "      <td>0.351980</td>\n",
       "      <td>0.484399</td>\n",
       "      <td>0.744188</td>\n",
       "      <td>1.030134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count      mean       std       min      2.5%       25%  \\\n",
       "parameters                                                             \n",
       "alpha       4000.0 -1.016862  0.144546 -1.578147 -1.311535 -1.112018   \n",
       "beta.1      4000.0  0.264932  0.178422 -0.426140 -0.089818  0.142755   \n",
       "beta.2      4000.0  1.210818  0.176717  0.611739  0.865590  1.087157   \n",
       "beta.3      4000.0 -0.013565  0.149726 -0.497163 -0.297799 -0.114905   \n",
       "beta.4      4000.0  0.124018  0.176936 -0.519438 -0.218820  0.009998   \n",
       "beta.5      4000.0 -0.095054  0.153800 -0.629984 -0.389173 -0.199717   \n",
       "beta.6      4000.0  0.504239  0.190303 -0.199331  0.140657  0.372355   \n",
       "beta.7      4000.0  0.407154  0.151684 -0.129616  0.110761  0.304798   \n",
       "beta.8      4000.0  0.360972  0.189553 -0.361327  0.007722  0.231943   \n",
       "\n",
       "                 50%       75%     97.5%       max  \n",
       "parameters                                          \n",
       "alpha      -1.014672 -0.918253 -0.740157 -0.487295  \n",
       "beta.1      0.266089  0.388841  0.605866  0.903036  \n",
       "beta.2      1.208064  1.329381  1.563500  1.811100  \n",
       "beta.3     -0.012731  0.088033  0.278724  0.451196  \n",
       "beta.4      0.123276  0.242995  0.471641  0.761128  \n",
       "beta.5     -0.097883  0.009244  0.212003  0.464952  \n",
       "beta.6      0.500615  0.636858  0.865561  1.206584  \n",
       "beta.7      0.405008  0.507456  0.708236  0.927356  \n",
       "beta.8      0.351980  0.484399  0.744188  1.030134  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print summary of selected variables\n",
    "# use pandas data frame for layout\n",
    "summary = fit1.to_frame()\n",
    "cols_alpha_beta=[c for c in summary.columns if 'alpha' in c or 'beta' in c]\n",
    "summary_sub=summary[cols_alpha_beta]#[['alpha', 'beta[0]']]\n",
    "summary_sub.describe(percentiles=[.025, .25, .5, .75, .975]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>-1.017</td>\n",
       "      <td>0.145</td>\n",
       "      <td>-1.295</td>\n",
       "      <td>-0.747</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>5019.0</td>\n",
       "      <td>2771.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[0]</th>\n",
       "      <td>0.265</td>\n",
       "      <td>0.178</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3537.0</td>\n",
       "      <td>3209.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[1]</th>\n",
       "      <td>1.211</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.877</td>\n",
       "      <td>1.541</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4181.0</td>\n",
       "      <td>3330.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[2]</th>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.150</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.268</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4697.0</td>\n",
       "      <td>3206.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[3]</th>\n",
       "      <td>0.124</td>\n",
       "      <td>0.177</td>\n",
       "      <td>-0.201</td>\n",
       "      <td>0.459</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3442.0</td>\n",
       "      <td>2723.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[4]</th>\n",
       "      <td>-0.095</td>\n",
       "      <td>0.154</td>\n",
       "      <td>-0.378</td>\n",
       "      <td>0.198</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>4447.0</td>\n",
       "      <td>3243.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[5]</th>\n",
       "      <td>0.504</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.174</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3407.0</td>\n",
       "      <td>2877.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[6]</th>\n",
       "      <td>0.407</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.120</td>\n",
       "      <td>0.692</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>5066.0</td>\n",
       "      <td>2712.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta[7]</th>\n",
       "      <td>0.361</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.718</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3591.0</td>\n",
       "      <td>2904.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>eta[0]</th>\n",
       "      <td>-3.670</td>\n",
       "      <td>0.362</td>\n",
       "      <td>-4.361</td>\n",
       "      <td>-3.007</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.004</td>\n",
       "      <td>4290.0</td>\n",
       "      <td>2907.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "alpha   -1.017  0.145  -1.295   -0.747      0.002    0.001    5019.0   \n",
       "beta[0]  0.265  0.178  -0.084    0.581      0.003    0.002    3537.0   \n",
       "beta[1]  1.211  0.177   0.877    1.541      0.003    0.002    4181.0   \n",
       "beta[2] -0.014  0.150  -0.287    0.268      0.002    0.002    4697.0   \n",
       "beta[3]  0.124  0.177  -0.201    0.459      0.003    0.002    3442.0   \n",
       "beta[4] -0.095  0.154  -0.378    0.198      0.002    0.002    4447.0   \n",
       "beta[5]  0.504  0.190   0.174    0.867      0.003    0.002    3407.0   \n",
       "beta[6]  0.407  0.152   0.120    0.692      0.002    0.002    5066.0   \n",
       "beta[7]  0.361  0.190   0.008    0.718      0.003    0.002    3591.0   \n",
       "eta[0]  -3.670  0.362  -4.361   -3.007      0.006    0.004    4290.0   \n",
       "\n",
       "         ess_tail  r_hat  \n",
       "alpha      2771.0    1.0  \n",
       "beta[0]    3209.0    1.0  \n",
       "beta[1]    3330.0    1.0  \n",
       "beta[2]    3206.0    1.0  \n",
       "beta[3]    2723.0    1.0  \n",
       "beta[4]    3243.0    1.0  \n",
       "beta[5]    2877.0    1.0  \n",
       "beta[6]    2712.0    1.0  \n",
       "beta[7]    2904.0    1.0  \n",
       "eta[0]     2907.0    1.0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "az.summary(fit1).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # print summary of selected variables\n",
    "# # use pandas data frame for layout\n",
    "# summary = fit1.summary(pars=['alpha', 'beta'])\n",
    "# pd.DataFrame(\n",
    "#     summary['summary'],\n",
    "#     index=summary['summary_rownames'],\n",
    "#     columns=summary['summary_colnames']\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n_effs are high and Rhats<1.1, which is good.\n",
    "\n",
    "Next we check divergences, E-BMFI and treedepth exceedences as explained in [Robust Statistical Workflow with PyStan Case Study](http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html) by Michael Betancourt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 4000 iterations saturated the maximum tree depth of 10 (0.0%)\n",
      "0.0 of 4000 iterations ended with a divergence (0.0%)\n"
     ]
    }
   ],
   "source": [
    "stan_v3_utility.check_treedepth(fit1)\n",
    "stan_v3_utility.check_energy(fit1)\n",
    "stan_v3_utility.check_div(fit1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stan_utility.check_treedepth(fit1)\n",
    "# stan_utility.check_energy(fit1)\n",
    "# stan_utility.check_div(fit1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is fine based on these diagnostics and we can proceed with our analysis.\n",
    "\n",
    "Visualise the marginal posterior distributions of each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAf8AAAJGCAYAAABV3IyeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy88F64QAAAACXBIWXMAAA9hAAAPYQGoP6dpAABBpElEQVR4nO3de3xM1/7/8feOS5LKzKSNS6LCKBGXciLCcalLS0tpWtUWrYPQr7R1q2rRi4relBZx6VW16O2ooqqtKkqooiWSw68Udc05otoiCV+HkP37oz/z65CQSDIzmf16Ph55kL3X7P1ZsyePd9aatSeGaZqmAACAZQR4uwAAAOBZhD8AABZD+AMAYDGEPwAAFkP4AwBgMYQ/AAAWQ/gDAGAxhD8A33Tge+nMSW9XAfglwh+Ab3r/LmnH596uAvBLhD8A35SX++cXgBJH+AMAYDHlvV0A4A29Z23Kd/v8xJYergQAPI/wh98rKOgBwKqY9gcAwGIIfwAALIbwBwDAYgh/AAAshgV/8BslsbCPuwAAWAEjfwAALIbwBwDAYsp0+Hfo0EEjRozwdhkAAJQpZfo9/8WLF6tChQqFanvgwAHVrl1baWlpiomJKd3CSkhCQoJOnDihJUuWeLsUAIAfKdPhf91113nlvLm5uYX+pQMAAF/jN9P+TqdTEyZM0MCBA2Wz2VSzZk3NmjXL1bZ27dqSpKZNm8owDHXo0MG1b86cOWrQoIGCgoJUv359vfHGG659Bw4ckGEYWrBggTp06KCgoCB9+OGHkqT33ntPjRo1UmBgoCIiIjR06FDX47KyspSYmKiqVavKbrfrlltu0b/+9S/X/vHjxysmJkZvv/22IiMjdc011+i+++7TiRMnXPvnzZunzz//XIZhyDAMpaSklPAzCACwojId/hebMmWK4uLilJaWpsGDB+uRRx7Rzz//LEn68ccfJUmrVq1SZmamFi9eLEl655139Mwzz+ill17Szp07NWHCBD377LOaN2+e27HHjBmj4cOHa+fOnercubPefPNNDRkyRImJidq+fbuWLl2qunXrSpJM01S3bt105MgRLVu2TKmpqYqNjVXHjh117Ngx1zF/+eUXLViwQF988YWWL1+u9PR0DRkyRJL0xBNPqGfPnurSpYsyMzOVmZmp1q1bl/pzCADwf2V62v9iXbt21eDBgyX9GdbJyclKSUlR/fr1VaVKFUlSWFiYwsPDXY954YUXNGXKFPXo0UPSnzMEO3bs0Ntvv63+/fu72o0YMcLVRpJefPFFPf7443r00Udd25o3by5JWrNmjbZv366jR48qMDBQkjR58mQtWbJECxcuVGJioiTpv//9r+bNm6caNWpIkmbOnKlu3bppypQpCg8PV3BwsM6cOeNWLwAAxeVX4d+kSRPX/w3DUHh4uI4ePVpg+99++00ZGRl68MEHNWjQINf2c+fOyeFwuLWNi4tz/f/o0aM6fPiwOnbsmO9xU1NTdfLkSYWFhbltP336tPbu3ev6vmbNmq7gl6RWrVopLy9Pu3btIvABAKXGr8L/4kV4hmEoLy+vwPYX9r3zzjv6+9//7ravXLlybt9XqlTJ9f/g4ODL1pGXl6eIiIh836MPDQ0t8HGGYbj9CwBAafCr8L+cihUrSpLOnz/v2latWjVdf/312rdvn/r06VPoY9lsNjmdTn377be6+eabL9kfGxurI0eOqHz58nI6nQUe59ChQzp8+LCqV68uSdq4caMCAgJUr149V81/rRcAgJJgmfCvWrWqgoODtXz5ctWoUUNBQUFyOBwaP368hg8fLrvdrttvv11nzpzRli1bdPz4cY0cObLA440fP14PP/ywqlatqttvv105OTn6/vvvNWzYMHXq1EmtWrVS9+7dNWnSJEVHR+vw4cNatmyZunfv7noLISgoSP3799fkyZOVnZ2t4cOHq2fPnq4pf6fTqW+++Ua7du1SWFiYHA4HtxiqZD7Dv6TOyWf+AyiL/Gq1/+WUL19eM2bM0Ntvv63q1avrrrvukiT9z//8j2bPnq25c+eqcePGat++vebOneu6NbAg/fv317Rp0/TGG2+oUaNGuuOOO7Rnzx5Jf07bL1u2TO3atdPAgQNVr1499e7dWwcOHFC1atVcx6hbt6569Oihrl276rbbbtONN97odpvhoEGDFB0drbi4OFWpUkXff/99KTwzAACrMUzTNL1dhBWNHz9eS5YsUXp6urdLKXO8MfIvCCP/UjTeIcVPl5oleLsSwO9YZuQPAAD+RPgDAGAxhL+XjB8/nil/AIBXEP4AAFgM4Q8AgMUQ/gAAWAzhDwCAxRD+AABYjGU+3hdljy99mA8A+BPCHygGPvMfQFnEtD8AABZD+AMAYDGEPwAAFkP4AwBgMYQ/AAAWQ/gDAGAxhD8AABbDff5AKeD+fwC+jPCH1/FJfgDgWUz7AwBgMYQ/AAAWw7Q/PIbpfQDwDYz8AQCwGEb+gAdxFwAAX8DIHwAAi2HkjxLHe/tFl99zxmwAgNJC+PsB0zSVk5Pj8fMOmLPZ4+e0kuzsbG+X4FV2SadP/1e5Fn8egKths9lkGEaB+w3TNE0P1oNSkJ2dLYfD4e0ygBJlJtk16IvTmr0119ulAGVOVlaW7HZ7gfsZ+fsBm82mrKysUj9Pdna2IiMjlZGRcdkXVVlBf3zc1EhJ8pv++Nv1oT++zWazXXY/4e8HDMPw6IvVbrf7xQ/HBfTHt9Ef30Z/yiZW+wMAYDGEPwAAFkP4o9ACAwOVlJSkwMBAb5dSIuiP74uPj/eb/vjb9aE/ZRur/QH4pvEOKX661CzB25UAfoeRPwAAFkP4AwBgMYQ/AAAWQ/gDAGAxhD8AABZD+AMAYDGEPwAAFkP4AwBgMYQ/AAAWQ/gDAGAxhD8AABZD+AMAYDGEPwAAFkP4AwBgMYQ/AAAWU97bBVhBSkqKbr75Zh0/flyhoaHeLgfA/9N71qZLts1PbOmFSgDPIvw9oHXr1srMzJTD4fB2KQCuIL9fCCR+KYB/8cnwP3v2rCpWrOjtMkpMxYoVFR4e7u0yAMsqKNABq/LIe/4dOnTQ0KFDNXToUIWGhiosLExjx46VaZqSJKfTqRdffFEJCQlyOBwaNGiQJGnDhg1q166dgoODFRkZqeHDh+vUqVOu42ZmZqpbt24KDg5W7dq19fHHH8vpdGratGmuNoZhaPbs2br77rt1zTXXKCoqSkuXLnXtP3/+vB588EHVrl1bwcHBio6O1vTp093qT0hIUPfu3TV58mRFREQoLCxMQ4YMUW5urqvNmTNnNHr0aEVGRiowMFBRUVF69913Jf057W8Yhk6cOOFqf6W+vfHGG4qKilJQUJCqVaume++9t/gXAgAAeXDB37x581S+fHn98MMPmjFjhpKTkzV79mzX/ldffVU33nijUlNT9eyzz2r79u3q3LmzevTooW3btumTTz7R+vXrNXToUNdj+vXrp8OHDyslJUWLFi3SrFmzdPTo0UvO/dxzz6lnz57atm2bunbtqj59+ujYsWOSpLy8PNWoUUMLFizQjh07NG7cOD399NNasGCB2zHWrFmjvXv3as2aNZo3b57mzp2ruXPnutUyf/58zZgxQzt37tRbb72lkJCQfJ+LK/Vty5YtGj58uJ5//nnt2rVLy5cvV7t27a76uQcA4K8M88LwuxR16NBBR48e1U8//STDMCRJTz75pJYuXaodO3bI6XSqadOm+uyzz1yP6devn4KDg/X222+7tq1fv17t27fXqVOndODAATVo0ECbN29WXFycJOmXX35RVFSUkpOTNWLEiD87aBgaO3asXnjhBUnSqVOnZLPZtGzZMnXp0iXfeocMGaJff/1VCxculPTnyD8lJUV79+5VuXLlJEk9e/ZUQECA5s+fr927dys6OlorV65Up06dLjnexQv+rtS3ZcuWacCAAfr3v/8tm812tU87ULaNd0jx06VmCcU+VElM+/OeP/yJx97zb9mypSv4JalVq1aaMmWKzp8/L0muAL8gNTVVv/zyiz766CPXNtM0lZeXp/3792v37t0qX768YmNjXfvr1q2ra6+99pJzN2nSxPX/SpUqyWazuc0QvPXWW5o9e7YOHjyo06dP6+zZs4qJiXE7RqNGjVzBL0kRERHavn27JCk9PV3lypVT+/btC/VcXKlvt956q2rVqqUbbrhBXbp0UZcuXVxvWwAAUFw+s+CvUqVKbt/n5eXpoYce0vDhwy9pW7NmTe3atSvf4+Q3kVGhQgW37w3DUF5eniRpwYIFeuyxxzRlyhS1atVKNptNr776qn744YdCHyM4OPgKvXN3pb5VrFhRW7duVUpKilasWKFx48Zp/Pjx2rx5M7cKAl7CXQDwJx4L/02bNl3yfVRUlNto+q9iY2P1008/qW7duvnur1+/vs6dO6e0tDQ1a9ZM0p/T/n9dVFcY3333nVq3bq3Bgwe7tu3du7dIx2jcuLHy8vK0du3afKf9L3alvklS+fLl1alTJ3Xq1ElJSUkKDQ3V6tWr1aNHjyLVBlgJq/qBwvHYgr+MjAyNHDlSu3bt0j//+U/NnDlTjz76aIHtx4wZo40bN2rIkCFKT0/Xnj17tHTpUg0bNkzSn+HfqVMnJSYm6scff1RaWpoSExMVHBzs9vbCldStW1dbtmzRN998o927d+vZZ5/V5s2bi9Q3p9Op/v37a+DAgVqyZIn279+vlJSUSxYNFrZvX375pWbMmKH09HQdPHhQ77//vvLy8hQdHV2kugAAyI/Hwr9fv346ffq0WrRooSFDhmjYsGFKTEwssH2TJk20du1a7dmzR23btlXTpk317LPPKiIiwtXm/fffV7Vq1dSuXTvdfffdGjRokGw2m4KCggpd18MPP6wePXqoV69e+vvf/64//vjDbRagsN58803de++9Gjx4sOrXr69Bgwa53bpXlL6FhoZq8eLFuuWWW9SgQQO99dZb+uc//6lGjRoVuS4AAC7msdX+MTExbvffl4Z///vfioyM1KpVq9SxY8dSPReAUnYVq/29Me3Pe/4oi3xmwd/VWL16tU6ePKnGjRsrMzNTo0ePltPp5J54AAAuo0yHf25urp5++mnt27dPNptNrVu31kcffXTJynwAKC3cBYCyyCPhn5KSUirH7dy5szp37lwqxwYAwF+V6ZE/AGvilj6geDy22h8AAPgGwh8AAIth2h8ASgELAeHLGPkDAGAxhD8AABbDtD8AnzVr3T6tTmVlP1DSGPkDAGAxhD8AABZD+AMAYDGEPwAAFkP4AwBgMaz2BwAP4sN/4AsY+QMAYDGEPwAAFsO0PwCvy28qfL4X6gCsgpE/AAAWQ/gDAGAxTPsD8JiCVrqDuwDgWYz8AQCwGEb+AEocI3zAtxH+AODDeDsApYHwB4AyKN/bI/mFAIVE+PsB0zSVk5Pj7TLg5wbM2ezxc57PPavc06c8ft6y6p7p3+a7fc6A5h6uBN5ms9lkGEaB+w3TNE0P1oNSkJ2dLYfD4e0ygBJlJtk16IvTmr0119ulAGVOVlaW7HZ7gfsZ+fsBm82mrKysUj9Pdna2IiMjlZGRcdkXVVlBf3zc1EhJ8pv++Nv1oT++zWazXXY/4e8HDMPw6IvVbrf7xQ/HBfTHt9Ef30Z/yibu8wcAwGIIfwAALIbwR6EFBgYqKSlJgYGB3i6lRNAf3xcfH+83/fG360N/yjZW+wPwTeMdUvx0qVmCtysB/A4jfwAALIbwBwDAYgh/AAAshvAHAMBiCH8AACyG8AcAwGIIfwAALIbwBwDAYgh/AAAshvAHAMBiCH8AACyG8AcAwGLKe7sAAAB6z9qU7/b5iS09XIk1MPIHAMBiCH8AACyG8JfkdDo1bdo0b5cBAIBHEP4AAFgMC/4AAD6LhYClwxIj/5ycHPXp00eVKlVSRESEkpOT1aFDB40YMeKStgcOHJBhGEpPT3dtO3HihAzDUEpKimvbTz/9pG7duslut8tms6lt27bau3evJCkvL0/PP/+8atSoocDAQMXExGj58uWux549e1ZDhw5VRESEgoKC5HQ69fLLL7v2Z2VlKTExUVWrVpXdbtctt9yif/3rXyX+vACAp/WetSnfL3iWJcJ/5MiR+v7777V06VKtXLlS3333nbZu3XrVx/vPf/6jdu3aKSgoSKtXr1ZqaqoGDhyoc+fOSZKmT5+uKVOmaPLkydq2bZs6d+6sO++8U3v27JEkzZgxQ0uXLtWCBQu0a9cuffjhh3I6nZIk0zTVrVs3HTlyRMuWLVNqaqpiY2PVsWNHHTt2rNjPBQAAfj/tn5OTo3nz5unjjz9Wx44dJUlz5sxR9erVr/qYr7/+uhwOh+bPn68KFSpIkurVq+faP3nyZI0ZM0a9e/eWJE2aNElr1qzRtGnT9Prrr+vQoUOKiorSTTfdJMMwVKtWLddj16xZo+3bt+vo0aMKDAx0HW/JkiVauHChEhMTr7puAPAkRvS+y+9H/vv27VNubq5atGjh2uZwOBQdHX3Vx0xPT1fbtm1dwf9X2dnZOnz4sNq0aeO2vU2bNtq5c6ckKSEhQenp6YqOjtbw4cO1YsUKV7vU1FSdPHlSYWFhCgkJcX3t37/f9bYCAADF4fcjf9M0JUmGYeS7/WIBAQGX7M/NzXVrExwcfMXz5ne+C9tiY2O1f/9+ff3111q1apV69uypTp06aeHChcrLy1NERITb+oILQkNDr3heAACuxO9H/nXq1FGFChX0448/urZlZ2e73n+/WJUqVSRJmZmZrm1/XfwnSU2aNNF33313yS8FkmS321W9enWtX7/ebfuGDRvUoEEDt3a9evXSO++8o08++USLFi3SsWPHFBsbqyNHjqh8+fKqW7eu21flypWL3H8AAC7m9yN/m82m/v37a9SoUbruuutUtWpVJSUlKSAg4JLRufTnqL5ly5aaOHGinE6nfv/9d40dO9atzdChQzVz5kz17t1bTz31lBwOhzZt2qQWLVooOjpao0aNUlJSkurUqaOYmBjNmTNH6enp+uijjyRJycnJioiIUExMjAICAvTpp58qPDxcoaGh6tSpk1q1aqXu3btr0qRJio6O1uHDh7Vs2TJ1795dcXFxHnneAAD+y+/DX5KmTp2qhx9+WHfccYfsdrtGjx6tjIwMBQUF5dv+vffe08CBAxUXF6fo6Gi98soruu2221z7w8LCtHr1ao0aNUrt27dXuXLlFBMT43qff/jw4crOztbjjz+uo0ePqmHDhlq6dKmioqIkSSEhIZo0aZL27NmjcuXKqXnz5lq2bJnrLYdly5bpmWee0cCBA/Xbb78pPDxc7dq1U7Vq1Ur5mQKAomNhX9ljmAW9+e3HTp06peuvv15TpkzRgw8+6O1yAORnvEOKny41S/B2JbgCXwp/PvyncCwx8k9LS9PPP/+sFi1aKCsrS88//7wk6a677vJyZQAAeJ4lwl/68175Xbt2qWLFimrWrJm+++47FtABACzJEuHftGlTpaamersMAEAp428BFI7f3+oHAADcEf4AAFgM4Q8AgMUQ/gAAWAzhDwCAxVhitT8AoPh86cN8UDyM/AEAsBhG/gAAv8f9/+4Y+QMAYDGEPwAAFkP4AwBgMYQ/AAAWQ/gDAGAxrPYHALjhfn7/R/gDACzLqrcAMu0PAIDFEP4AAFgM0/4AYFG8t29djPwBALAYRv4AAFwkv1kRf1oESPgDgJ9jeh8XI/wBACgEf7otkPAHAKAYyuIvBYS/HzBNUzk5Od4uAyhRdkmnT/9XudnZ3i6lzBgwZ7O3S8Bf3DP923y3zxnQvNTPbbPZZBhGgfsN0zTNUq8CpSo7O1sOh8PbZQAlykyya9AXpzV7a663SwHKnKysLNnt9gL3M/L3AzabTVlZWaV+nuzsbEVGRiojI+OyL6qygv74uKmRkuQ3/fG360N/fJvNZrvsfsLfDxiG4dEXq91u94sfjgvoj2+jP76N/pRNfMgPAAAWQ/gDAGAxhD8KLTAwUElJSQoMDPR2KSWC/vi++Ph4v+mPv10f+lO2sdofgG8a75Dip0vNErxdCeB3GPkDAGAxhD8AABZD+AMAYDGEPwAAFkP4AwBgMYQ/AAAWQ/gDAGAxhD8AABZD+AMAYDH8VT8AXtd71qZLts2XNGvdPq1OzWdfYksPVAX4L0b+AABYDCN/AB6T3wgfgOcx8gcAwGIIfwAALIbwBwDAYooU/h06dNCIESMK3O90OjVt2rRilnRlhmFoyZIlpX4eAAD8ESN/AAAsxpLhb5qmzp075+0y3PhiTQAA/1Tk8D937pyGDh2q0NBQhYWFaezYsTJNM9+2hw4d0l133aWQkBDZ7Xb17NlTv/76q1ubN998U3Xq1FHFihUVHR2tDz74wG3/nj171K5dOwUFBalhw4ZauXKl2/4DBw7IMAzNnz9frVu3VlBQkBo1aqSUlBRXm5SUFBmGoW+++UZxcXEKDAzUd999J9M09corr+iGG25QcHCw/va3v2nhwoWuxx0/flx9+vRRlSpVFBwcrKioKM2ZM0eSdPbsWQ0dOlQREREKCgqS0+nUyy+/7FZTenq661gnTpyQYRiuuq62JgB/3jKY3xeAwinyff7z5s3Tgw8+qB9++EFbtmxRYmKiatWqpUGDBrm1M01T3bt3V6VKlbR27VqdO3dOgwcPVq9evVwB+Nlnn+nRRx/VtGnT1KlTJ3355ZcaMGCAatSooZtvvll5eXnq0aOHKleurE2bNik7O7vANQejRo3StGnT1LBhQ02dOlV33nmn9u/fr7CwMFeb0aNHa/LkybrhhhsUGhqqsWPHavHixXrzzTcVFRWldevW6R//+IeqVKmi9u3b69lnn9WOHTv09ddfq3Llyvrll190+vRpSdKMGTO0dOlSLViwQDVr1lRGRoYyMjKK+nQWuSYAAIqryOEfGRmp5ORkGYah6Ohobd++XcnJyZeE/6pVq7Rt2zbt379fkZGRkqQPPvhAjRo10ubNm9W8eXNNnjxZCQkJGjx4sCRp5MiR2rRpkyZPnqybb75Zq1at0s6dO3XgwAHVqFFDkjRhwgTdfvvtl9Q1dOhQ3XPPPZL+nE1Yvny53n33XY0ePdrV5vnnn9ett94qSTp16pSmTp2q1atXq1WrVpKkG264QevXr9fbb7+t9u3b69ChQ2ratKni4uIk/bmg8YJDhw4pKipKN910kwzDUK1atYr6VF5VTQAAFFeRp/1btmwpwzBc37dq1Up79uzR+fPn3drt3LlTkZGRruCXpIYNGyo0NFQ7d+50tWnTpo3b49q0aeO2v2bNmq7gv3C+/Px1e/ny5RUXF+c6zgUXQlySduzYof/+97+69dZbFRIS4vp6//33tXfvXknSI488ovnz5ysmJkajR4/Whg0bXI9PSEhQenq6oqOjNXz4cK1YseIyz1rBiloTAADFVWof72uaptsvCQVtv7jNX/fnt5Ygv2MW5OK2lSpVcv0/Ly9PkvTVV1/p+uuvd2sXGBgoSbr99tt18OBBffXVV1q1apU6duyoIUOGaPLkyYqNjdX+/fv19ddfa9WqVerZs6c6deqkhQsXKiAg4JL6c3Nz862xqDUBAFBcRQ7/TZs2XfJ9VFSUypUr57a9YcOGOnTokDIyMlyj/x07digrK0sNGjSQJDVo0EDr169Xv379XI/bsGGDa/+FYxw+fFjVq1eXJG3cuLHAutq1ayfpz0WJqampGjp0aIH9aNiwoQIDA3Xo0KHLTqdXqVJFCQkJSkhIUNu2bTVq1ChNnjxZkmS329WrVy/16tVL9957r7p06aJjx46pSpUqkqTMzEw1bdpUktwW/xW3JsDXsfgO8G1FDv+MjAyNHDlSDz30kLZu3aqZM2dqypQpl7Tr1KmTmjRpoj59+mjatGmuBX/t27d3TXWPGjVKPXv2VGxsrDp27KgvvvhCixcv1qpVq1zHiI6OVr9+/TRlyhRlZ2frmWeeybeu119/XVFRUWrQoIGSk5N1/PhxDRw4sMB+2Gw2PfHEE3rssceUl5enm266SdnZ2dqwYYNCQkLUv39/jRs3Ts2aNVOjRo105swZffnll65fTJKTkxUREaGYmBgFBATo008/VXh4uEJDQxUQEKCWLVtq4sSJcjqd+v333zV27NgrPreFqQkAgOIqcvj369dPp0+fVosWLVSuXDkNGzZMiYmJl7S78Cl8w4YNU7t27RQQEKAuXbpo5syZrjbdu3fX9OnT9eqrr2r48OGqXbu25syZow4dOkiSAgIC9Nlnn+nBBx9UixYt5HQ6NWPGDHXp0uWS802cOFGTJk1SWlqa6tSpo88//1yVK1e+bF9eeOEFVa1aVS+//LL27dun0NBQxcbG6umnn5YkVaxYUU899ZQOHDig4OBgtW3bVvPnz5ckhYSEaNKkSdqzZ4/KlSun5s2ba9myZa4p//fee08DBw5UXFycoqOj9corr+i222674vN7pZoAACguwyzoJv0y4sCBA6pdu7bS0tIUExPj7XIAqGSm/ecf7qxZjke1ulLXwj8msWWxzwtYQakt+AMATyvolw5+KQDcWfLjfQEAsLIyP/J3Op0FfrwwAAC4VJkPfwDexW19QNnDtD8AABZD+AMAYDGEPwAAFkP4AwBgMYQ/AAAWw2p/AH6PD/8B3DHyBwDAYgh/AAAshml/AIXCh/kA/oORPwAAFkP4AwBgMYQ/AAAWQ/gDAGAxhD8AABbDan8AlpXfHQx88A+sgJE/AAAWw8gfgBvu5wf8HyN/AAAshvAHAMBimPYHgL/gLwDCCgh/wKJ4bx+wLsIfAAqBGQH4E8LfD5imqZycHG+XAR81YM5mb5dw1c7nnlXu6VPeLuOy7pn+bb7b5wxo7uFKgP/PZrPJMIwC9xumaZoerAelIDs7Ww6Hw9tlACXKTLJr0BenNXtrrrdLAcqcrKws2e32Avcz8vcDNptNWVlZpX6e7OxsRUZGKiMj47IvqrKC/vi4qZGS5Df98bfrQ398m81mu+x+wt8PGIbh0Rer3W73ix+OC+iPb6M/vo3+lE3c5w8AgMUQ/gAAWAzhj0ILDAxUUlKSAgMDvV1KiaA/vi8+Pt5v+uNv14f+lG2s9gfgm8Y7pPjpUrMEb1cC+B1G/gAAWAzhDwCAxRD+AABYDOEPAIDFEP4AAFgM4Q8AgMUQ/gAAWAzhDwCAxRD+AABYDOEPAIDFEP4AAFgM4Q8AgMUQ/gAAWEx5bxcAACWl96xN+W6fn9jSw5UAvo2RPwAAFsPIH0CZU9AIH0DhlOrIPyEhQd27dy9w//jx4xUTE1Ps88ydO1ehoaHFquWvDhw4IMMwlJ6eXuzaAADwNV4d+T/xxBMaNmxYkR7jdDo1YsQIjRgxokiPmz59ukzTLNJjAHjXrHX7tDqVUT5Q0rwa/iEhIQoJCfHIuRwOh0fOAwCAryuRaf+FCxeqcePGCg4OVlhYmDp16qRTp05d0i41NVVVq1bVSy+9JOnSaf8LU/OTJ09WRESEwsLCNGTIEOXm5kqSOnTooIMHD+qxxx6TYRgyDMPt+N98840aNGigkJAQdenSRZmZmZcc+4K8vDxNmjRJdevWVWBgoGrWrOmq62J5eXkaNGiQ6tWrp4MHD0qSDMPQ7Nmzdffdd+uaa65RVFSUli5d6va4HTt2qGvXrgoJCVG1atXUt29f/f7774V63lJSUtSiRQtVqlRJoaGhatOmjevcAAAUR7HDPzMzU/fff78GDhyonTt3KiUlRT169Lhkij0lJUUdO3bUc889p2eeeabA461Zs0Z79+7VmjVrNG/ePM2dO1dz586VJC1evFg1atTQ888/r8zMTLdw/9///V9NnjxZH3zwgdatW6dDhw7piSeeKPA8Tz31lCZNmqRnn31WO3bs0Mcff6xq1apd0u7s2bPq2bOntmzZovXr16tWrVqufc8995x69uypbdu2qWvXrurTp4+OHTvmel7at2+vmJgYbdmyRcuXL9evv/6qnj17XvF5O3funLp376727dtr27Zt2rhxoxITEy/5ZQdA4fSetSnfL8Cqij3tn5mZqXPnzqlHjx6uYGzcuLFbm88//1x9+/bV22+/rfvvv/+yx7v22mv12muvqVy5cqpfv766deumb7/9VoMGDdJ1112ncuXKyWazKTw83O1xubm5euutt1SnTh1J0tChQ/X888/ne46cnBxNnz5dr732mvr37y9JqlOnjm666Sa3didPnlS3bt10+vRppaSkXPLWQUJCgqs/EyZM0MyZM/Xjjz+qS5cuevPNNxUbG6sJEya42r/33nuKjIzU7t27dfLkyQKft2PHjikrK0t33HGHqz8NGjS47PMGAEBhFXvk/7e//U0dO3ZU48aNdd999+mdd97R8ePHXft/+OEH3XPPPZo3b94Vg1+SGjVqpHLlyrm+j4iI0NGjR6/4uGuuucYVlFd63M6dO3XmzBl17Njxsse8//77dfLkSa1YsSLfNQNNmjRx/b9SpUqy2Wyuc6ampmrNmjWudQ0hISGqX7++JGnv3r2Xfd6uu+46JSQkqHPnzoqPj9f06dPdZjkAACiOYod/uXLltHLlSn399ddq2LChZs6cqejoaO3fv1/SnyPq+vXr67333tPZs2eveLwKFSq4fW8YhvLy8q7qcQWt7g8ODr7i8SSpa9eu2rZtmzZtyn968HK15uXlKT4+Xunp6W5fe/bsUbt27a74vM2ZM0cbN25U69at9cknn6hevXoF1gEAQFGUyII/wzDUpk0bPffcc0pLS1PFihX12WefSZIqV66s1atXa+/everVq5dr8d7Vqlixos6fP1+sY0RFRSk4OFjffvvtZds98sgjmjhxou68806tXbu2SOeIjY3VTz/9JKfTqbp167p9VapUSdLlnzdJatq0qZ566ilt2LBBN954oz7++OOidxYAgIsUO/x/+OEHTZgwQVu2bNGhQ4e0ePFi/fbbb27vUVetWlWrV6/Wzz//rPvvv1/nzp276vM5nU6tW7dO//nPf9xWzhdFUFCQxowZo9GjR+v999/X3r17tWnTJr377ruXtB02bJhefPFF3XHHHVq/fn2hzzFkyBAdO3ZM999/v3788Uft27dPK1as0MCBA3X+/PnLPm/79+/XU089pY0bN+rgwYNasWKFdu/ezfv+AIASUewFf3a7XevWrdO0adOUnZ2tWrVqacqUKbr99tv1ySefuNqFh4dr9erV6tChg/r06XPVo9jnn39eDz30kOrUqaMzZ85c9Qf3PPvssypfvrzGjRunw4cPKyIiQg8//HC+bUeMGKG8vDx17dpVy5cvV+vWra94/OrVq+v777/XmDFj1LlzZ505c0a1atVSly5dFBAQcNnn7ddff9XPP/+sefPm6Y8//lBERISGDh2qhx566Kr6CiB/+a34548AwQoMk4+9A+CLxjs0y/GoVlfq6tHTEv6wAv6qHwAAFsNf9QPgdflOv3uhDsAqGPkDAGAxjPwB4C8K+thf1gLAnzDyBwDAYhj5A/AY/pgO4BsY+QMAYDGEPwAAFkP4AwBgMYQ/AAAWw4I/ACgEbgGEP2HkDwCAxRD+AABYDNP+AEoc9/MDvo2RPwAAFkP4AwBgMYQ/AAAWw3v+AIrF6u/vcwsgyiJG/gAAWAzhDwCAxRD+AABYDO/5A0ApYC0AfBnhD6BQrL6wD/AnhD8AeBAzAvAFhD8AN4zwAf9H+PsB0zSVk5Pj7TLgJ3JPn/J2CS7nc8/6VD2l6Z7p3+a7fc6A5h6uBP7AZrPJMIwC9xumaZoerAelIDs7Ww6Hw9tlACXKTLJr0BenNXtrrrdLAcqcrKws2e32Avcz8vcDNptNWVlZpX6e7OxsRUZGKiMj47IvqrKC/vi4qZGS5Df98bfrQ398m81mu+x+wt8PGIbh0Rer3W73ix+OC+iPb6M/vo3+lE18yA8AABZD+AMAYDGEPwotMDBQSUlJCgwM9HYpJYL++L74+Hi/6Y+/XR/6U7ax2h+AbxrvkOKnS80SvF0J4HcY+QMAYDGEPwAAFkP4AwBgMYQ/AAAWQ/gDAGAxhD8AABZD+AMAYDGEPwAAFsMf9gFQ4nrP2pTv9vmJLT1cCYD8MPIHAMBiCH8AACyGaX8AHsPbAYBvYOQPAIDFEP4AAFgM0/4AiqWgqXwAvovwB+B1+f0CMd8LdQBWwbS/pA4dOmjEiBEeOVdKSooMw9CJEyckSXPnzlVoaKhHzg0AgMTIX5K0ePFiVahQwSvn7tWrl7p27eqVcwMArInwl3Tdddd57dzBwcEKDg722vkBANbDtL/cp/2dTqcmTJiggQMHymazqWbNmpo1a5ar7dmzZzV06FBFREQoKChITqdTL7/8siTpwIEDMgxD6enprvYnTpyQYRhKSUnJ99wXT/uPHz9eMTEx+uCDD+R0OuVwONS7d2/l5OSUdLcBABZF+OdjypQpiouLU1pamgYPHqxHHnlEP//8syRpxowZWrp0qRYsWKBdu3bpww8/lNPpLNHz7927V0uWLNGXX36pL7/8UmvXrtXEiRNL9BwAAOti2j8fXbt21eDBgyVJY8aMUXJyslJSUlS/fn0dOnRIUVFRuummm2QYhmrVqlXi58/Ly9PcuXNls9kkSX379tW3336rl156qcTPBfiyWev2aXVqPncC8ImAQLEw8s9HkyZNXP83DEPh4eE6evSoJCkhIUHp6emKjo7W8OHDtWLFihI/v9PpdAW/JEVERLjODwBAcRH++bh45b9hGMrLy5MkxcbGav/+/XrhhRd0+vRp9ezZU/fee68kKSDgz6fTNE3XY3Nzc0v0/AAAFBfhfxXsdrt69eqld955R5988okWLVqkY8eOqUqVKpKkzMxMV9u/Lv4DAMAX8J5/ESUnJysiIkIxMTEKCAjQp59+qvDwcIWGhiogIEAtW7bUxIkT5XQ69fvvv2vs2LHeLhkAADeM/IsoJCREkyZNUlxcnJo3b64DBw5o2bJlrin/9957T7m5uYqLi9Ojjz6qF1980csVAwDgzjD/+gY1ABTA03/AZ/7hzprleFSrK136CZis9geKh5E/AAAWQ/gDAGAxhD8AABZD+AMAYDGEPwAAFsN9/gDKnILuPOAuAKBwGPkDAGAxhD8AABZD+AMAYDG85w/Ajac/yQ+A5zHyBwDAYgh/AAAshml/AH6DWwCBwmHkDwCAxRD+AABYDOEPAIDF8J4/YFHc0gdYFyN/AAAshvAHAMBimPYH4Pe4BRBwx8gfAACLYeQP+DkW9gG4GOEPwLLy+8WItwJgBUz7AwBgMYQ/AAAWw7S/HzBNUzk5Od4uA142YM5mb5dQ4s7nnlXu6VMePec907/Nd/ucAc09WgdQHDabTYZhFLjfME3T9GA9KAXZ2dlyOBzeLgMoUWaSXYO+OK3ZW3O9XQpQ5mRlZclutxe4n5G/H7DZbMrKyir182RnZysyMlIZGRmXfVGVFfTHx02NlCS/6Y+/XR/649tsNttl9xP+fsAwDI++WO12u1/8cFxAf3wb/fFt9KdsYsEfAAAWQ/gDAGAxhD8KLTAwUElJSQoMDPR2KSWC/vi++Ph4v+mPv10f+lO2sdofgG8a75Dip0vNErxdCeB3GPkDAGAxhD8AABZD+AMAYDGEPwAAFkP4AwBgMYQ/AAAWQ/gDAGAxhD8AABZD+AMAYDGEPwAAFkP4AwBgMYQ/AAAWQ/gDAGAxhD8AABZD+AMAYDGEPwAAFkP4l7AOHTpoxIgRJXrMuXPnKjQ0tESPCQCwLsK/DOjVq5d2797t7TIAAH6ivLcLwJUFBwcrODjY22UAKILeszblu31+YksPVwJcipF/KTh37pyGDh2q0NBQhYWFaezYsTJNU5LkdDr14osvql+/fgoJCVGtWrX0+eef67ffftNdd92lkJAQNW7cWFu2bHEdj2l/wHf1nrUp3y/AlxH+pWDevHkqX768fvjhB82YMUPJycmaPXu2a39ycrLatGmjtLQ0devWTX379lW/fv30j3/8Q1u3blXdunXVr18/1y8MAPwHvyzAFxD+pSAyMlLJycmKjo5Wnz59NGzYMCUnJ7v2d+3aVQ899JCioqI0btw45eTkqHnz5rrvvvtUr149jRkzRjt37tSvv/7qxV4AAPwV4V8KWrZsKcMwXN+3atVKe/bs0fnz5yVJTZo0ce2rVq2aJKlx48aXbDt69KgnygUAWAwL/rygQoUKrv9f+CUhv215eXmeLQyA17BAEJ7EyL8UbNq06ZLvo6KiVK5cOS9VBADA/8fIvxRkZGRo5MiReuihh7R161bNnDlTU6ZM8XZZAIqBRXnwJ4R/KejXr59Onz6tFi1aqFy5cho2bJgSExO9XRYAAJIkw+R+MgC+aLxDip8uNUvwdiWSfG/kz1oAFAfv+QMAYDGEPwAAFsN7/gDwF742vQ+UBsIfAMqg/H5JYR0ACotpfwAALIbwBwDAYgh/AAAshvAHAMBiCH8AACyG8AcAwGIIfwAALIb7/AFYlr99oE9B/eH+f1yMkT8AABZD+AMAYDFM+wPwe/42vQ8UFyN/AAAshpE/APg5FgLiYoz8AQCwGMIfAACLIfwBALAYwh8AAIthwR8Av8EtfUXDQkDrYuQPAIDFMPIHUOYwwi9dzAj4P8IfgM+atW6fVqcS9EBJI/z9gGmaysnJ8XYZwFUbMGfzJdsWSTqfe1a5p095viDk657p3+a7fc6A5h6uBFdis9lkGEaB+w3TNE0P1oNSkJ2dLYfD4e0ygBJlJtk16IvTmr0119ulAGVOVlaW7HZ7gfsZ+fsBm82mrKysUj9Pdna2IiMjlZGRcdkXVVlBf3zc1EhJ8pv++Nv1oT++zWazXXY/4e8HDMPw6IvVbrf7xQ/HBfTHt9Ef30Z/yiZu9QMAwGIIfwAALIbwR6EFBgYqKSlJgYGB3i6lRNAf3xcfH+83/fG360N/yjZW+wPwTeMdUvx0qVmCtysB/A4jfwAALIbwBwDAYgh/AAAshvAHAMBiCH8AACyG8AcAwGIIfwAALIbwBwDAYgh/AAAshr/qBwCF0HvWpny3z09s6eFKgOJj5A8AgMUw8geAYihoRqAgzBTAFzDyBwDAYgh/AAAshml/APiLok7jA2URI38AACyG8PewDh06aMSIEYVuf+TIEd16662qVKmSQkNDS60uAIB1MO3v45KTk5WZman09HQ5HA5vlwMA8AOEv4/bu3evmjVrpqioKG+XAgDwE0z7l6JTp06pX79+CgkJUUREhKZMmeK23+l06oUXXtADDzygkJAQVa9eXTNnznTbv2jRIr3//vsyDEMJCQke7gEAwB8R/qVo1KhRWrNmjT777DOtWLFCKSkpSk1NdWvz6quvqkmTJtq6daueeuopPfbYY1q5cqUkafPmzerSpYt69uypzMxMTZ8+3RvdAAD4Gab9S8nJkyf17rvv6v3339ett94qSZo3b55q1Kjh1q5NmzZ68sknJUn16tXT999/r+TkZN16662qUqWKAgMDFRwcrPDwcI/3AQDgnxj5l5K9e/fq7NmzatWqlWvbddddp+joaLd2f91/4fudO3d6pEYAgDUR/qXENM2rfqxhGCVYCQAA7pj2LyV169ZVhQoVtGnTJtWsWVOSdPz4ce3evVvt27d3tdu0yf3TxDZt2qT69et7tFYAnsOfBoYvIPxLSUhIiB588EGNGjVKYWFhqlatmp555hkFBLhPtnz//fd65ZVX1L17d61cuVKffvqpvvrqKy9VDVgLH+ULqyL8S9Grr76qkydP6s4775TNZtPjjz+urKwstzaPP/64UlNT9dxzz8lms2nKlCnq3LmzlyoGAFgB4V+KQkJC9MEHH+iDDz5wbRs1apRbG7vdrk8++aTAYyxZsqS0ygMAWBQL/gAAsBjCHwAAi2Ha34sOHDjg7RIAABbEyB8AAIsh/AEAsBjCHwAAi+E9fwDwAXzyHzyJkT8AABbDyB+A3+NjfAF3jPwBALAYwh8AAIsh/AEAsBjCHwAAi2HBHwD4MG4BRGlg5A8AgMUQ/gAAWAzT/gD8BvfzA4XDyB8AAIsh/AEAsBjCHwAAi+E9fwAog/Jb38Dtfygswh9AmcPCPqB4mPYHAMBiGPkDgJ/g0wBRWIz8AQCwGMM0TdPbRaB4TNNUTk6Ot8sASpR9aqTevOYRrQq81dul+K05A5p7uwSUEpvNJsMwCtxP+PuB7OxsORwOb5cBlCgzya5BX5zW7K253i4FKHOysrJkt9sL3M97/n7AZrMpKyur1M+TnZ2tyMhIZWRkXPZFVVbQHx83NVKS/KY//nZ96I9vs9lsl91P+PsBwzA8+mK12+1+8cNxAf3xbfTHt9GfsokFfwAAWAzhDwCAxRD+KLTAwEAlJSUpMDDQ26WUCPrj++Lj4/2mP/52fehP2cZqfwC+abxDip8uNUvwdiWA32HkDwCAxRD+AABYDOEPAIDFEP4AAFgM4Q8AgMUQ/gAAWAzhDwCAxRD+AABYDOEPAIDFEP4AAFgM4Q8AgMUQ/gAAWAzhDwCAxRD+AABYDOEPAIDFlPd2AQDgS3rP2pTv9vmJLT1cCVB6GPkDAGAxjPwBoBCYEYA/YeQPAIDFEP4lbPny5brpppsUGhqqsLAw3XHHHdq7d69r/4YNGxQTE6OgoCDFxcVpyZIlMgxD6enprjY7duxQ165dFRISomrVqqlv3776/fffvdAbAIA/Ytq/hJ06dUojR45U48aNderUKY0bN05333230tPTderUKcXHx6tr1676+OOPdfDgQY0YMcLt8ZmZmWrfvr0GDRqkqVOn6vTp0xozZox69uyp1atXe6dTgJ8qaCof8HeEfwm755573L5/9913VbVqVe3YsUPr16+XYRh65513FBQUpIYNG+o///mPBg0a5Gr/5ptvKjY2VhMmTHBte++99xQZGandu3erXr16HusLgCtjLQDKIqb9S9jevXv1wAMP6IYbbpDdblft2rUlSYcOHdKuXbvUpEkTBQUFudq3aNHC7fGpqalas2aNQkJCXF/169d3HRsAgOJi5F/C4uPjFRkZqXfeeUfVq1dXXl6ebrzxRp09e1amacowDLf2pmm6fZ+Xl6f4+HhNmjTpkmNHRESUau0AAGsg/EvQH3/8oZ07d+rtt99W27ZtJUnr16937a9fv74++ugjnTlzRoGBgZKkLVu2uB0jNjZWixYtktPpVPnyXB4AQMlj2r8EXXvttQoLC9OsWbP0yy+/aPXq1Ro5cqRr/wMPPKC8vDwlJiZq586d+uabbzR58mRJcs0IDBkyRMeOHdP999+vH3/8Ufv27dOKFSs0cOBAnT9/3iv9AgD4F4aWJSggIEDz58/X8OHDdeONNyo6OlozZsxQhw4dJEl2u11ffPGFHnnkEcXExKhx48YaN26cHnjgAdc6gOrVq+v777/XmDFj1LlzZ505c0a1atVSly5dFBDA72rA1WBVP+DOMC9+0xke9dFHH2nAgAHKyspScHCwt8sBfMd4hxQ/XWqWUOxD+VL4cxcAfAEjfw97//33dcMNN+j666/Xv/71L9c9/AQ/AMBTCH8PO3LkiMaNG6cjR44oIiJC9913n1566SVvlwUAsBDC38NGjx6t0aNHe7sMAICFsYIMAACLYeQPAB7ExwHDFzDyBwDAYgh/AAAshml/AH7Dl+7nB3wZI38AACyG8AcAwGKY9gcAH8BdAPAkRv4AAFgM4Q8AgMUQ/gAAWAzv+QMoc7ilDygewh8AfBgLAVEamPYHAMBiGPkD8Fmz1u3T6lSm+IGSxsgfAACLYeQPAGVQfmsBWAeAwmLkDwCAxTDyB+B1+Y5ivVAHYBWEPwD4CW4LRGEx7Q8AgMUw8gcAP8eMAC5G+PsB0zSVk5Pj7TKAKxowZ3Oh22afMXX6v2eUG3CqFCuytnumf5vv9jkDmnu4EpQ0m80mwzAK3G+Ypml6sB6UguzsbDkcDm+XAQDwEVlZWbLb7QXuJ/z9gKdG/tnZ2YqMjFRGRsZlX1RlBf3xbfTHt9Ef33alkT/T/n7AMAyPvljtdrtf/HBcQH98G/3xbfSnbGK1PwAAFkP4AwBgMYQ/Ci0wMFBJSUkKDAz0diklgv74Nvrj2+hP2caCPwAALIaRPwAAFkP4AwBgMYQ/AAAWQ/gDAGAxhD8AABZD+KNAL730klq3bq1rrrlGoaGhhXpMQkKCDMNw+2rZ0jf+ctjV9Mc0TY0fP17Vq1dXcHCwOnTooJ9++ql0Cy2k48ePq2/fvnI4HHI4HOrbt69OnDhx2cf42vV54403VLt2bQUFBalZs2b67rvvLtt+7dq1atasmYKCgnTDDTforbfe8lClV1aUvqSkpFxyHQzD0M8//+zBigu2bt06xcfHq3r16jIMQ0uWLLniY3z52hS1P75+fUoC4Y8CnT17Vvfdd58eeeSRIj2uS5cuyszMdH0tW7aslCosmqvpzyuvvKKpU6fqtdde0+bNmxUeHq5bb73VJ/6K4gMPPKD09HQtX75cy5cvV3p6uvr27XvFx/nK9fnkk080YsQIPfPMM0pLS1Pbtm11++2369ChQ/m2379/v7p27aq2bdsqLS1NTz/9tIYPH65FixZ5uPJLFbUvF+zatcvtWkRFRXmo4ss7deqU/va3v+m1114rVHtfvjZS0ftzga9enxJhAlcwZ84c0+FwFKpt//79zbvuuqtU6ymuwvYnLy/PDA8PNydOnOja9t///td0OBzmW2+9VYoVXtmOHTtMSeamTZtc2zZu3GhKMn/++ecCH+dL16dFixbmww8/7Latfv365pNPPplv+9GjR5v169d32/bQQw+ZLVu2LLUaC6uofVmzZo0pyTx+/LgHqiseSeZnn3122Ta+fG0uVpj+lKXrc7UY+aPEpaSkqGrVqqpXr54GDRqko0ePerukq7J//34dOXJEt912m2tbYGCg2rdvrw0bNnixMmnjxo1yOBz6+9//7trWsmVLORyOK9bmC9fn7NmzSk1NdXtuJem2224rsP6NGzde0r5z587asmWLcnNzS63WK7mavlzQtGlTRUREqGPHjlqzZk1pllmqfPXaFJe/XJ/8EP4oUbfffrs++ugjrV69WlOmTNHmzZt1yy236MyZM94urciOHDkiSapWrZrb9mrVqrn2ecuRI0dUtWrVS7ZXrVr1srX5yvX5/fffdf78+SI9t0eOHMm3/blz5/T777+XWq1XcjV9iYiI0KxZs7Ro0SItXrxY0dHR6tixo9atW+eJkkucr16bq+Vv1yc//Elfixk/fryee+65y7bZvHmz4uLirur4vXr1cv3/xhtvVFxcnGrVqqWvvvpKPXr0uKpjXk5p90fSJX8T2zTNy/6d7OIobH/yq6swtXn6+lxJUZ/b/Nrnt90bitKX6OhoRUdHu75v1aqVMjIyNHnyZLVr165U6ywtvnxtisofr8/FCH+LGTp0qHr37n3ZNk6ns8TOFxERoVq1amnPnj0ldsy/Ks3+hIeHS/pzVBMREeHafvTo0UtGOSWlsP3Ztm2bfv3110v2/fbbb0WqrbSvT0EqV66scuXKXTIyvtxzGx4enm/78uXLKywsrNRqvZKr6Ut+WrZsqQ8//LCky/MIX702JaksX5/8EP4WU7lyZVWuXNlj5/vjjz+UkZHhFp4lqTT7U7t2bYWHh2vlypVq2rSppD/f3127dq0mTZpUKucsbH9atWqlrKws/fjjj2rRooUk6YcfflBWVpZat25d6POV9vUpSMWKFdWsWTOtXLlSd999t2v7ypUrddddd+X7mFatWumLL75w27ZixQrFxcWpQoUKpVrv5VxNX/KTlpbm8etQUnz12pSksnx98uXN1YbwbQcPHjTT0tLM5557zgwJCTHT0tLMtLQ0Mycnx9UmOjraXLx4sWmappmTk2M+/vjj5oYNG8z9+/eba9asMVu1amVef/31ZnZ2tre64VLU/pimaU6cONF0OBzm4sWLze3bt5v333+/GRER4RP96dKli9mkSRNz48aN5saNG83GjRubd9xxh1sbX74+8+fPNytUqGC+++675o4dO8wRI0aYlSpVMg8cOGCapmk++eSTZt++fV3t9+3bZ15zzTXmY489Zu7YscN89913zQoVKpgLFy70eO0XK2pfkpOTzc8++8zcvXu3+X/+z/8xn3zySVOSuWjRIm91wU1OTo7r50OSOXXqVDMtLc08ePCgaZpl69qYZtH74+vXpyQQ/ihQ//79TUmXfK1Zs8bVRpI5Z84c0zRN83//93/N2267zaxSpYpZoUIFs2bNmmb//v3NQ4cOeacDFylqf0zzz9v9kpKSzPDwcDMwMNBs166duX37ds8Xn48//vjD7NOnj2mz2UybzWb26dPnkluTfP36vP7662atWrXMihUrmrGxsebatWtd+/r372+2b9/erX1KSorZtGlTs2LFiqbT6TTffPNND1dcsKL0ZdKkSWadOnXMoKAg89prrzVvuukm86uvvvJC1fm7cKvbxV/9+/c3TbPsXZui9sfXr09JMEzz/63KAAAAlsCtfgAAWAzhDwCAxRD+AABYDOEPAIDFEP4AAFgM4Q8AgMUQ/gAAWAzhDwCAxRD+AABYDOEPAIDFEP4AAFjM/wW2z6wC7NxMxAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x700 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histograms\n",
    "fig, axes = plot_tools.hist_multi_sharex(\n",
    "    np.vstack([fit1['alpha'], fit1['beta']]),\n",
    "    rowlabels=['intercept'] + list(X.columns),\n",
    "    n_bins=60,\n",
    "    x_lines=0,\n",
    "    figsize=(5, 7)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # plot histograms\n",
    "# fig, axes = plot_tools.hist_multi_sharex(\n",
    "#     [samples1['alpha']] + [sample for sample in samples1['beta'].T],\n",
    "#     rowlabels=['intercept'] + list(X.columns),\n",
    "#     n_bins=60,\n",
    "#     x_lines=0,\n",
    "#     figsize=(7, 10)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Pareto smoothed importance sampling leave-one-out cross-validation to estimate the predictive performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elpd_loo: -182.2 (SE 12.0)\n"
     ]
    }
   ],
   "source": [
    "loo1, loos1, ks1 = psis.psisloo(fit1['log_lik'].T)\n",
    "loo1_se = np.sqrt(np.var(loos1, ddof=1)*n)\n",
    "print('elpd_loo: {:.4} (SE {:.3})'.format(loo1, loo1_se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loo1, loos1, ks1 = psis.psisloo(samples1['log_lik'])\n",
    "# loo1_se = np.sqrt(np.var(loos1, ddof=1)*n)\n",
    "# print('elpd_loo: {:.4} (SE {:.3})'.format(loo1, loo1_se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of large (> 0.5) Pareto k estimates\n",
    "np.sum(ks1 > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative horseshoe prior on weights\n",
    "\n",
    "In this example, with $n \\gg p$ the difference is small, and thus we don’t expect much difference with a different prior and horseshoe prior is usually more useful for $n<p$.\n",
    "\n",
    "The global scale parameter for horseshoe prior is chosen as recommended by Juho Piironen and Aki Vehtari (2017). On the Hyperprior Choice for the Global Shrinkage Parameter in the Horseshoe Prior. Journal of Machine Learning Research: Workshop and Conference Proceedings (AISTATS 2017 Proceedings), accepted for publication. [arXiv preprint arXiv:1610.05559](http://arxiv.org/abs/1610.05559)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/**\n",
      " * Logistic regression HS-prior\n",
      " *\n",
      " * Priors:\n",
      " *     weights - hierarchical shrinkage\n",
      " *     intercept - student t\n",
      " */\n",
      "\n",
      "data {\n",
      "    int<lower=0> n;                     // number of data points\n",
      "    int<lower=1> d;                     // explanatory variable dimension\n",
      "    matrix[n, d] X;                     // explanatory variable\n",
      "    int<lower=0,upper=1> y[n];          // response variable\n",
      "    int<lower=1> p_alpha_df;            // prior alpha degrees of freedom\n",
      "    real p_alpha_loc;                   // prior alpha location\n",
      "    real<lower=0> p_alpha_scale;        // prior scale alpha\n",
      "    int<lower=1> p_beta_df;             // prior beta degrees of freedom\n",
      "    int<lower=1> p_beta_global_df;      // prior beta global degrees of freedom\n",
      "    real<lower=0> p_beta_global_scale;  // prior beta global scale\n",
      "}\n",
      "\n",
      "parameters {\n",
      "\n",
      "    // intercept\n",
      "    real alpha;\n",
      "\n",
      "    // auxiliary variables for the variance parameters\n",
      "    vector[d] z;\n",
      "    vector<lower=0>[d] lambda_r1;\n",
      "    vector<lower=0>[d] lambda_r2;\n",
      "    real<lower=0> tau_r1;\n",
      "    real<lower=0> tau_r2;\n",
      "}\n",
      "\n",
      "transformed parameters {\n",
      "\n",
      "    vector<lower=0>[d] lambda;  // local variance parameter\n",
      "    real<lower=0> tau;          // global variance parameter\n",
      "    vector[d] beta;             // explanatory variable weights\n",
      "    vector[n] eta;              // linear predictor\n",
      "\n",
      "    lambda = lambda_r1 .* sqrt(lambda_r2);\n",
      "    tau = tau_r1 * sqrt(tau_r2);\n",
      "    beta = z .* (lambda*tau);\n",
      "    eta = alpha + X * beta;\n",
      "}\n",
      "\n",
      "model {\n",
      "\n",
      "    // student t prior for intercept\n",
      "    alpha ~ student_t(p_alpha_df, p_alpha_loc, p_alpha_scale);\n",
      "\n",
      "    z ~ normal(0.0, 1.0);\n",
      "\n",
      "    // half t priors for lambdas\n",
      "    lambda_r1 ~ normal(0.0, 1.0);\n",
      "    lambda_r2 ~ inv_gamma(0.5*p_beta_df, 0.5*p_beta_df);\n",
      "\n",
      "    // half t priors for tau\n",
      "    tau_r1 ~ normal(0.0, p_beta_global_scale);\n",
      "    tau_r2 ~ inv_gamma(0.5*p_beta_global_df, 0.5*p_beta_global_df);\n",
      "\n",
      "    // observation model\n",
      "    y ~ bernoulli_logit(eta);\n",
      "}\n",
      "\n",
      "generated quantities {\n",
      "    vector[n] log_lik;\n",
      "    for (i in 1:n)\n",
      "        log_lik[i] = bernoulli_logit_lpmf(y[i] | eta[i]);\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('logistic_hs.stan') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached StanModel\n"
     ]
    }
   ],
   "source": [
    "# model = stan_utility.compile_model('logistic_hs.stan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "p0 = 2 # prior guess for the number of relevant variables\n",
    "tau0 = p0 / (p - p0) * 1 / np.sqrt(n)\n",
    "data2 = dict(\n",
    "    n=n,\n",
    "    d=p,\n",
    "    X=X.to_numpy(),\n",
    "    y=y.to_list(),\n",
    "    p_alpha_df=7,\n",
    "    p_alpha_loc=0,\n",
    "    p_alpha_scale=2.5,\n",
    "    p_beta_df=1,\n",
    "    p_beta_global_df=1,\n",
    "    p_beta_global_scale=tau0\n",
    ")\n",
    "# fit2 = model.sampling(data=data2, seed=74749)\n",
    "# samples2 = fit2.extract(permuted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mBuilding:\u001b[0m 0.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 0.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 0.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 0.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 0.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 0.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 0.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 0.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 1.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 1.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 1.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 1.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 1.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 1.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 1.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 1.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 1.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 2.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 2.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 2.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 2.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 2.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 2.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 2.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 2.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 2.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 2.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 3.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 3.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 3.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 3.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 3.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 3.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 3.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 3.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 3.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 4.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 4.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 4.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 4.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 4.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 4.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 4.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 4.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 4.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 4.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 5.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 5.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 5.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 5.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 5.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 5.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 5.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 5.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 5.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 5.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 6.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 6.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 6.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 6.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 6.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 6.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 6.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 6.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 6.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 7.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 7.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 7.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 7.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 7.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 7.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 7.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 7.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 7.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 7.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 8.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 8.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 8.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 8.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 8.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 8.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 8.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 8.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 8.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 9.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 9.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 9.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 9.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 9.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 9.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 9.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In file included from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun.hpp:124,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/multiply.hpp:7,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/elt_multiply.hpp:9,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun.hpp:55,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/rev.hpp:10,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math.hpp:19,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/model/model_header.hpp:4,\n",
      "                 from /home/jovyan/.cache/httpstan/4.9.1/models/5qtd2ev7/model_5qtd2ev7.cpp:2:\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp: In instantiation of ‘TupleT stan::math::internal::grad_2F1_impl(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; bool calc_z = true; T1 = double; T2 = double; T3 = double; T_z = double; ScalarT = double; TupleT = std::tuple<double, double, double, double>]’:\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:307:57:   required from ‘auto stan::math::grad_2F1(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool ReturnSameT = true; T1 = double; T2 = double; T3 = double; T_z = double; stan::require_t<std::integral_constant<bool, __v> >* <anonymous> = 0]’\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_inc_beta.hpp:37:25:   required from here\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:192:12: warning: unused variable ‘pre_mult’ [-Wunused-variable]\n",
      "  192 |       auto pre_mult = a2 * pow(1 - z, -1 - a2);\n",
      "      |            ^~~~~~~~\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp: In instantiation of ‘TupleT stan::math::internal::grad_2F1_impl(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; bool calc_z = true; T1 = stan::math::var_value<double>; T2 = stan::math::var_value<double>; T3 = stan::math::var_value<double>; T_z = stan::math::var_value<double>; ScalarT = stan::math::var_value<double>; TupleT = std::tuple<stan::math::var_value<double, void>, stan::math::var_value<double, void>, stan::math::var_value<double, void>, stan::math::var_value<double, void> >]’:\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:307:57:   required from ‘auto stan::math::grad_2F1(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool ReturnSameT = true; T1 = stan::math::var_value<double>; T2 = stan::math::var_value<double>; T3 = stan::math::var_value<double>; T_z = stan::math::var_value<double>; stan::require_t<std::integral_constant<bool, __v> >* <anonymous> = 0]’\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/grad_inc_beta.hpp:49:25:   required from here\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:192:12: warning: variable ‘pre_mult’ set but not used [-Wunused-but-set-variable]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 9.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 9.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 9.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 10.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 10.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.cache/httpstan/4.9.1/models/5qtd2ev7/model_5qtd2ev7.cpp: In instantiation of ‘void model_5qtd2ev7_namespace::model_5qtd2ev7::transform_inits_impl(VecVar&, VecI&, VecVar&, std::ostream*) const [with VecVar = std::vector<double, std::allocator<double> >; VecI = std::vector<int>; stan::require_vector_t<T_y>* <anonymous> = 0; stan::require_vector_like_vt<std::is_integral, VecI>* <anonymous> = 0; std::ostream = std::basic_ostream<char>]’:\n",
      "/home/jovyan/.cache/httpstan/4.9.1/models/5qtd2ev7/model_5qtd2ev7.cpp:752:26:   required from here\n",
      "/home/jovyan/.cache/httpstan/4.9.1/models/5qtd2ev7/model_5qtd2ev7.cpp:485:11: warning: variable ‘pos__’ set but not used [-Wunused-but-set-variable]\n",
      "  485 |       int pos__ = std::numeric_limits<int>::min();\n",
      "      |           ^~~~~\n",
      "In file included from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun.hpp:124,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/multiply.hpp:7,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/elt_multiply.hpp:9,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun.hpp:55,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/rev.hpp:10,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math.hpp:19,\n",
      "                 from /opt/conda/lib/python3.10/site-packages/httpstan/include/stan/model/model_header.hpp:4,\n",
      "                 from /home/jovyan/.cache/httpstan/4.9.1/models/5qtd2ev7/model_5qtd2ev7.cpp:2:\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp: In instantiation of ‘TupleT stan::math::internal::grad_2F1_impl_ab(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; T1 = double; T2 = double; T3 = double; T_z = double; ScalarT = double; TupleT = std::tuple<double, double, double>]’:\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:205:78:   required from ‘TupleT stan::math::internal::grad_2F1_impl(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; bool calc_z = true; T1 = double; T2 = double; T3 = double; T_z = double; ScalarT = double; TupleT = std::tuple<double, double, double, double>]’\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:307:57:   required from ‘auto stan::math::grad_2F1(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool ReturnSameT = true; T1 = double; T2 = double; T3 = double; T_z = double; stan::require_t<std::integral_constant<bool, __v> >* <anonymous> = 0]’\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_inc_beta.hpp:37:25:   required from here\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:68:10: warning: unused variable ‘log_precision’ [-Wunused-variable]\n",
      "   68 |   double log_precision = log(precision);\n",
      "      |          ^~~~~~~~~~~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 10.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 10.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 10.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 10.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp: In instantiation of ‘TupleT stan::math::internal::grad_2F1_impl_ab(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; T1 = stan::math::var_value<double>; T2 = stan::math::var_value<double>; T3 = stan::math::var_value<double>; T_z = stan::math::var_value<double>; ScalarT = stan::math::var_value<double>; TupleT = std::tuple<stan::math::var_value<double, void>, stan::math::var_value<double, void>, stan::math::var_value<double, void> >]’:\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:205:78:   required from ‘TupleT stan::math::internal::grad_2F1_impl(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool calc_a1 = true; bool calc_a2 = true; bool calc_b1 = true; bool calc_z = true; T1 = stan::math::var_value<double>; T2 = stan::math::var_value<double>; T3 = stan::math::var_value<double>; T_z = stan::math::var_value<double>; ScalarT = stan::math::var_value<double>; TupleT = std::tuple<stan::math::var_value<double, void>, stan::math::var_value<double, void>, stan::math::var_value<double, void>, stan::math::var_value<double, void> >]’\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:307:57:   required from ‘auto stan::math::grad_2F1(const T1&, const T2&, const T3&, const T_z&, double, int) [with bool ReturnSameT = true; T1 = stan::math::var_value<double>; T2 = stan::math::var_value<double>; T3 = stan::math::var_value<double>; T_z = stan::math::var_value<double>; stan::require_t<std::integral_constant<bool, __v> >* <anonymous> = 0]’\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/rev/fun/grad_inc_beta.hpp:49:25:   required from here\n",
      "/opt/conda/lib/python3.10/site-packages/httpstan/include/stan/math/prim/fun/grad_2F1.hpp:68:10: warning: unused variable ‘log_precision’ [-Wunused-variable]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 10.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 10.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 10.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 10.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 11.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 11.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 11.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 11.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 11.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 11.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 11.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 11.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 11.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 12.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 12.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 12.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 12.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 12.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 12.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 12.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 12.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 12.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 12.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 13.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 13.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 13.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 13.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 13.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 13.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 13.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 13.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 13.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 14.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 14.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 14.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 14.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 14.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 14.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 14.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 14.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 14.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 14.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 15.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 15.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 15.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 15.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 15.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 15.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 15.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 15.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 15.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 15.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 16.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 16.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 16.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 16.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 16.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 16.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 16.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 16.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 16.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 17.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 17.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 17.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 17.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 17.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 17.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 17.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 17.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 17.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 17.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 18.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 18.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 18.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 18.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 18.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 18.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 18.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 18.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 18.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 19.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 19.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 19.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 19.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 19.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 19.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 19.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 19.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 19.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 19.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 20.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 20.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 20.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 20.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 20.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 20.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 20.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 20.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 20.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 20.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 21.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 21.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 21.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 21.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 21.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 21.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 21.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 21.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 21.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 22.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 22.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 22.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 22.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 22.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 22.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 22.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 22.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 22.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 22.9s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 23.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 23.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 23.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 23.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 23.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 23.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 23.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 23.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 23.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 24.0s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 24.1s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 24.2s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 24.3s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 24.4s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 24.5s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 24.6s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 24.7s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 24.8s\n",
      "\u001b[1A\u001b[0J\u001b[36mBuilding:\u001b[0m 24.9s\n",
      "\u001b[1A\u001b[0J"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mBuilding:\u001b[0m 25.0s, done.\n",
      "\u001b[36mMessages from \u001b[0m\u001b[36;1mstanc\u001b[0m\u001b[36m:\u001b[0m\n",
      "Warning in '/tmp/httpstan_8_4g88os/model_5qtd2ev7.stan', line 13, column 4: Declaration\n",
      "    of arrays by placing brackets after a variable name is deprecated and\n",
      "    will be removed in Stan 2.32.0. Instead use the array keyword before the\n",
      "    type. This can be changed automatically using the auto-format flag to\n",
      "    stanc\n",
      "Warning: The parameter tau_r1 has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n",
      "Warning: The parameter alpha has no priors. This means either no prior is\n",
      "    provided, or the prior(s) depend on data variables. In the later case,\n",
      "    this may be a false positive.\n"
     ]
    }
   ],
   "source": [
    "model = stan_v3_utility.compile_model('logistic_hs.stan', data=data2, random_seed=74749)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36mSampling:\u001b[0m   0%\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m   0% (1/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m   1% (101/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m   3% (201/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m   4% (301/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m   6% (500/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m   8% (600/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  10% (800/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  12% (1000/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  15% (1200/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  18% (1400/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  20% (1600/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  22% (1800/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  26% (2100/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  29% (2300/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  32% (2600/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  36% (2900/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  52% (4200/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  70% (5600/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m  85% (6800/8000)\n",
      "\u001b[1A\u001b[0J\u001b[36mSampling:\u001b[0m 100% (8000/8000)\n",
      "\u001b[1A\u001b[0J\u001b[32mSampling:\u001b[0m 100% (8000/8000), done.\n",
      "\u001b[36mMessages received during sampling:\u001b[0m\n",
      "  Gradient evaluation took 5.1e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.51 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: bernoulli_logit_lpmf: Logit transformed probability parameter[1] is -nan, but must be not nan! (in '/tmp/httpstan_9af49hiu/model_5qtd2ev7.stan', line 64, column 4 to column 29)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 8.2e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.82 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: bernoulli_logit_lpmf: Logit transformed probability parameter[1] is -nan, but must be not nan! (in '/tmp/httpstan_9af49hiu/model_5qtd2ev7.stan', line 64, column 4 to column 29)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: bernoulli_logit_lpmf: Logit transformed probability parameter[1] is -nan, but must be not nan! (in '/tmp/httpstan_9af49hiu/model_5qtd2ev7.stan', line 64, column 4 to column 29)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: bernoulli_logit_lpmf: Logit transformed probability parameter[1] is -nan, but must be not nan! (in '/tmp/httpstan_9af49hiu/model_5qtd2ev7.stan', line 64, column 4 to column 29)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: bernoulli_logit_lpmf: Logit transformed probability parameter[1] is -nan, but must be not nan! (in '/tmp/httpstan_9af49hiu/model_5qtd2ev7.stan', line 64, column 4 to column 29)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 3.4e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.34 seconds.\n",
      "  Adjust your expectations accordingly!\n",
      "  Informational Message: The current Metropolis proposal is about to be rejected because of the following issue:\n",
      "  Exception: bernoulli_logit_lpmf: Logit transformed probability parameter[1] is -nan, but must be not nan! (in '/tmp/httpstan_9af49hiu/model_5qtd2ev7.stan', line 64, column 4 to column 29)\n",
      "  If this warning occurs sporadically, such as for highly constrained variable types like covariance matrices, then the sampler is fine,\n",
      "  but if this warning occurs often then your model may be either severely ill-conditioned or misspecified.\n",
      "  Gradient evaluation took 4.6e-05 seconds\n",
      "  1000 transitions using 10 leapfrog steps per transition would take 0.46 seconds.\n",
      "  Adjust your expectations accordingly!\n"
     ]
    }
   ],
   "source": [
    "fit2 = model.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the horseshoe prior has shrunk the posterior distribution of irrelevant features closer to zero, without affecting the posterior distribution of the relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sd</th>\n",
       "      <th>hdi_3%</th>\n",
       "      <th>hdi_97%</th>\n",
       "      <th>mcse_mean</th>\n",
       "      <th>mcse_sd</th>\n",
       "      <th>ess_bulk</th>\n",
       "      <th>ess_tail</th>\n",
       "      <th>r_hat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>-0.975</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-1.220</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.002</td>\n",
       "      <td>3132.0</td>\n",
       "      <td>2986.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z[0]</th>\n",
       "      <td>0.727</td>\n",
       "      <td>0.731</td>\n",
       "      <td>-0.736</td>\n",
       "      <td>2.107</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.013</td>\n",
       "      <td>1592.0</td>\n",
       "      <td>1895.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z[1]</th>\n",
       "      <td>1.184</td>\n",
       "      <td>0.640</td>\n",
       "      <td>0.070</td>\n",
       "      <td>2.272</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.021</td>\n",
       "      <td>239.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z[2]</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.768</td>\n",
       "      <td>-1.362</td>\n",
       "      <td>1.632</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.013</td>\n",
       "      <td>2997.0</td>\n",
       "      <td>2668.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z[3]</th>\n",
       "      <td>0.387</td>\n",
       "      <td>0.856</td>\n",
       "      <td>-1.328</td>\n",
       "      <td>1.861</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.035</td>\n",
       "      <td>358.0</td>\n",
       "      <td>152.0</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z[4]</th>\n",
       "      <td>-0.122</td>\n",
       "      <td>0.770</td>\n",
       "      <td>-1.589</td>\n",
       "      <td>1.477</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.013</td>\n",
       "      <td>2468.0</td>\n",
       "      <td>2110.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z[5]</th>\n",
       "      <td>1.020</td>\n",
       "      <td>0.624</td>\n",
       "      <td>0.052</td>\n",
       "      <td>2.200</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.009</td>\n",
       "      <td>2658.0</td>\n",
       "      <td>1687.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z[6]</th>\n",
       "      <td>0.912</td>\n",
       "      <td>0.642</td>\n",
       "      <td>0.030</td>\n",
       "      <td>2.248</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.011</td>\n",
       "      <td>1281.0</td>\n",
       "      <td>830.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z[7]</th>\n",
       "      <td>0.930</td>\n",
       "      <td>0.669</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>2.343</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.010</td>\n",
       "      <td>2190.0</td>\n",
       "      <td>2026.0</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lambda_r1[0]</th>\n",
       "      <td>0.813</td>\n",
       "      <td>0.570</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.836</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.007</td>\n",
       "      <td>2460.0</td>\n",
       "      <td>1743.0</td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  \\\n",
       "alpha        -0.975  0.141  -1.220   -0.699      0.003    0.002    3132.0   \n",
       "z[0]          0.727  0.731  -0.736    2.107      0.018    0.013    1592.0   \n",
       "z[1]          1.184  0.640   0.070    2.272      0.029    0.021     239.0   \n",
       "z[2]          0.094  0.768  -1.362    1.632      0.014    0.013    2997.0   \n",
       "z[3]          0.387  0.856  -1.328    1.861      0.049    0.035     358.0   \n",
       "z[4]         -0.122  0.770  -1.589    1.477      0.015    0.013    2468.0   \n",
       "z[5]          1.020  0.624   0.052    2.200      0.012    0.009    2658.0   \n",
       "z[6]          0.912  0.642   0.030    2.248      0.015    0.011    1281.0   \n",
       "z[7]          0.930  0.669  -0.116    2.343      0.015    0.010    2190.0   \n",
       "lambda_r1[0]  0.813  0.570   0.002    1.836      0.010    0.007    2460.0   \n",
       "\n",
       "              ess_tail  r_hat  \n",
       "alpha           2986.0   1.00  \n",
       "z[0]            1895.0   1.01  \n",
       "z[1]              60.0   1.02  \n",
       "z[2]            2668.0   1.01  \n",
       "z[3]             152.0   1.02  \n",
       "z[4]            2110.0   1.00  \n",
       "z[5]            1687.0   1.01  \n",
       "z[6]             830.0   1.00  \n",
       "z[7]            2026.0   1.00  \n",
       "lambda_r1[0]    1743.0   1.01  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print summary of selected variables\n",
    "# use pandas data frame for layout\n",
    "az.summary(fit2).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>2.5%</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>97.5%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parameters</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>alpha</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>-0.975124</td>\n",
       "      <td>0.141428</td>\n",
       "      <td>-1.514585</td>\n",
       "      <td>-1.247662</td>\n",
       "      <td>-1.075258</td>\n",
       "      <td>-0.975465</td>\n",
       "      <td>-0.875913</td>\n",
       "      <td>-0.702603</td>\n",
       "      <td>-0.520041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.1</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.191085</td>\n",
       "      <td>0.170884</td>\n",
       "      <td>-0.279482</td>\n",
       "      <td>-0.050076</td>\n",
       "      <td>0.044466</td>\n",
       "      <td>0.166135</td>\n",
       "      <td>0.313838</td>\n",
       "      <td>0.550130</td>\n",
       "      <td>0.878112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.2</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>1.139210</td>\n",
       "      <td>0.163940</td>\n",
       "      <td>0.603618</td>\n",
       "      <td>0.827988</td>\n",
       "      <td>1.028935</td>\n",
       "      <td>1.134645</td>\n",
       "      <td>1.246906</td>\n",
       "      <td>1.475723</td>\n",
       "      <td>1.746558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.3</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.014335</td>\n",
       "      <td>0.092268</td>\n",
       "      <td>-0.456975</td>\n",
       "      <td>-0.175331</td>\n",
       "      <td>-0.027982</td>\n",
       "      <td>0.005412</td>\n",
       "      <td>0.056294</td>\n",
       "      <td>0.223909</td>\n",
       "      <td>0.524847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.4</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.092531</td>\n",
       "      <td>0.145311</td>\n",
       "      <td>-0.394722</td>\n",
       "      <td>-0.118193</td>\n",
       "      <td>-0.003335</td>\n",
       "      <td>0.051505</td>\n",
       "      <td>0.172894</td>\n",
       "      <td>0.443900</td>\n",
       "      <td>0.752488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.5</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>-0.019661</td>\n",
       "      <td>0.100781</td>\n",
       "      <td>-0.552754</td>\n",
       "      <td>-0.259410</td>\n",
       "      <td>-0.065482</td>\n",
       "      <td>-0.006573</td>\n",
       "      <td>0.029372</td>\n",
       "      <td>0.173296</td>\n",
       "      <td>0.416478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.6</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.407181</td>\n",
       "      <td>0.190399</td>\n",
       "      <td>-0.239355</td>\n",
       "      <td>0.011884</td>\n",
       "      <td>0.283028</td>\n",
       "      <td>0.417462</td>\n",
       "      <td>0.535278</td>\n",
       "      <td>0.760401</td>\n",
       "      <td>1.011934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.7</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.289465</td>\n",
       "      <td>0.161537</td>\n",
       "      <td>-0.141713</td>\n",
       "      <td>-0.003434</td>\n",
       "      <td>0.178119</td>\n",
       "      <td>0.289119</td>\n",
       "      <td>0.397798</td>\n",
       "      <td>0.607167</td>\n",
       "      <td>0.795619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>beta.8</th>\n",
       "      <td>4000.0</td>\n",
       "      <td>0.311078</td>\n",
       "      <td>0.198469</td>\n",
       "      <td>-0.159226</td>\n",
       "      <td>-0.015070</td>\n",
       "      <td>0.158245</td>\n",
       "      <td>0.320490</td>\n",
       "      <td>0.456837</td>\n",
       "      <td>0.688383</td>\n",
       "      <td>1.023414</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             count      mean       std       min      2.5%       25%  \\\n",
       "parameters                                                             \n",
       "alpha       4000.0 -0.975124  0.141428 -1.514585 -1.247662 -1.075258   \n",
       "beta.1      4000.0  0.191085  0.170884 -0.279482 -0.050076  0.044466   \n",
       "beta.2      4000.0  1.139210  0.163940  0.603618  0.827988  1.028935   \n",
       "beta.3      4000.0  0.014335  0.092268 -0.456975 -0.175331 -0.027982   \n",
       "beta.4      4000.0  0.092531  0.145311 -0.394722 -0.118193 -0.003335   \n",
       "beta.5      4000.0 -0.019661  0.100781 -0.552754 -0.259410 -0.065482   \n",
       "beta.6      4000.0  0.407181  0.190399 -0.239355  0.011884  0.283028   \n",
       "beta.7      4000.0  0.289465  0.161537 -0.141713 -0.003434  0.178119   \n",
       "beta.8      4000.0  0.311078  0.198469 -0.159226 -0.015070  0.158245   \n",
       "\n",
       "                 50%       75%     97.5%       max  \n",
       "parameters                                          \n",
       "alpha      -0.975465 -0.875913 -0.702603 -0.520041  \n",
       "beta.1      0.166135  0.313838  0.550130  0.878112  \n",
       "beta.2      1.134645  1.246906  1.475723  1.746558  \n",
       "beta.3      0.005412  0.056294  0.223909  0.524847  \n",
       "beta.4      0.051505  0.172894  0.443900  0.752488  \n",
       "beta.5     -0.006573  0.029372  0.173296  0.416478  \n",
       "beta.6      0.417462  0.535278  0.760401  1.011934  \n",
       "beta.7      0.289119  0.397798  0.607167  0.795619  \n",
       "beta.8      0.320490  0.456837  0.688383  1.023414  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary2 = fit2.to_frame()\n",
    "cols_alpha_beta=[c for c in summary2.columns if 'alpha' in c or 'beta' in c]\n",
    "summary2_sub=summary2[cols_alpha_beta]#[['alpha', 'beta[0]']]\n",
    "summary2_sub.describe(percentiles=[.025, .25, .5, .75, .975]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # print summary of selected variables\n",
    "# # use pandas data frame for layout\n",
    "# summary = fit2.summary(pars=['alpha', 'beta'])\n",
    "# pd.DataFrame(\n",
    "#     summary['summary'],\n",
    "#     index=summary['summary_rownames'],\n",
    "#     columns=summary['summary_colnames']\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x700 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot histograms\n",
    "fig, axes = plot_tools.hist_multi_sharex(\n",
    "    np.vstack([fit2['alpha'], fit2['beta']]),\n",
    "    rowlabels=['intercept'] + list(X.columns),\n",
    "    n_bins=60,\n",
    "    x_lines=0,\n",
    "    figsize=(5, 7)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # plot histograms\n",
    "# fig, axes = plot_tools.hist_multi_sharex(\n",
    "#     [samples2['alpha']] + [sample for sample in samples2['beta'].T],\n",
    "#     rowlabels=['intercept'] + list(X.columns),\n",
    "#     n_bins=60,\n",
    "#     x_lines=0,\n",
    "#     figsize=(7, 10)\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute LOO also for the model with Horseshoe prior. Expected log predictive density is higher, but not significantly. This is not surprising as this is a easy data with $n \\gg p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elpd_loo: -181.8 (SE 11.1)\n"
     ]
    }
   ],
   "source": [
    "loo2, loos2, ks2 = psis.psisloo(fit2['log_lik'].T)\n",
    "loo2_se = np.sqrt(np.var(loos2, ddof=1)*n)\n",
    "print('elpd_loo: {:.4} (SE {:.3})'.format(loo2, loo2_se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loo2, loos2, ks2 = psis.psisloo(samples2['log_lik'])\n",
    "# loo2_se = np.sqrt(np.var(loos2, ddof=1)*n)\n",
    "# print('elpd_loo: {:.4} (SE {:.3})'.format(loo2, loo2_se))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the number of large (> 0.5) Pareto k estimates\n",
    "np.sum(ks2 > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elpd_diff: 0.3492 (SE 1.52)\n"
     ]
    }
   ],
   "source": [
    "elpd_diff = loos2 - loos1\n",
    "elpd_diff_se = np.sqrt(np.var(elpd_diff, ddof=1)*n)\n",
    "elpd_diff = np.sum(elpd_diff)\n",
    "print('elpd_diff: {:.4} (SE {:.3})'.format(elpd_diff, elpd_diff_se))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
