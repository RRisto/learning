{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dc0ab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import logging\n",
    "import itertools\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from scipy.special import psi, polygamma, gammaln\n",
    "\n",
    "from lda.utils import dict_from_corpus, get_random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e02a3f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588f052a",
   "metadata": {},
   "source": [
    "This example is taken from gensim, it has been modified only for learning purposes (to have everything in one page)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c75b71",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df11446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = ['comp.windows.x', 'talk.religion.misc']\n",
    "sw_nltk = stopwords.words('english')\n",
    "texts=fetch_20newsgroups(subset='train', categories=cats).data\n",
    "\n",
    "def remove_punct(text):\n",
    "    text=text.lower()\n",
    "    text=text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return text\n",
    "\n",
    "texts_toks=[remove_punct(t).split() for t in texts]\n",
    "texts_toks_nostop=[[tok for tok in text_toks if tok not in sw_nltk] for text_toks in texts_toks]\n",
    "\n",
    "common_dictionary = Dictionary(texts_toks_nostop)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts_toks_nostop]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6793f71b",
   "metadata": {},
   "source": [
    "## State class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76ff829b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LdaState():\n",
    "    \"\"\"Encapsulate information for distributed computation of :class:`~gensim.models.ldamodel.LdaModel` objects.\n",
    "\n",
    "    Objects of this class are sent over the network, so try to keep them lean to\n",
    "    reduce traffic.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta, shape, dtype=np.float32):\n",
    "        \"\"\"\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        eta : numpy.ndarray\n",
    "            The prior probabilities assigned to each term.\n",
    "        shape : tuple of (int, int)\n",
    "            Shape of the sufficient statistics: (number of topics to be found, number of terms in the vocabulary).\n",
    "        dtype : type\n",
    "            Overrides the numpy array default types.\n",
    "\n",
    "        \"\"\"\n",
    "        self.eta = eta.astype(dtype, copy=False)\n",
    "        self.sstats = np.zeros(shape, dtype=dtype)\n",
    "        self.numdocs = 0\n",
    "        self.dtype = dtype\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Prepare the state for a new EM iteration (reset sufficient stats).\"\"\"\n",
    "        self.sstats[:] = 0.0\n",
    "        self.numdocs = 0\n",
    "\n",
    "    def merge(self, other):\n",
    "        \"\"\"Merge the result of an E step from one node with that of another node (summing up sufficient statistics).\n",
    "\n",
    "        The merging is trivial and after merging all cluster nodes, we have the\n",
    "        exact same result as if the computation was run on a single node (no\n",
    "        approximation).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        other : :class:`~gensim.models.ldamodel.LdaState`\n",
    "            The state object with which the current one will be merged.\n",
    "\n",
    "        \"\"\"\n",
    "        assert other is not None\n",
    "        self.sstats += other.sstats\n",
    "        self.numdocs += other.numdocs\n",
    "\n",
    "    def blend(self, rhot, other, targetsize=None):\n",
    "        \"\"\"Merge the current state with another one using a weighted average for the sufficient statistics.\n",
    "\n",
    "        The number of documents is stretched in both state objects, so that they are of comparable magnitude.\n",
    "        This procedure corresponds to the stochastic gradient update from\n",
    "        `'Online Learning for LDA' by Hoffman et al.`_, see equations (5) and (9).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rhot : float\n",
    "            Weight of the `other` state in the computed average. A value of 0.0 means that `other`\n",
    "            is completely ignored. A value of 1.0 means `self` is completely ignored.\n",
    "        other : :class:`~gensim.models.ldamodel.LdaState`\n",
    "            The state object with which the current one will be merged.\n",
    "        targetsize : int, optional\n",
    "            The number of documents to stretch both states to.\n",
    "\n",
    "        \"\"\"\n",
    "        assert other is not None\n",
    "        if targetsize is None:\n",
    "            targetsize = self.numdocs\n",
    "\n",
    "        # stretch the current model's expected n*phi counts to target size\n",
    "        if self.numdocs == 0 or targetsize == self.numdocs:\n",
    "            scale = 1.0\n",
    "        else:\n",
    "            scale = 1.0 * targetsize / self.numdocs\n",
    "        self.sstats *= (1.0 - rhot) * scale\n",
    "\n",
    "        # stretch the incoming n*phi counts to target size\n",
    "        if other.numdocs == 0 or targetsize == other.numdocs:\n",
    "            scale = 1.0\n",
    "        else:\n",
    "            logger.info(\"merging changes from %i documents into a model of %i documents\", other.numdocs, targetsize)\n",
    "            scale = 1.0 * targetsize / other.numdocs\n",
    "        self.sstats += rhot * scale * other.sstats\n",
    "\n",
    "        self.numdocs = targetsize\n",
    "\n",
    "    def blend2(self, rhot, other, targetsize=None):\n",
    "        \"\"\"Merge the current state with another one using a weighted sum for the sufficient statistics.\n",
    "\n",
    "        In contrast to :meth:`~gensim.models.ldamodel.LdaState.blend`, the sufficient statistics are not scaled\n",
    "        prior to aggregation.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        rhot : float\n",
    "            Unused.\n",
    "        other : :class:`~gensim.models.ldamodel.LdaState`\n",
    "            The state object with which the current one will be merged.\n",
    "        targetsize : int, optional\n",
    "            The number of documents to stretch both states to.\n",
    "\n",
    "        \"\"\"\n",
    "        assert other is not None\n",
    "        if targetsize is None:\n",
    "            targetsize = self.numdocs\n",
    "\n",
    "        # merge the two matrices by summing\n",
    "        self.sstats += other.sstats\n",
    "        self.numdocs = targetsize\n",
    "\n",
    "    def get_lambda(self):\n",
    "        \"\"\"Get the parameters of the posterior over the topics, also referred to as \"the topics\".\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Parameters of the posterior probability over topics.\n",
    "\n",
    "        \"\"\"\n",
    "        return self.eta + self.sstats\n",
    "\n",
    "    def get_Elogbeta(self):\n",
    "        \"\"\"Get the log (posterior) probabilities for each topic.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        numpy.ndarray\n",
    "            Posterior probabilities for each topic.\n",
    "        \"\"\"\n",
    "        return dirichlet_expectation(self.get_lambda())\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, fname, *args, **kwargs):\n",
    "        \"\"\"Load a previously stored state from disk.\n",
    "\n",
    "        Overrides :class:`~gensim.utils.SaveLoad.load` by enforcing the `dtype` parameter\n",
    "        to ensure backwards compatibility.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        fname : str\n",
    "            Path to file that contains the needed object.\n",
    "        args : object\n",
    "            Positional parameters to be propagated to class:`~gensim.utils.SaveLoad.load`\n",
    "        kwargs : object\n",
    "            Key-word parameters to be propagated to class:`~gensim.utils.SaveLoad.load`\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        :class:`~gensim.models.ldamodel.LdaState`\n",
    "            The state loaded from the given file.\n",
    "\n",
    "        \"\"\"\n",
    "        result = super(LdaState, cls).load(fname, *args, **kwargs)\n",
    "\n",
    "        # dtype could be absent in old models\n",
    "        if not hasattr(result, 'dtype'):\n",
    "            result.dtype = np.float64  # float64 was implicitly used before (because it's the default in numpy)\n",
    "            logging.info(\"dtype was not set in saved %s file %s, assuming np.float64\", result.__class__.__name__, fname)\n",
    "\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565be4c0",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "252fc829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dirichlet_expectation(alpha):\n",
    "    \"\"\"Expected value of log(theta) where theta is drawn from a Dirichlet distribution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : numpy.ndarray\n",
    "        Dirichlet parameter 2d matrix or 1d vector, if 2d - each row is treated as a separate parameter vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Log of expected values, dimension same as `alpha.ndim`.\n",
    "\n",
    "    \"\"\"\n",
    "    if len(alpha.shape) == 1:\n",
    "        result = psi(alpha) - psi(np.sum(alpha))\n",
    "    else:\n",
    "        result = psi(alpha) - psi(np.sum(alpha, 1))[:, np.newaxis]\n",
    "    return result.astype(alpha.dtype, copy=False)  # keep the same precision as input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdda002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkize_serial(iterable, chunksize, as_numpy=False, dtype=np.float32):\n",
    "    \"\"\"Yield elements from `iterable` in \"chunksize\"-ed groups.\n",
    "\n",
    "    The last returned element may be smaller if the length of collection is not divisible by `chunksize`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    iterable : iterable of object\n",
    "        An iterable.\n",
    "    chunksize : int\n",
    "        Split iterable into chunks of this size.\n",
    "    as_numpy : bool, optional\n",
    "        Yield chunks as `np.ndarray` instead of lists.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    list OR np.ndarray\n",
    "        \"chunksize\"-ed chunks of elements from `iterable`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    .. sourcecode:: pycon\n",
    "\n",
    "        >>> print(list(grouper(range(10), 3)))\n",
    "        [[0, 1, 2], [3, 4, 5], [6, 7, 8], [9]]\n",
    "\n",
    "    \"\"\"\n",
    "    it = iter(iterable)\n",
    "    while True:\n",
    "        if as_numpy:\n",
    "            # convert each document to a 2d numpy array (~6x faster when transmitting\n",
    "            # chunk data over the wire, in Pyro)\n",
    "            wrapped_chunk = [[np.array(doc, dtype=dtype) for doc in itertools.islice(it, int(chunksize))]]\n",
    "        else:\n",
    "            wrapped_chunk = [list(itertools.islice(it, int(chunksize)))]\n",
    "        if not wrapped_chunk[0]:\n",
    "            break\n",
    "        # memory opt: wrap the chunk and then pop(), to avoid leaving behind a dangling reference\n",
    "        yield wrapped_chunk.pop()\n",
    "        \n",
    "grouper=chunkize_serial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8daee148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_difference(a, b):\n",
    "    \"\"\"Mean absolute difference between two arrays.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    a : numpy.ndarray\n",
    "        Input 1d array.\n",
    "    b : numpy.ndarray\n",
    "        Input 1d array.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        mean(abs(a - b)).\n",
    "\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(a - b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfccfe9b",
   "metadata": {},
   "source": [
    "## Init model params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4569780",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dict_from_corpus(common_corpus)\n",
    "num_terms = len(id2word)\n",
    "num_topics = 3\n",
    "chunksize = 2000\n",
    "decay = 0.5\n",
    "offset = 1.0\n",
    "minimum_probability = 0.01\n",
    "num_updates = 0\n",
    "\n",
    "passes = 10\n",
    "update_every = 1\n",
    "eval_every = 10\n",
    "minimum_phi_value = 0.01\n",
    "per_word_topics = False\n",
    "\n",
    "dtype = np.finfo(np.float32).dtype\n",
    "iterations = 50\n",
    "gamma_threshold = 0.001\n",
    "\n",
    "random_state = get_random_state(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70c3db7",
   "metadata": {},
   "source": [
    "#### init alpha and eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4679a1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple uniform priors\n",
    "alpha =  init_prior = np.fromiter(\n",
    "            (1.0 / num_topics for i in range(num_topics)),\n",
    "            dtype=dtype, count=num_topics,\n",
    "        )\n",
    "eta =  init_prior = np.fromiter(\n",
    "            (1.0 / num_topics for i in range(num_terms)),\n",
    "            dtype=dtype, count=num_terms,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3a17a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333334, 0.33333334, 0.33333334], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "006f02d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333334, 0.33333334, 0.33333334, ..., 0.33333334, 0.33333334,\n",
       "       0.33333334], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119e055d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24296,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b3e210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the variational distribution q(beta|lambda)\n",
    "state = LdaState(eta, (num_topics, num_terms), dtype=dtype)\n",
    "state.sstats= np.zeros((num_topics, num_terms), dtype=dtype)\n",
    "state.sstats[...] = np.random.mtrand._rand.gamma(100., 1. / 100., (num_topics, num_terms))\n",
    "expElogbeta = np.exp(dirichlet_expectation(state.sstats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0c29d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 24296)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.sstats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15294688",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0780917 , 1.1127311 , 0.90114397, ..., 1.0393286 , 0.90416795,\n",
       "        1.0334735 ],\n",
       "       [1.003914  , 0.9444049 , 1.1655728 , ..., 0.88193494, 1.1274736 ,\n",
       "        1.0421051 ],\n",
       "       [0.9466089 , 0.88825935, 1.1553314 , ..., 1.1808109 , 1.0925727 ,\n",
       "        1.0331155 ]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.sstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fd2b966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 24296)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expElogbeta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71fd933",
   "metadata": {},
   "source": [
    "## More helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36d9ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_estep(chunk, expElogbeta, dtype, num_topics,iterations, alpha, gamma_threshold, state=None):\n",
    "    \"\"\"Perform inference on a chunk of documents, and accumulate the collected sufficient statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk : list of list of (int, float)\n",
    "        The corpus chunk on which the inference step will be performed.\n",
    "    state : :class:`~gensim.models.ldamodel.LdaState`, optional\n",
    "        The state to be updated with the newly accumulated sufficient statistics. If none, the models\n",
    "        `self.state` is updated.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Gamma parameters controlling the topic weights, shape (`len(chunk)`, `self.num_topics`).\n",
    "\n",
    "    \"\"\"\n",
    "#     gamma, sstats = inference(chunk, collect_sstats=True)\n",
    "    gamma, sstats =  inference(chunk, expElogbeta, dtype, num_topics,iterations, alpha, gamma_threshold,\n",
    "              collect_sstats=True)\n",
    "    state.sstats += sstats\n",
    "    state.numdocs += gamma.shape[0]  # avoids calling len(chunk) on a generator\n",
    "    assert gamma.dtype == dtype\n",
    "    return gamma, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "367898f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(chunk, expElogbeta, dtype, num_topics,iterations, alpha, gamma_threshold,\n",
    "              collect_sstats=False):\n",
    "    \"\"\"Given a chunk of sparse document vectors, estimate gamma (parameters controlling the topic weights)\n",
    "    for each document in the chunk.\n",
    "\n",
    "    This function does not modify the model. The whole input chunk of document is assumed to fit in RAM;\n",
    "    chunking of a large corpus must be done earlier in the pipeline. Avoids computing the `phi` variational\n",
    "    parameter directly using the optimization presented in\n",
    "    `Lee, Seung: Algorithms for non-negative matrix factorization\"\n",
    "    <https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf>`_.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    chunk : list of list of (int, float)\n",
    "        The corpus chunk on which the inference step will be performed.\n",
    "    collect_sstats : bool, optional\n",
    "        If set to True, also collect (and return) sufficient statistics needed to update the model's topic-word\n",
    "        distributions.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (numpy.ndarray, {numpy.ndarray, None})\n",
    "        The first element is always returned and it corresponds to the states gamma matrix. The second element is\n",
    "        only returned if `collect_sstats` == True and corresponds to the sufficient statistics for the M step.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        len(chunk)\n",
    "    except TypeError:\n",
    "        # convert iterators/generators to plain list, so we have len() etc.\n",
    "        chunk = list(chunk)\n",
    "\n",
    "\n",
    "    # Initialize the variational distribution q(theta|gamma) for the chunk\n",
    "#     gamma = random_state.gamma(100., 1. / 100., (len(chunk), num_topics)).astype(dtype, copy=False)\n",
    "    gamma =  np.random.mtrand._rand.gamma(100., 1. / 100., (len(chunk), num_topics)).astype(dtype, copy=False)\n",
    "    Elogtheta = dirichlet_expectation(gamma)\n",
    "    expElogtheta = np.exp(Elogtheta)\n",
    "\n",
    "    assert Elogtheta.dtype == dtype\n",
    "    assert expElogtheta.dtype == dtype\n",
    "\n",
    "    if collect_sstats:\n",
    "        sstats = np.zeros_like(expElogbeta, dtype=dtype)\n",
    "    else:\n",
    "        sstats = None\n",
    "    converged = 0\n",
    "\n",
    "    # Now, for each document d update that document's gamma and phi\n",
    "    # Inference code copied from Hoffman's `onlineldavb.py` (esp. the\n",
    "    # Lee&Seung trick which speeds things up by an order of magnitude, compared\n",
    "    # to Blei's original LDA-C code, cool!).\n",
    "    integer_types = (int, np.integer,)\n",
    "    epsilon = np.finfo(dtype).eps\n",
    "    for d, doc in enumerate(chunk):\n",
    "        if len(doc) > 0 and not isinstance(doc[0][0], integer_types):\n",
    "            # make sure the term IDs are ints, otherwise np will get upset\n",
    "            ids = [int(idx) for idx, _ in doc]\n",
    "        else:\n",
    "            ids = [idx for idx, _ in doc]\n",
    "        cts = np.fromiter((cnt for _, cnt in doc), dtype=dtype, count=len(doc))\n",
    "        gammad = gamma[d, :]\n",
    "        Elogthetad = Elogtheta[d, :]\n",
    "        expElogthetad = expElogtheta[d, :]\n",
    "        #expElogbeta is global!\n",
    "        expElogbetad = expElogbeta[:, ids]\n",
    "\n",
    "        # The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_kw.\n",
    "        # phinorm is the normalizer.\n",
    "        # TODO treat zeros explicitly, instead of adding epsilon?\n",
    "        phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n",
    "\n",
    "        # Iterate between gamma and phi until convergence\n",
    "        for _ in range(iterations):\n",
    "            lastgamma = gammad\n",
    "            # We represent phi implicitly to save memory and time.\n",
    "            # Substituting the value of the optimal phi back into\n",
    "            # the update for gamma gives this update. Cf. Lee&Seung 2001.\n",
    "            gammad = alpha + expElogthetad * np.dot(cts / phinorm, expElogbetad.T)\n",
    "            Elogthetad = dirichlet_expectation(gammad)\n",
    "            expElogthetad = np.exp(Elogthetad)\n",
    "            phinorm = np.dot(expElogthetad, expElogbetad) + epsilon\n",
    "            # If gamma hasn't changed much, we're done.\n",
    "            meanchange = mean_absolute_difference(gammad, lastgamma)\n",
    "            if meanchange < gamma_threshold:\n",
    "                converged += 1\n",
    "                break\n",
    "        gamma[d, :] = gammad\n",
    "        assert gammad.dtype == dtype\n",
    "        if collect_sstats:\n",
    "            # Contribution of document d to the expected sufficient\n",
    "            # statistics for the M step.\n",
    "            sstats[:, ids] += np.outer(expElogthetad.T, cts / phinorm)\n",
    "\n",
    "#     if len(chunk) > 1:\n",
    "#         logger.debug(\"%i/%i documents converged within %i iterations\", converged, len(chunk), self.iterations)\n",
    "\n",
    "    if collect_sstats:\n",
    "        # This step finishes computing the sufficient statistics for the\n",
    "        # M step, so that\n",
    "        # sstats[k, w] = \\sum_d n_{dw} * phi_{dwk}\n",
    "        # = \\sum_d n_{dw} * exp{Elogtheta_{dk} + Elogbeta_{kw}} / phinorm_{dw}.\n",
    "        sstats *= expElogbeta\n",
    "        assert sstats.dtype == dtype\n",
    "\n",
    "    assert gamma.dtype == dtype\n",
    "    return gamma, sstats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6203280",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_mstep(rho, state, other,random_state, num_updates, alpha, id2word, extra_pass=False):\n",
    "    \"\"\"Maximization step: use linear interpolation between the existing topics and\n",
    "    collected sufficient statistics in `other` to update the topics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rho : float\n",
    "        Learning rate.\n",
    "    other : :class:`~gensim.models.ldamodel.LdaModel`\n",
    "        The model whose sufficient statistics will be used to update the topics.\n",
    "    extra_pass : bool, optional\n",
    "        Whether this step required an additional pass over the corpus.\n",
    "\n",
    "    \"\"\"\n",
    "    logger.debug(\"updating topics\")\n",
    "    # update self with the new blend; also keep track of how much did\n",
    "    # the topics change through this update, to assess convergence\n",
    "    previous_Elogbeta = state.get_Elogbeta()\n",
    "    state.blend(rho, other)\n",
    "\n",
    "    current_Elogbeta = state.get_Elogbeta()\n",
    "#     sync_state(current_Elogbeta)\n",
    "    expElogbeta=sync_state(state, dtype, current_Elogbeta=current_Elogbeta)\n",
    "\n",
    "    # print out some debug info at the end of each EM iteration\n",
    "    # self.print_topics(5)\n",
    "    show_topics(num_topics=num_topics, num_words=20, alpha=alpha, id2word=id2word, random_state=random_state, state=state,\n",
    "                log=True)\n",
    "    diff = mean_absolute_difference(previous_Elogbeta.ravel(), current_Elogbeta.ravel())\n",
    "    logger.info(\"topic diff=%f, rho=%f\", diff, rho)\n",
    "\n",
    "    # if self.optimize_eta:\n",
    "    #     self.update_eta(self.state.get_lambda(), rho)\n",
    "\n",
    "    if not extra_pass:\n",
    "        # only update if this isn't an additional pass\n",
    "        num_updates += other.numdocs\n",
    "    return num_updates, expElogbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "004053a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topics(num_topics, num_words, alpha, id2word, random_state, state, log=False, formatted=True):\n",
    "    \"\"\"Get a representation for selected topics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_topics : int, optional\n",
    "        Number of topics to be returned. Unlike LSA, there is no natural ordering between the topics in LDA.\n",
    "        The returned topics subset of all topics is therefore arbitrary and may change between two LDA\n",
    "        training runs.\n",
    "    num_words : int, optional\n",
    "        Number of words to be presented for each topic. These will be the most relevant words (assigned the highest\n",
    "        probability for each topic).\n",
    "    log : bool, optional\n",
    "        Whether the output is also logged, besides being returned.\n",
    "    formatted : bool, optional\n",
    "        Whether the topic representations should be formatted as strings. If False, they are returned as\n",
    "        2 tuples of (word, probability).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of {str, tuple of (str, float)}\n",
    "        a list of topics, each represented either as a string (when `formatted` == True) or word-probability\n",
    "        pairs.\n",
    "\n",
    "    \"\"\"\n",
    "#     if num_topics < 0 or num_topics >= self.num_topics:\n",
    "#         num_topics = self.num_topics\n",
    "#         chosen_topics = range(num_topics)\n",
    "#     else:\n",
    "#         num_topics = min(num_topics, self.num_topics)\n",
    "\n",
    "    # add a little random jitter, to randomize results around the same alpha\n",
    "    sort_alpha = alpha + 0.0001 * random_state.rand(len(alpha))\n",
    "    # random_state.rand returns float64, but converting back to dtype won't speed up anything\n",
    "\n",
    "    sorted_topics = list(argsort(sort_alpha))\n",
    "    chosen_topics = sorted_topics[:num_topics // 2] + sorted_topics[-num_topics // 2:]\n",
    "\n",
    "    shown = []\n",
    "\n",
    "    topic = state.get_lambda()\n",
    "    for i in chosen_topics:\n",
    "        topic_ = topic[i]\n",
    "        topic_ = topic_ / topic_.sum()  # normalize to probability distribution\n",
    "        bestn = argsort(topic_, num_words, reverse=True)\n",
    "        topic_ = [(id2word[id], topic_[id]) for id in bestn]\n",
    "        if formatted:\n",
    "            topic_ = ' + '.join('%.3f*\"%s\"' % (v, k) for k, v in topic_)\n",
    "\n",
    "        shown.append((i, topic_))\n",
    "        logger.info(\"topic #%i (%.3f): %s\", i, alpha[i], topic_)\n",
    "\n",
    "    return shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50721645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sync_state(state, dtype, current_Elogbeta=None):\n",
    "    \"\"\"Propagate the states topic probabilities to the inner object's attribute.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    current_Elogbeta: numpy.ndarray\n",
    "        Posterior probabilities for each topic, optional.\n",
    "        If omitted, it will get Elogbeta from state.\n",
    "\n",
    "    \"\"\"\n",
    "    if current_Elogbeta is None:\n",
    "        current_Elogbeta = state.get_Elogbeta()\n",
    "    expElogbeta = np.exp(current_Elogbeta)\n",
    "    assert expElogbeta.dtype == dtype\n",
    "    return expElogbeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59f72324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def argsort(x, topn=None, reverse=False):\n",
    "    \"\"\"Efficiently calculate indices of the `topn` smallest elements in array `x`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array_like\n",
    "        Array to get the smallest element indices from.\n",
    "    topn : int, optional\n",
    "        Number of indices of the smallest (greatest) elements to be returned.\n",
    "        If not given, indices of all elements will be returned in ascending (descending) order.\n",
    "    reverse : bool, optional\n",
    "        Return the `topn` greatest elements in descending order,\n",
    "        instead of smallest elements in ascending order?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Array of `topn` indices that sort the array in the requested order.\n",
    "\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)  # unify code path for when `x` is not a np array (list, tuple...)\n",
    "    if topn is None:\n",
    "        topn = x.size\n",
    "    if topn <= 0:\n",
    "        return []\n",
    "    if reverse:\n",
    "        x = -x\n",
    "    if topn >= x.size or not hasattr(np, 'argpartition'):\n",
    "        return np.argsort(x)[:topn]\n",
    "    # np >= 1.8 has a fast partial argsort, use that!\n",
    "    most_extreme = np.argpartition(x, topn)[:topn]\n",
    "    return most_extreme.take(np.argsort(x.take(most_extreme)))  # resort topn into order\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45583b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_terms(state, topicid, topn=10):\n",
    "    \"\"\"Get the representation for a single topic. Words the integer IDs, in constrast to\n",
    "    :meth:`~gensim.models.ldamodel.LdaModel.show_topic` that represents words by the actual strings.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    topicid : int\n",
    "        The ID of the topic to be returned\n",
    "    topn : int, optional\n",
    "        Number of the most significant words that are associated with the topic.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of (int, float)\n",
    "        Word ID - probability pairs for the most relevant words generated by the topic.\n",
    "\n",
    "    \"\"\"\n",
    "    topic = get_topics(state)[topicid]\n",
    "    topic = topic / topic.sum()  # normalize to probability distribution\n",
    "    bestn = argsort(topic, topn, reverse=True)\n",
    "    return [(idx, topic[idx]) for idx in bestn]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7582ae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics(state):\n",
    "    \"\"\"Get the term-topic matrix learned during inference.\n",
    "\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The probability for each word in each topic, shape (`num_topics`, `vocabulary_size`).\n",
    "\n",
    "    \"\"\"\n",
    "    topics = state.get_lambda()\n",
    "    return topics / topics.sum(axis=1)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e6c763",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6f0223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lencorpus = len(common_corpus)\n",
    "chunksize = min(lencorpus, chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "55dbff6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "970"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numworkers=1\n",
    "updateafter = min(lencorpus, update_every * numworkers * chunksize)\n",
    "updateafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dfd39c32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "970"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evalafter = min(lencorpus, (eval_every or 0) * numworkers * chunksize)\n",
    "evalafter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5bf4cc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updates_per_pass = max(1, lencorpus / updateafter)\n",
    "updates_per_pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88feb3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_as_numpy=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe472023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rho is the \"speed\" of updating; TODO try other fncs\n",
    "# pass_ + num_updates handles increasing the starting t for each pass,\n",
    "# while allowing it to \"reset\" on the first pass of each update\n",
    "def rho():\n",
    "    return pow(offset + pass_ + (num_updates / chunksize), -decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32053746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26b46870",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pass_ in range(passes):\n",
    "    #todo! can get without this class?\n",
    "    other = LdaState(eta, state.sstats.shape, dtype)\n",
    "    dirty = False\n",
    "    reallen = 0\n",
    "    chunks = grouper(common_corpus, chunksize, as_numpy=chunks_as_numpy, dtype=dtype)\n",
    "    for chunk_no, chunk in enumerate(chunks):\n",
    "        reallen += len(chunk)  # keep track of how many documents we've processed so far\n",
    "        gammat, state = do_estep(chunk, expElogbeta, dtype, num_topics,iterations, alpha, gamma_threshold, other)\n",
    "        dirty = True\n",
    "        del chunk\n",
    "        # perform an M step. determine when based on update_every, don't do this after every chunk\n",
    "        if update_every and (chunk_no + 1) % (update_every * numworkers) == 0:\n",
    "            num_updates, expElogbeta=do_mstep(rho(), state, other,random_state, num_updates, alpha, id2word, pass_ > 0)\n",
    "            del other \n",
    "            other = LdaState(eta, state.sstats.shape, dtype)\n",
    "    if dirty:\n",
    "        # finish any remaining updates\n",
    "        num_updates, expElogbeta=do_mstep(rho(), state, other,random_state, num_updates, alpha, id2word, pass_ > 0)\n",
    "        del other\n",
    "        dirty = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53072a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "topic nr 0\n",
      "x\n",
      "file\n",
      "entry\n",
      "program\n",
      "0\n",
      "c\n",
      "output\n",
      "entries\n",
      "oname\n",
      "rules\n",
      "\n",
      "topic nr 1\n",
      "one\n",
      "subject\n",
      "lines\n",
      "organization\n",
      "would\n",
      "people\n",
      "god\n",
      "writes\n",
      "jesus\n",
      "article\n",
      "\n",
      "topic nr 2\n",
      "x\n",
      "window\n",
      "subject\n",
      "lines\n",
      "organization\n",
      "server\n",
      "use\n",
      "get\n",
      "motif\n",
      "available\n"
     ]
    }
   ],
   "source": [
    "for topic_id in range(3):\n",
    "    print()\n",
    "    print(f'topic nr {topic_id}')\n",
    "    topic_words=get_topic_terms(state, topic_id)\n",
    "    for word_weight in topic_words:\n",
    "        print(common_dictionary[word_weight[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba4ec58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
