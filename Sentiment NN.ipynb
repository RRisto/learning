{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel('data/sentiment_data/training_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y                                               text\n",
       "0  positive            The Da Vinci Code book is just awesome.\n",
       "1  positive  this was the first clive cussler i've ever rea...\n",
       "2  positive                   i liked the Da Vinci Code a lot.\n",
       "3  positive                   i liked the Da Vinci Code a lot.\n",
       "4  positive  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    '''generic class for network building. Exact implementation is in subclasses'''\n",
    "    \n",
    "    def __init__(self,x, y, nn_structure, test_size):\n",
    "        \"\"\"initiate NN with x and y texts, also clean data and turn it into matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X=x #matrix\n",
    "        self.y=y #matrix\n",
    "        self.nn_structure=nn_structure #list of nr of in each layer\n",
    "    \n",
    "        #split test and trainig data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=test_size)\n",
    "        #vectorize y\n",
    "        self.y_v_train=self.convert_y_to_vect(self.y_train)\n",
    "        self.y_v_test=self.convert_y_to_vect(self.y_test)\n",
    "        self.y_pred=None\n",
    "        \n",
    "        self.avg_cost_func=[]\n",
    "        self.W=None\n",
    "        self.b=None\n",
    "        \n",
    "                 \n",
    "    def convert_y_to_vect(self, y):\n",
    "        y_vect = np.zeros((len(y), len(self.y_dictionary)))\n",
    "        for i in range(len(y)):\n",
    "            y_vect[i, y[i]] = 1\n",
    "        return y_vect\n",
    "\n",
    "    \n",
    "    def f(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        \n",
    "    def f_deriv(self,x):\n",
    "        return self.f(x) * (1 - self.f(x))\n",
    "    \n",
    "    \n",
    "    def setup_and_init_weights(self):\n",
    "        W = {}\n",
    "        b = {}\n",
    "        for l in range(1, len(self.nn_structure)):\n",
    "            W[l] = np.random.random_sample((self.nn_structure[l], self.nn_structure[l-1]))\n",
    "            b[l] = np.random.random_sample((self.nn_structure[l],))\n",
    "        return W, b\n",
    "    \n",
    "    \n",
    "    def init_tri_values(self):\n",
    "        tri_W = {}\n",
    "        tri_b = {}\n",
    "        for l in range(1, len(self.nn_structure)):\n",
    "            tri_W[l] = np.zeros((self.nn_structure[l], self.nn_structure[l-1]))\n",
    "            tri_b[l] = np.zeros((self.nn_structure[l],))\n",
    "        return tri_W, tri_b\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, x, W, b):\n",
    "        h = {1: x}\n",
    "        z = {}\n",
    "        for l in range(1, len(W) + 1):\n",
    "            # if it is the first layer, then the input into the weights is x, otherwise, \n",
    "            # it is the output from the last layer\n",
    "            if l == 1:\n",
    "                node_in = x\n",
    "            else:\n",
    "                node_in = h[l]\n",
    "            z[l+1] = W[l].dot(node_in) + b[l] # z^(l+1) = W^(l)*h^(l) + b^(l)  \n",
    "            h[l+1] = self.f(z[l+1]) # h^(l) = f(z^(l)) \n",
    "        return h, z\n",
    "    \n",
    "    \n",
    "    def calculate_out_layer_delta(self, y, h_out, z_out):\n",
    "        # delta^(nl) = -(y_i - h_i^(nl)) * f'(z_i^(nl))\n",
    "        return -(y-h_out) * self.f_deriv(z_out)\n",
    "    \n",
    "    \n",
    "    def calculate_hidden_delta(self, delta_plus_1, w_l, z_l):\n",
    "        # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "        return np.dot(np.transpose(w_l), delta_plus_1) * self.f_deriv(z_l)\n",
    "    \n",
    "    \n",
    "    def train_nn(self):\n",
    "        '''implemented in subclass'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "    def plot_avg_cost_func(self):\n",
    "        if len(self.avg_cost_func)==0:\n",
    "            print('Please train model before visalizing average cost')\n",
    "            return\n",
    "        plt.plot(self.avg_cost_func)\n",
    "        plt.ylabel('Average J')\n",
    "        plt.xlabel('Iteration number')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict_test_data(self):\n",
    "        m = self.X_test.shape[0]\n",
    "        y = np.zeros((m,))\n",
    "        for i in range(m):\n",
    "            h, z = self.feed_forward(self.X_test[i, :], self.W, self.b)\n",
    "            y[i] = np.argmax(h[len(self.nn_structure)])\n",
    "        self.y_pred=y\n",
    "        \n",
    "        \n",
    "    def get_test_accuracy(self):\n",
    "        self.predict_test_data()\n",
    "        return accuracy_score(self.y_test, self.y_pred)*100\n",
    "    \n",
    "    \n",
    "    def pickle(self, filename):\n",
    "        '''save model to file'''\n",
    "        f = open(filename, 'wb')\n",
    "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def unpickle(filename):\n",
    "        '''read model from file'''\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NetworkMBGD(Network):\n",
    "    '''class for minibatch gradient descent'''\n",
    "    \n",
    "    def __init__(self,x, y, nn_structure, test_size):\n",
    "        Network.__init__(self,  x, y, nn_structure, test_size)\n",
    "        self.lamb=None\n",
    "        self.alpha=None\n",
    "        self.iter_num=None\n",
    "        \n",
    "        \n",
    "    def get_mini_batches(self, X, y, batch_size):\n",
    "        random_idxs = np.random.choice(len(y), len(y), replace=False)\n",
    "        X_shuffled = self.X_train[random_idxs,:]\n",
    "        y_shuffled = self.y_v_train[random_idxs]\n",
    "        mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for\n",
    "                       i in range(0, len(self.y_v_train), batch_size)]\n",
    "        return mini_batches\n",
    "    \n",
    "    \n",
    "    def train_nn(self, iter_num=3000, bs=100, alpha=0.25, lamb=0.000):\n",
    "        self.lamb=lamb\n",
    "        self.alpha=alpha\n",
    "        self.iter_num=iter_num\n",
    "        #reset avg cost\n",
    "        if len(self.avg_cost_func)>0:\n",
    "            self.avg_cost_func=[]\n",
    "        \n",
    "        W, b = self.setup_and_init_weights()\n",
    "        cnt = 0\n",
    "        m = len(self.y_v_train)\n",
    "        print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "        while cnt < iter_num:\n",
    "            if cnt%1000 == 0:\n",
    "                print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "            tri_W, tri_b = self.init_tri_values()\n",
    "            avg_cost = 0\n",
    "            mini_batches = self.get_mini_batches(self.X_train, self.y_v_train, bs)\n",
    "            for mb in mini_batches:\n",
    "                X_mb = mb[0]\n",
    "                y_mb = mb[1]\n",
    "                # pdb.set_trace()\n",
    "                for i in range(len(y_mb)):\n",
    "                    delta = {}\n",
    "                    # perform the feed forward pass and return the stored h and z values, \n",
    "                    # to be used in the gradient descent step\n",
    "                    h, z = self.feed_forward(X_mb[i, :], W, b)\n",
    "                    # loop from nl-1 to 1 backpropagating the errors\n",
    "                    for l in range(len(self.nn_structure), 0, -1):\n",
    "                        if l == len(self.nn_structure):\n",
    "                            delta[l] = self.calculate_out_layer_delta(y_mb[i,:], h[l], z[l])\n",
    "                            avg_cost += np.linalg.norm((y_mb[i,:]-h[l]))\n",
    "                        else:\n",
    "                            if l > 1:\n",
    "                                delta[l] = self.calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                            # triW^(l) = triW^(l) + delta^(l+1) * transpose(h^(l))\n",
    "                            tri_W[l] += np.dot(delta[l+1][:,np.newaxis], \n",
    "                                              np.transpose(h[l][:,np.newaxis])) \n",
    "                            # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                            tri_b[l] += delta[l+1]\n",
    "                # perform the gradient descent step for the weights in each layer\n",
    "                for l in range(len(self.nn_structure) - 1, 0, -1):\n",
    "                    W[l] += -alpha * (1.0/bs * tri_W[l] + lamb * W[l])\n",
    "                    b[l] += -alpha * (1.0/bs * tri_b[l])\n",
    "            # complete the average cost calculation\n",
    "            avg_cost = 1.0/m * avg_cost\n",
    "            self.avg_cost_func.append(avg_cost)\n",
    "            cnt += 1\n",
    "        self.W=W\n",
    "        self.b=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextMBGD(NetworkMBGD):\n",
    "    '''wrapper class for NetworkMBGD to make textual data edible for it'''\n",
    "    \n",
    "    def __init__(self,x_texts, y_texts, nn_structure, test_size, n_words=10000, custom_chars_to_remove=None):\n",
    "            \n",
    "        self.x_texts=x_texts\n",
    "        self.y_texts=y_texts  \n",
    "        \n",
    "        #cleaning stuff\n",
    "        self.n_words=n_words\n",
    "        self.custom_chars_to_remove=custom_chars_to_remove\n",
    "        self.cleaner=TextCleaner(self.custom_chars_to_remove)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        #init X dataset\n",
    "        self.X_count=None\n",
    "        self.X_dictionary=None\n",
    "        self.X_reversed_dictionary=None\n",
    "        self.build_x_dataset()\n",
    "        \n",
    "        #init y dataset\n",
    "        self.y_dictionary=None\n",
    "        self.y_reversed_dictionary=None\n",
    "        self.build_y_dataset()\n",
    "        \n",
    "        NetworkMBGD.__init__(self, self.build_x_dataset(), self.build_y_dataset(), nn_structure, test_size)\n",
    "        \n",
    "        \n",
    "    def build_x_dataset(self):\n",
    "        '''build dataset for neural network. turns texts into matrix\n",
    "        INPUT: \n",
    "            - n_words: nr of top sequence words kept, others words are makerd as unknown\n",
    "            - texts_list_raw: list of texts to be build a dataset. each list element is 1 row\n",
    "        '''\n",
    "        \n",
    "        texts_list=[]\n",
    "        texts_tokenized=[]\n",
    "        words=[]\n",
    "        max_sentence_len=0\n",
    "\n",
    "        for sentence in self.x_texts:\n",
    "            text_words=self.cleaner.preporcess_text(sentence)\n",
    "            texts_tokenized.append(text_words)\n",
    "            words.extend(text_words)\n",
    "            if len(text_words)>max_sentence_len:\n",
    "                max_sentence_len=len(text_words)\n",
    "\n",
    "        #set normalised sentence len, other ideas?\n",
    "        normalized_sentence_len=max_sentence_len\n",
    "        #unknonown words count\n",
    "        count = [['UNK', -1]]\n",
    "\n",
    "        count.extend(collections.Counter(words).most_common(self.n_words - 1))\n",
    "        dictionary = dict()\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = len(dictionary)\n",
    "        data = list()\n",
    "        unk_count = 0\n",
    "        for text in texts_tokenized:\n",
    "            text_data=[]\n",
    "            for word in text:\n",
    "                if word in dictionary:\n",
    "                    index = dictionary[word]\n",
    "                else:\n",
    "                    index = 0  # dictionary['UNK']\n",
    "                    unk_count += 1\n",
    "                text_data.append(index)\n",
    "            if len(text_data)<normalized_sentence_len:\n",
    "                text_data.extend([0] * (normalized_sentence_len-len(text_data)))\n",
    "\n",
    "            data.append(text_data)\n",
    "\n",
    "        count[0][1] = unk_count\n",
    "        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        \n",
    "        self.X_count=count\n",
    "        self.X_dictionary=dictionary\n",
    "        self.X_reversed_dictionary=reversed_dictionary\n",
    "        return np.array(self.scaler.fit_transform(data))\n",
    "        \n",
    "        \n",
    "    def build_y_dataset(self):\n",
    "        '''build y dataset, dataset_y will be one hot vector. turns text into matrix'''\n",
    "        dictionary={}\n",
    "        cnt=0\n",
    "        for word in set(self.y_texts):\n",
    "            dictionary[word]=cnt\n",
    "            cnt+=1\n",
    "\n",
    "        dataset=[]\n",
    "        for category in self.y_texts:\n",
    "            dataset.append([dictionary[category]])\n",
    "        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        \n",
    "        self.y_dictionary=dictionary\n",
    "        self.y_reversed_dictionary=reversed_dictionary\n",
    "        return np.array(dataset)\n",
    "        \n",
    "        \n",
    "    def text_to_vector(self, text):\n",
    "        '''turns text to vector based on input data dictionary, for random text vectorization'''\n",
    "        #make sep function for tokenization\n",
    "        texts_tokenized=[]\n",
    "        text_vector=[]\n",
    "        text_words=self.cleaner.preporcess_text(text)\n",
    "        texts_tokenized.extend(text_words)\n",
    "\n",
    "        for token in texts_tokenized:\n",
    "            if token in self.X_dictionary:\n",
    "                text_vector.extend([self.X_dictionary[token]])\n",
    "            else:\n",
    "                text_vector.extend([0])\n",
    "\n",
    "        return text_vector\n",
    "    \n",
    "    \n",
    "    def predict_text_label(self, text):\n",
    "        '''predicts label of text based on model'''\n",
    "        text_vector=self.text_to_vector(text)\n",
    "        #normalize lenght\n",
    "        if len(text_vector)<self.X_test.shape[1]:\n",
    "            text_vector.extend([0] * (self.X_test.shape[1]-len(text_vector)))\n",
    "\n",
    "        h, z = self.feed_forward(text_vector, self.W, self.b)\n",
    "        y = np.argmax(h[len(self.nn_structure)])\n",
    "        y_label=self.y_reversed_dictionary[y]\n",
    "        return y_label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_MBGD=TextMBGD(df['text'].tolist(), df['y'].tolist(),[40, 30, 2], 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 120 iterations\n",
      "Iteration 0 of 120\n",
      "CPU times: user 41.3 s, sys: 28 ms, total: 41.4 s\n",
      "Wall time: 43.4 s\n"
     ]
    }
   ],
   "source": [
    "% time nn_MBGD.train_nn(iter_num=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nigAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f43e4988390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_MBGD.plot_avg_cost_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.46913580246914"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_MBGD.get_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
