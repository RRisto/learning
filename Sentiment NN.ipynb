{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel('data/sentiment_data/training_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y                                               text\n",
       "0  positive            The Da Vinci Code book is just awesome.\n",
       "1  positive  this was the first clive cussler i've ever rea...\n",
       "2  positive                   i liked the Da Vinci Code a lot.\n",
       "3  positive                   i liked the Da Vinci Code a lot.\n",
       "4  positive  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " class TextCleaner(object):\n",
    "        '''class to clean text'''\n",
    "        \n",
    "        def __init__(self, stemmer=None, make_lower=True, custom_chars_to_remove=None):\n",
    "            self.stemmer=stemmer\n",
    "            self.make_lower=make_lower\n",
    "            self.custom_chars_to_remove=custom_chars_to_remove\n",
    "            \n",
    "        def remove_punctutation(self, text): \n",
    "            '''remove punctuation and custom characters if set'''\n",
    "            \n",
    "            text_clean = text.translate(str.maketrans(\"\", \"\", string.punctuation))\t\t\n",
    "            if self.custom_chars_to_remove is not None:\n",
    "                text_clean = text_clean.translate(str.maketrans(\"\", \"\",self.custom_chars_to_remove))\n",
    "            return text_clean\n",
    "            \n",
    "        def tokenize_text(self, text):\n",
    "            '''tokenize text into words, if stemmer, stems. returns list of words in order in text'''\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            if self.stemmer is not None:\n",
    "                words_list=[]\n",
    "                for w in words:\n",
    "                    words_list.append(stemmer.stem(w))\n",
    "                    return words_list\n",
    "            return words\n",
    "            \n",
    "        def preporcess_text(self, text):\n",
    "            '''generic function to process data'''\n",
    "            if self.make_lower:\n",
    "                text=text.lower()\n",
    "            text=self.remove_punctutation(text)\n",
    "            text_tokens=self.tokenize_text(text)\n",
    "            return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    '''generic class for network building. Exact implementation is in subclasses'''\n",
    "    \n",
    "    def __init__(self,x, y, nn_structure, test_size):\n",
    "        \"\"\"initiate NN with x and y texts, also clean data and turn it into matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X=x #matrix\n",
    "        self.y=y #matrix\n",
    "        self.nn_structure=nn_structure #list of nr of in each layer\n",
    "    \n",
    "        #split test and trainig data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=test_size)\n",
    "        #vectorize y\n",
    "        self.y_v_train=self.convert_y_to_vect(self.y_train)\n",
    "        self.y_v_test=self.convert_y_to_vect(self.y_test)\n",
    "        self.y_pred=None\n",
    "        \n",
    "        self.avg_cost_func=[]\n",
    "        self.W=None\n",
    "        self.b=None\n",
    "        \n",
    "                 \n",
    "    def convert_y_to_vect(self, y):\n",
    "        y_vect = np.zeros((len(y), len(self.y_dictionary)))\n",
    "        for i in range(len(y)):\n",
    "            y_vect[i, y[i]] = 1\n",
    "        return y_vect\n",
    "\n",
    "    \n",
    "    def f(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        \n",
    "    def f_deriv(self,x):\n",
    "        return self.f(x) * (1 - self.f(x))\n",
    "    \n",
    "    \n",
    "    def setup_and_init_weights(self):\n",
    "        W = {}\n",
    "        b = {}\n",
    "        for l in range(1, len(self.nn_structure)):\n",
    "            W[l] = np.random.random_sample((self.nn_structure[l], self.nn_structure[l-1]))\n",
    "            b[l] = np.random.random_sample((self.nn_structure[l],))\n",
    "        return W, b\n",
    "    \n",
    "    \n",
    "    def init_tri_values(self):\n",
    "        tri_W = {}\n",
    "        tri_b = {}\n",
    "        for l in range(1, len(self.nn_structure)):\n",
    "            tri_W[l] = np.zeros((self.nn_structure[l], self.nn_structure[l-1]))\n",
    "            tri_b[l] = np.zeros((self.nn_structure[l],))\n",
    "        return tri_W, tri_b\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, x, W, b):\n",
    "        h = {1: x}\n",
    "        z = {}\n",
    "        for l in range(1, len(W) + 1):\n",
    "            # if it is the first layer, then the input into the weights is x, otherwise, \n",
    "            # it is the output from the last layer\n",
    "            if l == 1:\n",
    "                node_in = x\n",
    "            else:\n",
    "                node_in = h[l]\n",
    "            z[l+1] = W[l].dot(node_in) + b[l] # z^(l+1) = W^(l)*h^(l) + b^(l)  \n",
    "            h[l+1] = self.f(z[l+1]) # h^(l) = f(z^(l)) \n",
    "        return h, z\n",
    "    \n",
    "    \n",
    "    def calculate_out_layer_delta(self, y, h_out, z_out):\n",
    "        # delta^(nl) = -(y_i - h_i^(nl)) * f'(z_i^(nl))\n",
    "        return -(y-h_out) * self.f_deriv(z_out)\n",
    "    \n",
    "    \n",
    "    def calculate_hidden_delta(self, delta_plus_1, w_l, z_l):\n",
    "        # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "        return np.dot(np.transpose(w_l), delta_plus_1) * self.f_deriv(z_l)\n",
    "    \n",
    "    \n",
    "    def train_nn(self):\n",
    "        '''implemented in subclass'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "    def plot_avg_cost_func(self):\n",
    "        if len(self.avg_cost_func)==0:\n",
    "            print('Please train model before visalizing average cost')\n",
    "            return\n",
    "        plt.plot(self.avg_cost_func)\n",
    "        plt.ylabel('Average J')\n",
    "        plt.xlabel('Iteration number')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict_test_data(self):\n",
    "        m = self.X_test.shape[0]\n",
    "        y = np.zeros((m,))\n",
    "        for i in range(m):\n",
    "            h, z = self.feed_forward(self.X_test[i, :], self.W, self.b)\n",
    "            y[i] = np.argmax(h[len(self.nn_structure)])\n",
    "        self.y_pred=y\n",
    "        \n",
    "        \n",
    "    def get_test_accuracy(self):\n",
    "        self.predict_test_data()\n",
    "        return accuracy_score(self.y_test, self.y_pred)*100\n",
    "    \n",
    "    \n",
    "    def pickle(self, filename):\n",
    "        '''save model to file'''\n",
    "        f = open(filename, 'wb')\n",
    "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def unpickle(filename):\n",
    "        '''read model from file'''\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NetworkMBGD(Network):\n",
    "    '''class for minibatch gradient descent'''\n",
    "    \n",
    "    def __init__(self,x, y, nn_structure, test_size):\n",
    "        Network.__init__(self,  x, y, nn_structure, test_size)\n",
    "        self.lamb=None\n",
    "        self.alpha=None\n",
    "        self.iter_num=None\n",
    "        \n",
    "        \n",
    "    def get_mini_batches(self, X, y, batch_size):\n",
    "        random_idxs = np.random.choice(len(y), len(y), replace=False)\n",
    "        X_shuffled = self.X_train[random_idxs,:]\n",
    "        y_shuffled = self.y_v_train[random_idxs]\n",
    "        mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for\n",
    "                       i in range(0, len(self.y_v_train), batch_size)]\n",
    "        return mini_batches\n",
    "    \n",
    "    \n",
    "    def train_nn(self, iter_num=3000, bs=100, alpha=0.25, lamb=0.000):\n",
    "        self.lamb=lamb\n",
    "        self.alpha=alpha\n",
    "        self.iter_num=iter_num\n",
    "        #reset avg cost\n",
    "        if len(self.avg_cost_func)>0:\n",
    "            self.avg_cost_func=[]\n",
    "        \n",
    "        W, b = self.setup_and_init_weights()\n",
    "        cnt = 0\n",
    "        m = len(self.y_v_train)\n",
    "        print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "        while cnt < iter_num:\n",
    "            if cnt%1000 == 0:\n",
    "                print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "            tri_W, tri_b = self.init_tri_values()\n",
    "            avg_cost = 0\n",
    "            mini_batches = self.get_mini_batches(self.X_train, self.y_v_train, bs)\n",
    "            for mb in mini_batches:\n",
    "                X_mb = mb[0]\n",
    "                y_mb = mb[1]\n",
    "                # pdb.set_trace()\n",
    "                for i in range(len(y_mb)):\n",
    "                    delta = {}\n",
    "                    # perform the feed forward pass and return the stored h and z values, \n",
    "                    # to be used in the gradient descent step\n",
    "                    h, z = self.feed_forward(X_mb[i, :], W, b)\n",
    "                    # loop from nl-1 to 1 backpropagating the errors\n",
    "                    for l in range(len(self.nn_structure), 0, -1):\n",
    "                        if l == len(self.nn_structure):\n",
    "                            delta[l] = self.calculate_out_layer_delta(y_mb[i,:], h[l], z[l])\n",
    "                            avg_cost += np.linalg.norm((y_mb[i,:]-h[l]))\n",
    "                        else:\n",
    "                            if l > 1:\n",
    "                                delta[l] = self.calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                            # triW^(l) = triW^(l) + delta^(l+1) * transpose(h^(l))\n",
    "                            tri_W[l] += np.dot(delta[l+1][:,np.newaxis], \n",
    "                                              np.transpose(h[l][:,np.newaxis])) \n",
    "                            # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                            tri_b[l] += delta[l+1]\n",
    "                # perform the gradient descent step for the weights in each layer\n",
    "                for l in range(len(self.nn_structure) - 1, 0, -1):\n",
    "                    W[l] += -alpha * (1.0/bs * tri_W[l] + lamb * W[l])\n",
    "                    b[l] += -alpha * (1.0/bs * tri_b[l])\n",
    "            # complete the average cost calculation\n",
    "            avg_cost = 1.0/m * avg_cost\n",
    "            self.avg_cost_func.append(avg_cost)\n",
    "            cnt += 1\n",
    "        self.W=W\n",
    "        self.b=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextMBGD(NetworkMBGD):\n",
    "    '''wrapper class for NetworkMBGD to make textual data edible for it'''\n",
    "    \n",
    "    def __init__(self,x_texts, y_texts, test_size, n_words=10000, custom_chars_to_remove=None):\n",
    "            \n",
    "        self.x_texts=x_texts\n",
    "        self.y_texts=y_texts  \n",
    "        \n",
    "        #cleaning stuff\n",
    "        self.n_words=n_words\n",
    "        self.custom_chars_to_remove=custom_chars_to_remove\n",
    "        self.cleaner=TextCleaner(self.custom_chars_to_remove)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        #init X dataset\n",
    "        self.X_count=None\n",
    "        self.X_dictionary=None\n",
    "        self.X_reversed_dictionary=None\n",
    "        self.build_x_dataset()\n",
    "        \n",
    "        #init y dataset\n",
    "        self.y_dictionary=None\n",
    "        self.y_reversed_dictionary=None\n",
    "        self.build_y_dataset()\n",
    "        \n",
    "        X=self.build_x_dataset()\n",
    "        y=self.build_y_dataset()\n",
    "        \n",
    "        self.nn_structure=None\n",
    "        #init structure\n",
    "        self.init_nn_structure(X)\n",
    "        \n",
    "        NetworkMBGD.__init__(self, X, y, self.nn_structure, test_size)\n",
    "        \n",
    "        \n",
    "    def build_x_dataset(self):\n",
    "        '''build dataset for neural network. turns texts into matrix\n",
    "        INPUT: \n",
    "            - n_words: nr of top sequence words kept, others words are makerd as unknown\n",
    "            - texts_list_raw: list of texts to be build a dataset. each list element is 1 row\n",
    "        '''\n",
    "        \n",
    "        texts_list=[]\n",
    "        texts_tokenized=[]\n",
    "        words=[]\n",
    "        max_sentence_len=0\n",
    "\n",
    "        for sentence in self.x_texts:\n",
    "            text_words=self.cleaner.preporcess_text(sentence)\n",
    "            texts_tokenized.append(text_words)\n",
    "            words.extend(text_words)\n",
    "            if len(text_words)>max_sentence_len:\n",
    "                max_sentence_len=len(text_words)\n",
    "\n",
    "        #set normalised sentence len, other ideas?\n",
    "        normalized_sentence_len=max_sentence_len\n",
    "        #unknonown words count\n",
    "        count = [['UNK', -1]]\n",
    "\n",
    "        count.extend(collections.Counter(words).most_common(self.n_words - 1))\n",
    "        dictionary = dict()\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = len(dictionary)\n",
    "        data = list()\n",
    "        unk_count = 0\n",
    "        for text in texts_tokenized:\n",
    "            text_data=[]\n",
    "            for word in text:\n",
    "                if word in dictionary:\n",
    "                    index = dictionary[word]\n",
    "                else:\n",
    "                    index = 0  # dictionary['UNK']\n",
    "                    unk_count += 1\n",
    "                text_data.append(index)\n",
    "            if len(text_data)<normalized_sentence_len:\n",
    "                text_data.extend([0] * (normalized_sentence_len-len(text_data)))\n",
    "\n",
    "            data.append(text_data)\n",
    "\n",
    "        count[0][1] = unk_count\n",
    "        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        \n",
    "        self.X_count=count\n",
    "        self.X_dictionary=dictionary\n",
    "        self.X_reversed_dictionary=reversed_dictionary\n",
    "        return np.array(self.scaler.fit_transform(data))\n",
    "        \n",
    "        \n",
    "    def build_y_dataset(self):\n",
    "        '''build y dataset, dataset_y will be one hot vector. turns text into matrix'''\n",
    "        dictionary={}\n",
    "        cnt=0\n",
    "        for word in set(self.y_texts):\n",
    "            dictionary[word]=cnt\n",
    "            cnt+=1\n",
    "\n",
    "        dataset=[]\n",
    "        for category in self.y_texts:\n",
    "            dataset.append([dictionary[category]])\n",
    "        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        \n",
    "        self.y_dictionary=dictionary\n",
    "        self.y_reversed_dictionary=reversed_dictionary\n",
    "        return np.array(dataset)\n",
    "      \n",
    "    def init_nn_structure(self, X):\n",
    "        x=len(X[0])\n",
    "        y=len(self.y_dictionary)\n",
    "        h=int(round(np.mean([x, y])))\n",
    "        self.nn_structure=[x,h,y]\n",
    "        \n",
    "    def text_to_vector(self, text):\n",
    "        '''turns text to vector based on input data dictionary, for random text vectorization'''\n",
    "        #make sep function for tokenization\n",
    "        texts_tokenized=[]\n",
    "        text_vector=[]\n",
    "        text_words=self.cleaner.preporcess_text(text)\n",
    "        texts_tokenized.extend(text_words)\n",
    "\n",
    "        for token in texts_tokenized:\n",
    "            if token in self.X_dictionary:\n",
    "                text_vector.extend([self.X_dictionary[token]])\n",
    "            else:\n",
    "                text_vector.extend([0])\n",
    "\n",
    "        return text_vector\n",
    "    \n",
    "    \n",
    "    def predict_text_label(self, text):\n",
    "        '''predicts label of text based on model'''\n",
    "        text_vector=self.text_to_vector(text)\n",
    "        #normalize lenght\n",
    "        if len(text_vector)<self.X_test.shape[1]:\n",
    "            text_vector.extend([0] * (self.X_test.shape[1]-len(text_vector)))\n",
    "\n",
    "        h, z = self.feed_forward(text_vector, self.W, self.b)\n",
    "        y = np.argmax(h[len(self.nn_structure)])\n",
    "        y_label=self.y_reversed_dictionary[y]\n",
    "        return y_label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_MBGD=TextMBGD(df['text'].tolist(), df['y'].tolist(),[40, 30, 2], 0.4)\n",
    "nn_MBGD=TextMBGD(df['text'].tolist(), df['y'].tolist(), 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nn_MBGD.X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.mean([len(nn_MBGD.X[0]), len(nn_MBGD.y_dictionary)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nn_MBGD.y_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, 21, 2]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_MBGD.nn_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 120 iterations\n",
      "Iteration 0 of 120\n",
      "CPU times: user 37.8 s, sys: 52 ms, total: 37.9 s\n",
      "Wall time: 1min 13s\n"
     ]
    }
   ],
   "source": [
    "% time nn_MBGD.train_nn(iter_num=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VfWd//HX52ZfIHtYwg4BCQIKiIJoXVDRWp0WW7fq\n2EVrq6N2+rOj02VaO7/ftOOMnbYuU6tVp61atVZxqY4balWEsMi+hH0JEJYQAmT//P64J2nABMJy\nc3Nz38/H4z6459xv7v0cDtx3zvme7/eYuyMiIgIQinYBIiLSdSgURESkhUJBRERaKBRERKSFQkFE\nRFooFEREpIVCQUREWigURESkhUJBRERaJEa7gKOVn5/vgwYNinYZIiIxZe7cuTvcveBI7WIuFAYN\nGkRpaWm0yxARiSlmtr4j7XT6SEREWigURESkhUJBRERaKBRERKSFQkFERFooFEREpIVCQUREWkQ0\nFMxsmpmtMLMyM7urjdcHmNk7ZjbfzBaa2SWRqmXOul387LXl6PajIiLti1gomFkC8ABwMVACXG1m\nJYc0+z7wjLufClwFPBipej7ZWMlDM1dTVdMQqY8QEYl5kTxSmAiUufsad68DngYuP6SNAz2D51nA\nlkgVk5eZDMDufXWR+ggRkZgXyVAoAja2Wt4UrGvtR8CXzWwT8CrwD5EqJic9HAo7FQoiIu2Kdkfz\n1cDj7t4PuAT4nZl9qiYzu8nMSs2stKKi4pg+KDdDRwoiIkcSyVDYDPRvtdwvWNfa14BnANz9IyAV\nyD/0jdz9YXef4O4TCgqOOMlfm5qPFHbtVyiIiLQnkqEwByg2s8Fmlky4I3nGIW02AOcDmNlIwqFw\nbIcCR6AjBRGRI4tYKLh7A3Ar8DqwjPBVRkvM7B4zuyxo9h3gRjP7BHgKuMEjdM1oenICyYkhHSmI\niBxGRO+n4O6vEu5Abr3uh62eLwXOjGQNzcyMvIxkdlUrFERE2hPtjuZOlZOezG4dKYiItCuuQiE3\nI5ld6lMQEWlXXIVCTkYyu/fXR7sMEZEuK65CITc9SUcKIiKHEVehkJORzJ4D9dQ3NkW7FBGRLimu\nQiEvGKtQqVNIIiJtiqtQyGkewKYrkERE2hRXoZDbPNWF+hVERNoUV6GQo6kuREQOK65CoXn+I02f\nLSLStrgKhez0JEBHCiIi7YmrUEhJTCAzJVGT4omItCOuQgHCp5B0pCAi0ra4C4WcjGT1KYiItCPu\nQiE3PUnjFERE2hF3oZCTkczufRrRLCLSlrgLhdx0TZ8tItKeuAuFnIxkDtQ3cqCuMdqliIh0OXEX\nCs2T4umyVBGRT4u7UGg91UVTk7Nq294oVyQi0nXEXSg0T3Wxa18dD84s44Kfv8faHfuiXJWISNcQ\nd6GQE8yUunBTJb96uwyA5eVV0SxJRKTLiLtQaD5S+OXbZSSEDIA1OlIQEQHiMBSy0pIwg7qGJr49\ndTi9e6ayuqI62mWJiHQJcRcKCSEjPzOFEb16cMOZgxhamMGaCh0piIgAJEa7gGh48Npx9MlKJSkh\nxJD8TF5YsBl3x8yiXZqISFRF9EjBzKaZ2QozKzOzu9p4/edmtiB4rDSzykjW0+y0Qbn0y0kHYEhB\nBntrGthRrXELIiIRO1IwswTgAeACYBMwx8xmuPvS5jbu/u1W7f8BODVS9bRnSEEmAGsqqinokdLZ\nHy8i0qVE8khhIlDm7mvcvQ54Grj8MO2vBp6KYD1tGlqQAegKJBERiGwoFAEbWy1vCtZ9ipkNBAYD\nb0ewnjb1zUojNSnE6u26AklEpKtcfXQV8Jy7tzlLnZndZGalZlZaUVFxQj84FDIG5WXoSEFEhMiG\nwmagf6vlfsG6tlzFYU4dufvD7j7B3ScUFBScwBLDhhZkskZjFUREIhoKc4BiMxtsZsmEv/hnHNrI\nzE4CcoCPIljLYQ0tyGDDrv3UNmg6bRGJbxELBXdvAG4FXgeWAc+4+xIzu8fMLmvV9CrgaXf3SNVy\nJEMKMmly2LBzf7RKEBHpEiI6eM3dXwVePWTdDw9Z/lEka+iIIcEVSKsr9lHcq0eUqxERiZ6u0tEc\nVS1jFXaoX0FE4ptCAchMSaRXzxRWb9cVSCIS3xQKgZI+PSldv4sodm2IiESdQiFw4ajerN+5n+Vb\ndXtOEYlfCoXABSW9MIPXFm+NdikiIlGjUAjkZ6Zw2qBcXl+iUBCR+KVQaGXaqN4s37qXtZryQkTi\nlEKhlQtH9QLQ0YKIxC2FQiv9ctIZXZSlfgURiVsKhUNMO7k3CzZWsnVPTbRLERHpdAqFQ5w7ohCA\nWWt2RrkSEZHOp1A4xPBemaQmhVi0eU+0SxER6XQKhUMkJoQY2aenQkFE4pJCoQ2ji7JYuqWKpiZN\neSEi8UWh0IaTi7Korm1g7U6NVxCR+KJQaMPooiwAFusUkojEGYVCG4oLM0lJDLFok0JBROKLQqEN\n6mwWkXilUGjH6KIslqizWUTijEKhHaODzuZ16mwWkTiiUGjHyUFns04hiUg8USi0o7hXJsmJIV2B\nJCJxRaHQjqSgs3mhrkASkTiiUDiM0wbmMH9DJftqG6JdiohIp1AoHMZ5Iwupa2zi/VU7ol2KiEin\nUCgcxmmDcumZmshby7ZFuxQRkU4R0VAws2lmtsLMyszsrnbafMnMlprZEjN7MpL1HK2khBDnjCjk\n7eXbadR4BRGJAxELBTNLAB4ALgZKgKvNrOSQNsXA3cCZ7j4KuCNS9Ryr80cWsnNfHQs2Vka7FBGR\niIvkkcJEoMzd17h7HfA0cPkhbW4EHnD33QDuvj2C9RyTc4YXkhgy3tQpJBGJA5EMhSJgY6vlTcG6\n1oYDw83sAzObZWbTIljPMclKT+K0QbnqVxCRuBDtjuZEoBg4B7ga+I2ZZR/ayMxuMrNSMyutqKjo\n5BLDp5BWbqtmw879nf7ZIiKdKZKhsBno32q5X7CutU3ADHevd/e1wErCIXEQd3/Y3Se4+4SCgoKI\nFdyeqSN7AfDWch0tiEj3FslQmAMUm9lgM0sGrgJmHNLmBcJHCZhZPuHTSWsiWNMxGZSfweD8DGau\n6PyjFBGRzhSxUHD3BuBW4HVgGfCMuy8xs3vM7LKg2evATjNbCrwD3OnuOyNV0/H4zPACZq3ZSU19\nY7RLERGJmMT2XjCzl4D2Ls6vBVYTvnJoYzttcPdXgVcPWffDVs8d+Mfg0aWdM6KAxz9cx0drdnLu\niMJolyMiEhHthgLwH0f4uVHAM8CkE1pRF3XGkDxSk0K8u6JCoSAi3Va7oeDu7x7hZ98yszEnuJ4u\nKzUpgUlD8pi5YjvhPBQR6X6Oq0/B3b9+ogqJBeeMKGTdzv2s26G7sYlI9xTtcQox5ZwR4cthw0cL\nIiLdT4dDwczSI1lILBiYF1yaulKXpopI93TEUDCzycElo8uD5bFm9mDEK+uiPjO8gI9W69JUEeme\nOnKk8HPgImAngLt/ApwdyaK6srOH51Pb0MTc9bujXYqIyAnXodNHbYxFiNtfk08fnEdSgulubCLS\nLXUkFDaa2WTAzSzJzP4P4RHKcSkjJZFTB+Tw1zL1K4hI99ORULgZuIXwtNebgVOC5bh11rB8lmyp\nYte+umiXIiJyQh0xFNx9h7tf6+693L3Q3b/cVecn6ixTivNxhw/KdApJRLqXw01zAYCZ/bKN1XuA\nUnd/8cSX1PWN6ZdNz9RE/rpqB58b2zfa5YiInDAdOX2USviU0argMYbwvRG+Zmb/FcHauqyEkDF5\naD7vr6ogPKefiEj3cMQjBcIhcKa7NwKY2UPA+8AUYFEEa+vSphTn89qSrazZsY+hBZnRLkdE5ITo\nyJFCDtD6Wy8DyA1CojYiVcWAs4rzAXhfo5tFpBvpSCj8O7DAzB4zs8eB+cC9ZpYBvBnJ4rqygXkZ\nDO+VyYxPtkS7FBGRE6YjVx89CkwmfOvMPwNT3P0Rd9/n7ndGusCubPq4fszbUMmaiupolyIickJ0\ndEK8GqAc2A0MM7O4neaitc+fWkTI4Pl5m6NdiojICdGRCfG+DrxH+H7KPw7+/FFky4oNhT1TOau4\ngOfnbaKpSVchiUjs68iRwu3AacB6dz8XOBWojGhVMeQL44rYsqeGj9bE9Xg+EekmOhIKNe5eA2Bm\nKe6+HBgR2bJix0WjetMjJZE/zd0U7VJERI5bR0Jhk5llE+5ofsPMXgTWR7as2JGalMBnx/ThL4u3\nsremPtrliIgcl45cffR5d6909x8BPwAeBf4u0oXFkitP68+B+kZemK8OZxGJbYcNBTNLMLPlzcvu\n/q67z3B3TQ/ayin9szm5qCe/n7VB016ISEw7bCgEo5ZXmNmATqonJpkZ150xkBXb9jJnne7IJiKx\nq6PTXCwxs7fMbEbzI9KFxZrLxhbRIzWR381Sd4uIxK6OTIj3g2N9czObBvwCSAAecfefHvL6DcC9\nhG/eA3C/uz9yrJ8XTWnJCXxxfH9+N2sdFXtLKOiREu2SRESOWkc6mt8F1gFJwfM5wLwj/ZyZJQAP\nABcDJcDVZlbSRtM/uvspwSMmA6HZtWcMoL7R+eOcDdEuRUTkmHRkRPONwHPAr4NVRYQvTz2SiUCZ\nu68JOqafBi4/1kJjwdCCTM4qzuf3szZQ39gU7XJERI5aR/oUbgHOBKoA3H0VUNiBnysCNrZa3hSs\nO9R0M1toZs+ZWf8OvG+X9tUzB7O1qoZXF5VHuxQRkaPWkVCobX0JqpklAifqusuXgEHuPgZ4A3ii\nrUZmdpOZlZpZaUVF175/wWeGFzCkIINH/7pWl6eKSMzpSCi8a2b/DKSZ2QXAs4S/zI9kM9D6N/9+\n/K1DGQB33+nuzTfqeQQY39YbufvD7j7B3ScUFBR04KOjJxQyvnLmYBZu2sPc9bo8VURiS0dC4S6g\ngvCtN78BvAp8vwM/NwcoNrPBZpYMXAUcdCmrmfVptXgZsKwjRXd108cVkZWWxKN/XRvtUkREjkpH\nLkn9O+B/3P03R/PG7t5gZrcSnmo7Afituy8xs3uAUnefAdxmZpcBDcAu4Iajqr6LSk9O5JrTB/Dr\nd1ezuqJa93AWkZhhRzrvbWaPAecRvqfCH4HX3L2hE2pr04QJE7y0tDRaH99hFXtrOe8/ZnLqwBye\n+MppmFm0SxKROGZmc919wpHadWScwleAYYT7Eq4GVptZTI8n6AwFPVL49gXDeW9lBa8v2RbtckRE\nOqRDt+N093rgL4THGsxFs6R2yPWTBnJS7x785OWlHKhrjHY5IiJH1JHBaxeb2ePAKmA64auEeke4\nrm4hMSHEjy8bxebKAzw0syza5YiIHFFHjhSuJzyCeYS73+Dur0azTyHWnD4kj8+O7sNjH6zTTXhE\npMvrSJ/C1e7+QvN4AjObYmYPRL607uMbnxnC3toG/jhn45Ebi4hEUYf6FMzsVDO718zWAT8Blh/h\nR6SVMf2ymTg4l8c+WEeD5kQSkS6s3VAws+Fm9i/Bndd+BWwgfAnrue7+q06rsJu48awhbK48wGtL\ntka7FBGRdh3uSGE54fEJl7r7lCAIdAnNMTr/pEIG52fwm/c1J5KIdF2HC4UvAOXAO2b2GzM7H9AI\nrGMUChlfnTKYTzZW8viH6xQMItIltRsKQefyVcBJwDvAHUChmT1kZhd2VoHdyRfH9+PcEQX8+KWl\n3PncQmrqdeAlIl1LR64+2ufuT7r75wjPdDof+KeIV9YNpSYl8Ojfn8Zt5xfz3NxNXP2bWew5oMtU\nRaTr6NDVR83cfXcwjfX5kSqouwuFjH+8YDj//eVxLN68h+sf/VjBICJdxlGFgpw4007uw0PXjmdp\neRXXP/ox63bsi3ZJIiIKhWiaWtKLh64dz7LyvZzzHzO59Ffv8/TsDdEuS0TimEIhyqaW9GLmnefw\nvUtG4g53Pb+Iv+j+ziISJQqFLqBvdho3nj2EF245kzH9srjr+UVs3VMT7bJEJA4pFLqQpIQQ/3Xl\nKdQ1NPGdZxfQ1KSxDCLSuTpyO07pREMKMvnh50q4+/lFnPXv75CfmcyAvAz+adoI+uWkR7s8Eenm\nFApd0FWn9Wd/XSMLN1VSub+ed5Zv590V2/nZ9DFcPLpPtMsTkW5ModAFmRlfmzK4ZXn9zn3c9tR8\nvvmHeVw5oT/fv3QkPVKTolihiHRX6lOIAQPzMnj25sl885yhPDt3Ixf9/D3eX1UR7bJEpBtSKMSI\n5MQQ/zTtJJ775mTSkhO47tHZ/M9H66Jdloh0MwqFGDNuQA6v3HYWF5T04ocvLuGxD9ZGuyQR6UYU\nCjEoNSmBB64Zx0WjevHjl5Zy3xsrqW3QjKsicvwUCjEqOTHE/deM4/OnFvHLt1ZxwX3v8drict2n\nQUSOi0IhhiUlhPj5lafwu69NJC0pgZt/P48HZ66OdlkiEsMiGgpmNs3MVphZmZnddZh2083MzWxC\nJOvprs4qLuCV26Zw+Sl9uff1Fby2WHMnicixiVgomFkC8ABwMVACXG1mJW206wHcDnwcqVriQWJC\niJ9NH8O4Adnc8ccFzFm3i/rGpmiXJSIxJpJHChOBMndf4+51wNPA5W20+wnwM0AzwB2n1KQEfn3d\nBPIyUvjif39E8ff+wrifvMGLCzZHuzQRiRGRHNFcBGxstbwJOL11AzMbB/R391fM7M4I1hI3Cnqk\n8Nw3J/Hmsu3sqq7j7eXbuPO5hQzKy2Bs/+xolyciXVzUOprNLATcB3ynA21vMrNSMyutqNBI3iPp\nk5XGdWcM5PapxTz2lYkUZKbwjd/NZfve8MGYu7N9bw1z1+9i3obdUa5WRLoSi9QljGY2CfiRu18U\nLN8N4O7/FixnAauB6uBHegO7gMvcvbS9950wYYKXlrb7srRhyZY9TH/oQ7LTkklKNLZX1VLb8Lf+\nhjsvGsEt5w6LYoUiEmlmNtfdj3gxTyRPH80Bis1sMLAZuAq4pvlFd98D5Dcvm9lM4P8cLhDk2Izq\nm8VD147nsQ/XkZOeRGGPFPrlpDMgN50/z9/Mva+vIC0pga+2moRPROJTxELB3RvM7FbgdSAB+K27\nLzGze4BSd58Rqc+WTzv3pELOPanwU+vPKs6nrqGJe15eSo/URL44oX8UqhORriJip48iRaePTry6\nhiZueGw2CzZW8vodZ9M/VzfzEeluOnr6SCOaheTEEPd+cSwG3P38Ik2VIRLHFAoCQFF2GnddfBJ/\nLdvBs6Wbol2OiESJQkFaXHv6QCYOzuUnryzlw9U7ol2OiESBQkFahELGvVeMITs9iWt+8zG3PDmP\n1RXVOp0kEkd0j2Y5yMC8DN749mf49btreHBmGa8sLKewRwqnDc7l2okDmDQ0DzOLdpkiEiG6+kja\ntaXyAG8t307pul18ULaDHdV1TBiYwy3nDuPs4QUkhBQOIrGio1cfKRSkQ2rqG3m2dCMPzlxN+Z4a\n+mSlcsX4fnxtymCy05OjXZ6IHIFCQSKirqGJN5dt45nSjby3soJB+Rk88ZWJGtsg0sVpnIJERHJi\niEtG9+Hxr0zk6ZsmsWNvLdMf+pBl5VXRLk1ETgCFghyziYNzefbmyYTM+NJ/f8THa3ZGuyQROU4K\nBTkuI3r34PlvTaawZwrX/3Y2by7dFu2SROQ4qE9BTohd++q44bHZLNlSxcg+PdheVUtiyPjF1ady\n2qDcaJcnEvfUpyCdKjcjmSdvPIPPn1pEXkYK544oJDUpgese/Zh3V+rGSCKxQkcKEjE7qmu5/tHZ\nrNq+lx9cWsKXJvQnNSkh2mWJxCUdKUjU5Wem8NRNZzB+YA4/fHEJU372Nr98axW1DY3RLk1E2qFQ\nkIjKSkviqRvP4Mmvn87ooizue2MlX3lsDtW1DQe1q6lv5OWFW3h54RbNtSQSRZr7SCLOzJg8LJ/J\nw/L509xNfPdPC7n64Vn86LISVlfsY9763by6qJyqmnBQvD9hB//6+ZNJStDvLCKdTaEgnWr6+H5k\npSVxy5PzmP7QRwBkJCdwQUkvrhjfn4/X7uRXb5exZc8Bbju/mMIeKfTqmaq+CJFOolCQTje1pBcv\n/8MUlm3dy6i+PRmcl0EomFxvSnE+/XLS+Oc/L+b9VeF7OqQkhph2cm++OL4/k4fmtbQVkRNPVx9J\nl7S58gCrt1ezraqGTzZVMmPBFqpqGvjCuCL+84tjNX23yFHq6NVHOlKQLqkoO42i7DQAvjihP9//\nbAk/f2Mlv35vDWcOzWf6+H5RrlCke1JPnsSE1KQEvjvtJCYOzuUHLy5mTUV1u22fnr2B3/51bSdW\nJ9J9KBQkZiSEjF9cdQrJiSFueXI+c9fvPujyVXfn315dxl3PL+Kel5cy45MtUaxWJDbp9JHElD5Z\nadz3pbHc8of5TH/oQwbmpTN5aD5F2ams2FbNS59s4drTB7By217u+tNCSvr0YFhhj2iXLRIz1NEs\nMam6toHXFm/lxQWbWbKlil376gC4Y2oxt59fzLaqWi791ftkpyczdWQvlpVXkZWWxH1fGkuixj9I\nHNKd1ySu1NQ3sr+ukdyMv90a9IOyHVz/29mEDAbkprO6Yh93XjSCW84dFsVKRaKjS8x9ZGbTzGyF\nmZWZ2V1tvH6zmS0yswVm9lczK4lkPdJ9pSYlHBQIAGcOy2f2P5/Pkh9P481//AyfHd2H/3pzJcu3\nhu8St3jzHp6evYH9dQ1tvaVIXIrYkYKZJQArgQuATcAc4Gp3X9qqTU93rwqeXwZ8y92nHe59daQg\nx2rXvjou/Pm7FPRIZXRRT56duwl36JOVyg8uLWH8wBxWb69mz4F6zhlRSFqyRlFL99EVxilMBMrc\nfU1Q0NPA5UBLKDQHQiADiK1zWRJTcjOS+X+fH81Nv5tL2fa9fH3KYM4eXsD/e3U53/rDvIPa5qQn\ncd2kQdwwedCnjkBEurNIhkIRsLHV8ibg9EMbmdktwD8CycB5EaxHhAtH9eaR6ycwtDCTwfkZALx0\nax4vLtjCvroGhhZkAvDYB+v45Vur+P2s9fzbF0Zz0aje0SxbpNNE8vTRFcA0d/96sHwdcLq739pO\n+2uAi9z979t47SbgJoABAwaMX79+fURqFmlt+dYqvvPMJyzZUsUlo3vT2OTMXV9JenICt547jC+M\nK4r4lUyNTU7VgXpydLQixynqVx+Z2STgR+5+UbB8N4C7/1s77UPAbnfPOtz7qk9BOlNdQxO/fGsV\nv35vNX2z0xg3IIfVFdUs3LSHAbnpjOzTg9SkBHpnpfK5MX0Z1bcnZkZDYxMNTX7cs7ve/vR83ly6\njZdvO6vlyEbkWHSFUEgk3NF8PrCZcEfzNe6+pFWbYndfFTz/HPAvRypaoSDR0NjkJASzs7o7by7b\nzmMfrGVndR01DY1sqTxAfaNTXJhJQshYs2MfAN+9aARfPXPwMc3s+tricm7+/TzMYGy/bJ67eZLG\nWMgxi3pHs7s3mNmtwOtAAvBbd19iZvcApe4+A7jVzKYC9cBu4FOnjkS6goRWX+pmxgUlvbigpFfL\nut376nh5UTl/WVROalICnxleQNn2av71lWW8tWw7pw3K4eO1u1hWXkUoZCQnhOifm86ZQ/OYNDSf\nkj49yUpPOuj9vv/CYkb17clNZw/h9qcXcP87ZdwxdXinbrfEHw1eE4kQd+eZ0o3c89JSDtQ3Mqpv\nFmP7ZxEyo7a+iRXb9rJwUyVNwX/B/MxkhhVmMrJPT1ZX7OPDsh3MuHUKJX178u0/LmDGJ1uYOrKQ\n9Tv3Y2bcf82pLR3jIkcS9dNHkaJQkFizt6YegB6pSZ96bc+Beuat382q7Xsp217Nym3VrNi6lwP1\njXx76nBun1oMQFVNPVf+ehY19Y0Myc9gwcZKUhJDPPvNyS1TjIscjkJBJEY1Njlbq2rom5Xa7s2E\nlmzZw1UPzyI/M4XvXjSCjbv3U13TwHWTBlHQI6WTK5ZYoFAQ6eZK1+3iukdnc6C+sWVdfmYy914x\nlnNPKqSmvpHtVbX0z007pjvVNTU5Zugud92EQkEkDmyuPEDF3loG52WwbW8Ntz01n+Vb9zIgN51N\nu/fT5DCyT0++dc5QJg7OZdaancxdv5vhvXow7eTe5GeGjyrcnV376li/az9Lt1Tx7soKPizbQXZ6\nMtPH9+PCkl6U76lh6ZYqBualc/kpfRUWMUahIBKHauobuf/tMlZXVFNcmElWejJPfrye1RX7Wtqk\nJIaobWgiZDCsMJPqmgZ2768/6IijKDuNs4cXsLnyAO+vquDQr4mLT+7NT6ePISvt0/0k0jUpFEQE\nCPdRvLF0Kxt3HeCMIXmU9O3Jqu17eWVhOcvK95KVlkROehJ9s9MYmJfO0IJMBualtxwJlO85wEer\ndzIwL50RvXvy5Mfr+ffXVtCrZyp3X3ISF5/c56BLdqVrUiiISMTM37CbO59bSNn2aoYUZHDF+H7k\npieTnpJIeeUBVldUU7m/ngG56QzMz2DqyEL6ZHXOVVJLt1SRmZLIgLz0Tvm8WKFQEJGIamxyXlu8\nlfvfKWNZedVBr+VnppCdnsTGXfupbWgiNSnEzZ8ZyjfOHkooBFsqw/0T8zfsZuX2anLTk+iTncbo\noizOH1lISuLRTw9Sub+On722nKdmb6RHSiIPXz+BSUPzTtTmxjyFgoh0CnenqqaBfbXhR2GP1JbR\n2U1Nztqd+7jvjZW8srCctKSEg/ouUhJDDCvMZM+BerbuqaGhyclOT+KysX0Z3qsHeRnJ7K1tYNbq\nnczfWMm4ATl869yhnxq098rCcn7w4mL2HKjnujMG8kHZDtbv3M99V47l0jF9O/Xvo6tSKIhIl/Lx\nmp28vLCc/MwUinLSKA5GbycnhudzamxyPly9g2dKN/H6kq3UNTS1/GxeRjInF2Xx8dqd1DY0cWFJ\nL847qZBTB+Tw0MzV/Hn+Zsb2y+Kn08cwsk9PKvfXceP/lDJn3W7656Zxav8czirO53Nj+3ZoksKm\nJufBmWUMLcjk4tF9WtbX1DfiTkzegEmhICIxq76xid376thRXUdSgjG0IJNQyNhRXcsj76/lT/M2\nUbG3FgjPS3XbecXccu7QgyYMrKlv5KnZG5izbhfz1leytaqGnPQkrjl9AFdOGNBun4O78+OXlvL4\nh+sAuO1BtLFQAAAKSElEQVS8YdwxdTjPz9/MT15eSlOTc+0ZA/nyGQMo31PD7LW7KMpO4+9OLYro\n38mqbXsZVph5zJcCKxREpNtyd1Ztr2bOul2M7ZfNyUWHnXEfd+ejNTt57IN1vLlsG+4wfmAOJ/ft\nydLyKlZs3cuYftlcP2kgS7ZU8Yu3VnHD5EHsr2vgmdJNFGWnsbnyABMG5tArK5W/LCpvmbOq2a3n\nDuM7Fw7/1Jf2iRgE+Oqicu54egHfnTaCr5815JjeI+qzpIqIRIqZMbxXD4b36tHh9pOH5jN5aD6b\nKw8wY8EW/jx/E38s3UhJn55MO7k376/awU2/mwvA9HH9+OGlJVgwluOR99dyz+Wj+PLpAwmFjHU7\n9vGXxVsZWpDB+IE53Pv6Cu5/p4ya+kZuPHsIKYkh1uzYxzNzNvLywnLqGpsoyEyhX04al4zuw6Vj\n+pCYEGLe+t1s2XOAy8b2bXNuLIAnPlzHj15awrgBOVwxvt8J+zts9+9KRwoiEq/cveU3+IbGJt5c\nto3VFfv4xtlDjureFU1Nzo9fWsITHx18V8i0pAQuHt2bgswUKqprWbqliuVb95IQMprcWwYF9s1K\n5f9+fjSThuYxf0Mlc9fvYsOu/azbuZ/Za3cxdWQv7r/m1OO6aZNOH4mIdCJ35+3l29laVUNtfRPZ\n6UlcOKo3mSkHn5BZvrWKVxaWkxgKcdrgHEJm/OCFxazaXk1yYqilg72wR7hD/syh+dwxtfi4b7Ck\nUBARiRG1DY088eE6tlXVMmlIHhOH5NKzndNJx0p9CiIiMSIlMYGbzh4a7TIA0A1fRUSkhUJBRERa\nKBRERKSFQkFERFooFEREpIVCQUREWigURESkhUJBRERaxNyIZjOrANYfsWHb8oEdJ7CcaOpO2wLd\na3u0LV1TvG/LQHcvOFKjmAuF42FmpR0Z5h0LutO2QPfaHm1L16Rt6RidPhIRkRYKBRERaRFvofBw\ntAs4gbrTtkD32h5tS9ekbemAuOpTEBGRw4u3IwURETmMuAkFM5tmZivMrMzM7op2PUfDzPqb2Ttm\nttTMlpjZ7cH6XDN7w8xWBX/mRLvWjjKzBDObb2YvB8uDzezjYP/80cySo11jR5hZtpk9Z2bLzWyZ\nmU2K1f1iZt8O/n0tNrOnzCw1lvaLmf3WzLab2eJW69rcFxb2y2C7FprZuOhV/mntbMu9wb+zhWb2\nZzPLbvXa3cG2rDCzi47ns+MiFMwsAXgAuBgoAa42s5LoVnVUGoDvuHsJcAZwS1D/XcBb7l4MvBUs\nx4rbgWWtln8G/NzdhwG7ga9Fpaqj9wvgNXc/CRhLeJtibr+YWRFwGzDB3U8GEoCriK398jgw7ZB1\n7e2Li4Hi4HET8FAn1dhRj/PpbXkDONndxwArgbsBgu+Cq4BRwc88GHznHZO4CAVgIlDm7mvcvQ54\nGrg8yjV1mLuXu/u84Plewl88RYS34Ymg2RPA30WnwqNjZv2AzwKPBMsGnAc8FzSJiW0xsyzgbOBR\nAHevc/dKYnS/EL4TY5qZJQLpQDkxtF/c/T1g1yGr29sXlwP/42GzgGwz69M5lR5ZW9vi7v/r7g3B\n4iygX/D8cuBpd69197VAGeHvvGMSL6FQBGxstbwpWBdzzGwQcCrwMdDL3cuDl7YCvaJU1tH6L+C7\nQFOwnAdUtvoHHyv7ZzBQATwWnAp7xMwyiMH94u6bgf8ANhAOgz3AXGJzv7TW3r6I9e+ErwJ/CZ6f\n0G2Jl1DoFswsE/gTcIe7V7V+zcOXkXX5S8nM7FJgu7vPjXYtJ0AiMA54yN1PBfZxyKmiGNovOYR/\n4xwM9AUy+PTpi5gWK/viSMzse4RPKf8hEu8fL6GwGejfarlfsC5mmFkS4UD4g7s/H6ze1nzIG/y5\nPVr1HYUzgcvMbB3h03jnET4vnx2ctoDY2T+bgE3u/nGw/BzhkIjF/TIVWOvuFe5eDzxPeF/F4n5p\nrb19EZPfCWZ2A3ApcK3/bTzBCd2WeAmFOUBxcCVFMuFOmRlRrqnDgnPujwLL3P2+Vi/NAP4+eP73\nwIudXdvRcve73b2fuw8ivB/edvdrgXeAK4JmsbItW4GNZjYiWHU+sJQY3C+ETxudYWbpwb+35m2J\nuf1yiPb2xQzg+uAqpDOAPa1OM3VJZjaN8GnXy9x9f6uXZgBXmVmKmQ0m3Hk++5g/yN3j4gFcQrjH\nfjXwvWjXc5S1TyF82LsQWBA8LiF8Lv4tYBXwJpAb7VqPcrvOAV4Ong8J/iGXAc8CKdGur4PbcApQ\nGuybF4CcWN0vwI+B5cBi4HdASiztF+Apwv0h9YSP4r7W3r4AjPAViauBRYSvuor6NhxhW8oI9x00\nfwf8d6v23wu2ZQVw8fF8tkY0i4hIi3g5fSQiIh2gUBARkRYKBRERaaFQEBGRFgoFERFpoVCQmGBm\n1cGfg8zsmhP83v98yPKHJ/L9TzQzu8HM7o92HdI9KRQk1gwCjioUWo3Ibc9BoeDuk4+ypphyPDNo\nSvenUJBY81PgLDNbEMz/nxDMMz8nmGf+GwBmdo6ZvW9mMwiPzMXMXjCzucE9A24K1v2U8MygC8zs\nD8G65qMSC957sZktMrMrW733TPvbfRT+EIwCPkjQ5mdmNtvMVprZWcH6g37TN7OXzeyc5s8OPnOJ\nmb1pZhOD91ljZpe1evv+wfpVZvYvrd7ry8HnLTCzXzcHQPC+/2lmnwCTTtTOkG4o2iP39NCjIw+g\nOvjzHIJR0MHyTcD3g+cphEcXDw7a7QMGt2rbPJo1jfCo3bzW793GZ00nPId9AuHZNTcAfYL33kN4\njpkQ8BEwpY2aZwL/GTy/BHgzeH4DcH+rdi8D5wTPnWBEKvBn4H+BJML3aljQ6ufLCY/Wbd6WCcBI\n4CUgKWj3IHB9q/f9UrT3ox5d/3Gkw2qRru5CYIyZNc/Pk0V47pc6YLaH55dvdpuZfT543j9ot/Mw\n7z0FeMrdGwlPrPYucBpQFbz3JgAzW0D4tNZf23iP5skL5wZtjqQOeC14vgiodfd6M1t0yM+/4e47\ng89/Pqi1ARgPzAkOXNL42wRwjYQnVBQ5LIWCxDoD/sHdXz9oZfh0zL5DlqcCk9x9v5nNBFKP43Nr\nWz1vpP3/S7VttGng4FO3reuod/fmuWeamn/e3ZsO6Rs5dH4aJ/x38YS7391GHTVBuIkclvoUJNbs\nBXq0Wn4d+GYwtThmNjy40c2hsoDdQSCcRPi2ps3qm3/+EO8DVwb9FgWE77J27LNP/s064BQzC5lZ\nf47tLlkXWPj+w2mE7yb2AeGJ364ws0JouT/xwBNQr8QRHSlIrFkINAYdpo8TvhfDIGBe0NlbQdu3\njHwNuNnMlhGeSXJWq9ceBhaa2TwPT+Pd7M+EO2U/Ifyb+HfdfWsQKsfjA2At4Q7wZcC8Y3iP2YRP\nB/UDfu/upQBm9n3gf80sRHiGzVuA9cdZr8QRzZIqIiItdPpIRERaKBRERKSFQkFERFooFEREpIVC\nQUREWigURESkhUJBRERaKBRERKTF/wfHRniSUYoDcAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7faaa194a208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_MBGD.plot_avg_cost_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.610229276895936"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_MBGD.get_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_MBGD.pickle('proov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
