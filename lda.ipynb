{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we’ll need a function to randomly choose an index based on an arbitrary set of weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from(weights):\n",
    "    \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n",
    "    total = sum(weights)\n",
    "    rnd = total * random.random() # uniform between 0 and total\n",
    "    for i, w in enumerate(weights):\n",
    "        rnd -= w # return the smallest i such that\n",
    "        if rnd <= 0: \n",
    "            return i # weights[0] + ... + weights[i] >= rnd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_from([1, 1, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, if you give it weights [1, 1, 3], then one-fifth of the time it will return 0, one-fifth of the time it will return 1, and three-fifths of the time it will return 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]\n",
    "\n",
    "K=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we’ll try to find K = 4 topics.\n",
    "\n",
    "In order to calculate the sampling weights, we’ll need to keep track of several counts. Let’s first create the data structures for them.\n",
    "\n",
    "How many times each topic is assigned to each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of Counters, one for each document\n",
    "document_topic_counts = [Counter() for _ in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter(),\n",
       " Counter()]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topic_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times each word is assigned to each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of Counters, one for each topic\n",
    "topic_word_counts = [Counter() for _ in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of words assigned to each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of numbers, one for each topic\n",
    "topic_counts = [0 for _ in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of words contained in each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# a list of numbers, one for each document\n",
    "document_lengths = list(map(len, documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of distinct words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the number of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, once we populate these, we can find the number of words in documents[3] associated with topic 1 as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topic_counts[3][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can find the number of times nlp is associated with topic 2 as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_word_counts[2][\"nlp\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’re ready to define our conditional probability functions. As in Chapter 13, each has a smoothing term that ensures every topic has a nonzero chance of being chosen in any document and that every word has a nonzero chance of being chosen for any topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def p_topic_given_document(topic, d, alpha=0.1):\n",
    "    \"\"\"the fraction of words in document _d_\n",
    "    that are assigned to _topic_ (plus some smoothing)\"\"\"\n",
    "    return ((document_topic_counts[d][topic] + alpha) /\n",
    "            (document_lengths[d] + K * alpha))\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):\n",
    "    \"\"\"the fraction of words assigned to _topic_\n",
    "    that equal _word_ (plus some smoothing)\"\"\"\n",
    "    return ((topic_word_counts[topic][word] + beta) /\n",
    "            (topic_counts[topic] + W * beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll use these to create the weights for updating topics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topic_weight(d, word, k):\n",
    "    \"\"\"given a document and a word in that document,\n",
    "    return the weight for the kth topic\"\"\"\n",
    "    return p_word_given_topic(word, k) * p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k)\n",
    "                        for k in range(K)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are solid mathematical reasons why topic_weight is defined the way it is, but their details would lead us too far afield. Hopefully it makes at least intuitive sense that—given a word and its document—the likelihood of any topic choice depends on both how likely that topic is for the document and how likely that word is for the topic.\n",
    "\n",
    "This is all the machinery we need. We start by assigning every word to a random topic, and populating our counters appropriately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document]\n",
    "                   for document in documents]\n",
    "\n",
    "for d in range(D):\n",
    "    for word, topic in zip(documents[d], document_topics[d]):\n",
    "        document_topic_counts[d][topic] += 1\n",
    "        topic_word_counts[topic][word] += 1\n",
    "        topic_counts[topic] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'support vector machines'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to get a joint sample of the topics-words distribution and the documents-topics distribution. We do this using a form of Gibbs sampling that uses the conditional probabilities defined previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for iter in range(1000):\n",
    "    for d in range(D):\n",
    "        for i, (word, topic) in enumerate(zip(documents[d],\n",
    "                                              document_topics[d])):\n",
    "\n",
    "            # remove this word / topic from the counts\n",
    "            # so that it doesn't influence the weights\n",
    "            document_topic_counts[d][topic] -= 1\n",
    "            topic_word_counts[topic][word] -= 1\n",
    "            topic_counts[topic] -= 1\n",
    "            document_lengths[d] -= 1\n",
    "\n",
    "            # choose a new topic based on the weights\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic\n",
    "\n",
    "            # and now add it back to the counts\n",
    "            document_topic_counts[d][new_topic] += 1\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the topics? They’re just numbers 0, 1, 2, and 3. If we want names for them we have to do that ourselves. Let’s look at the five most heavily weighted words for each (Table 20-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 pandas 4\n",
      "0 scikit-learn 4\n",
      "0 R 2\n",
      "0 Big Data 2\n",
      "0 C++ 2\n",
      "0 Java 2\n",
      "0 statsmodels 2\n",
      "0 HBase 2\n",
      "0 artificial intelligence 2\n",
      "0 Haskell 2\n",
      "0 scipy 1\n",
      "0 libsvm 1\n",
      "0 numpy 1\n",
      "0 deep learning 1\n",
      "0 Hadoop 1\n",
      "0 regression 1\n",
      "0 mathematics 1\n",
      "0 statistics 1\n",
      "1 neural networks 4\n",
      "1 deep learning 3\n",
      "1 MongoDB 3\n",
      "1 Postgres 2\n",
      "1 theory 2\n",
      "1 decision trees 2\n",
      "1 MySQL 2\n",
      "1 HBase 2\n",
      "1 databases 2\n",
      "1 Mahout 2\n",
      "1 numpy 1\n",
      "1 Big Data 1\n",
      "1 Cassandra 1\n",
      "1 Python 1\n",
      "1 statistics 1\n",
      "2 R 5\n",
      "2 Python 5\n",
      "2 regression 5\n",
      "2 Java 4\n",
      "2 machine learning 3\n",
      "2 Cassandra 3\n",
      "2 probability 3\n",
      "2 statistics 3\n",
      "2 Postgres 2\n",
      "2 C++ 2\n",
      "2 statsmodels 2\n",
      "2 artificial intelligence 2\n",
      "2 HBase 2\n",
      "2 scipy 1\n",
      "2 programming languages 1\n",
      "2 Storm 1\n",
      "2 mathematics 1\n",
      "2 MongoDB 1\n",
      "3 libsvm 3\n",
      "3 Big Data 3\n",
      "3 probability 3\n",
      "3 NoSQL 2\n",
      "3 support vector machines 2\n",
      "3 Spark 2\n",
      "3 Hadoop 2\n",
      "3 Python 2\n",
      "3 MapReduce 2\n",
      "3 R 1\n",
      "3 Storm 1\n",
      "3 machine learning 1\n",
      "3 programming languages 1\n",
      "3 statistics 1\n"
     ]
    }
   ],
   "source": [
    "for k, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common():\n",
    "        if count > 0: print(k, word, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these I’d probably assign topic names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topic_names = [\"Big Data and programming languages\",\n",
    "               \"Python and statistics\",\n",
    "               \"databases\",\n",
    "               \"machine learning\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at which point we can see how the model assigns topics to each user’s interests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "machine learning 6\n",
      "databases 5\n",
      "Big Data and programming languages 2\n",
      "\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "databases 5\n",
      "Python and statistics 3\n",
      "machine learning 2\n",
      "\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "Big Data and programming languages 6\n",
      "databases 4\n",
      "Python and statistics 2\n",
      "\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "databases 6\n",
      "Big Data and programming languages 3\n",
      "machine learning 1\n",
      "\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "databases 3\n",
      "machine learning 3\n",
      "Python and statistics 2\n",
      "\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "Big Data and programming languages 5\n",
      "databases 4\n",
      "machine learning 3\n",
      "\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "Python and statistics 3\n",
      "Big Data and programming languages 2\n",
      "machine learning 2\n",
      "databases 1\n",
      "\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "Python and statistics 4\n",
      "Big Data and programming languages 2\n",
      "databases 2\n",
      "\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "Python and statistics 5\n",
      "Big Data and programming languages 2\n",
      "machine learning 1\n",
      "\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Big Data and programming languages 4\n",
      "machine learning 3\n",
      "databases 1\n",
      "\n",
      "['statistics', 'R', 'statsmodels']\n",
      "databases 4\n",
      "Big Data and programming languages 2\n",
      "\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "databases 5\n",
      "Big Data and programming languages 1\n",
      "Python and statistics 1\n",
      "machine learning 1\n",
      "\n",
      "['pandas', 'R', 'Python']\n",
      "databases 3\n",
      "Big Data and programming languages 2\n",
      "machine learning 1\n",
      "\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "Python and statistics 9\n",
      "databases 1\n",
      "\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "machine learning 3\n",
      "databases 2\n",
      "Big Data and programming languages 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for document, topic_counts in zip(documents, document_topic_counts):\n",
    "    print(document)\n",
    "    for topic, count in topic_counts.most_common():\n",
    "        if count > 0:\n",
    "            print(topic_names[topic], count)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Turn into class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLDA(object):\n",
    "    \n",
    "    def __init__(self, documents,nr_topics ):\n",
    "        self.documents=documents\n",
    "        self.nr_topics=nr_topics\n",
    "        self.distinct_words=None\n",
    "        self.count_distinct_words()\n",
    "        \n",
    "        self.D= len(self.documents)\n",
    "        self.W=len(self.distinct_words)\n",
    "        \n",
    "        self.document_topic_counts =None\n",
    "        self.count_document_topic()\n",
    "        \n",
    "        self.topic_word_counts =None\n",
    "        self.count_topic_word()\n",
    "        \n",
    "        self.topic_counts = None\n",
    "        self.count_topic()\n",
    "        \n",
    "        self.document_lengths =None\n",
    "        self.get_document_lengths()\n",
    "        \n",
    "        self.document_topics = None\n",
    "        self.init_document_topics()\n",
    "        \n",
    "        self.init_counts()\n",
    "        \n",
    "        self.nr_iter=None\n",
    "        self.topic_names=None\n",
    "        \n",
    "    def count_document_topic(self):\n",
    "         self.document_topic_counts = [Counter() for _ in self.documents]\n",
    "    \n",
    "    def count_topic_word(self):\n",
    "        self.topic_word_counts= [Counter() for _ in range(self.nr_topics)]\n",
    "    \n",
    "    def sample_from(self, weights):\n",
    "        \"\"\"returns i with probability weights[i] / sum(weights)\"\"\"\n",
    "        total = sum(weights)\n",
    "        rnd = total * random.random() # uniform between 0 and total\n",
    "        for i, w in enumerate(weights):\n",
    "            rnd -= w # return the smallest i such that\n",
    "            if rnd <= 0: \n",
    "                return i # weights[0] + ... + weights[i] >= rnd\n",
    "            \n",
    "    def count_topic(self):\n",
    "        self.topic_counts= [0 for _ in range(self.nr_topics)]\n",
    "    \n",
    "    def get_document_lengths(self):\n",
    "        self.document_lengths= list(map(len, self.documents))\n",
    "    \n",
    "    def count_distinct_words(self):\n",
    "        self.distinct_words=set(word for document in self.documents for word in document)\n",
    "    \n",
    "    def p_topic_given_document(self, topic, d, alpha=0.1):\n",
    "        \"\"\"the fraction of words in document _d_\n",
    "        that are assigned to _topic_ (plus some smoothing)\"\"\"\n",
    "        return ((self.document_topic_counts[d][topic] + alpha) /\n",
    "                (self.document_lengths[d] + self.nr_topics * alpha))\n",
    "\n",
    "    def p_word_given_topic(self,word, topic, beta=0.1):\n",
    "        \"\"\"the fraction of words assigned to _topic_\n",
    "        that equal _word_ (plus some smoothing)\"\"\"\n",
    "        return ((self.topic_word_counts[topic][word] + beta) /\n",
    "                (self.topic_counts[topic] + self.W * beta))\n",
    "    \n",
    "    def init_document_topics(self):\n",
    "        random.seed(0)\n",
    "        self.document_topics = [[random.randrange(self.nr_topics) for word in document] for document in self.documents]\n",
    "\n",
    "    def init_counts(self):\n",
    "        for d in range(self.D):\n",
    "            for word, topic in zip(self.documents[d], self.document_topics[d]):\n",
    "                self.document_topic_counts[d][topic] += 1\n",
    "                self.topic_word_counts[topic][word] += 1\n",
    "                self.topic_counts[topic] += 1\n",
    "                \n",
    "    def topic_weight(self, d, word, k):\n",
    "        \"\"\"given a document and a word in that document,\n",
    "        return the weight for the kth topic\"\"\"\n",
    "        return self.p_word_given_topic(word, k) * self.p_topic_given_document(k, d)\n",
    "\n",
    "    def choose_new_topic(self, d, word):\n",
    "        return self.sample_from([self.topic_weight(d, word, k)\n",
    "                            for k in range(self.nr_topics)])\n",
    "\n",
    "    def train(self, nr_iter):  \n",
    "        self.nr_iter=nr_iter\n",
    "        for iter in range(self.nr_iter):\n",
    "            for d in range(self.D):\n",
    "                for i, (word, topic) in enumerate(zip(self.documents[d],\n",
    "                                                      self.document_topics[d])):\n",
    "\n",
    "                    # remove this word / topic from the counts\n",
    "                    # so that it doesn't influence the weights\n",
    "                    self.document_topic_counts[d][topic] -= 1\n",
    "                    self.topic_word_counts[topic][word] -= 1\n",
    "                    self.topic_counts[topic] -= 1\n",
    "                    self.document_lengths[d] -= 1\n",
    "\n",
    "                    # choose a new topic based on the weights\n",
    "                    new_topic = self.choose_new_topic(d, word)\n",
    "                    self.document_topics[d][i] = new_topic\n",
    "\n",
    "                    # and now add it back to the counts\n",
    "                    self.document_topic_counts[d][new_topic] += 1\n",
    "                    self.topic_word_counts[new_topic][word] += 1\n",
    "                    self.topic_counts[new_topic] += 1\n",
    "                    self.document_lengths[d] += 1\n",
    "                    \n",
    "    def top_words_per_topic(self):\n",
    "        for k, word_counts in enumerate(self.topic_word_counts):\n",
    "            for word, count in word_counts.most_common():\n",
    "                if count > 0: \n",
    "                    print(k, word, count)\n",
    "    \n",
    "    def assign_topics(self, topic_names):\n",
    "        self.topic_names=topic_names\n",
    "        for document, topic_counts in zip(self.documents, self.document_topic_counts):\n",
    "            print(document)\n",
    "            for topic, count in topic_counts.most_common():\n",
    "                if count > 0:\n",
    "                    \n",
    "                    print(self.topic_names[topic], count)\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model=CustomLDA(documents, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda_model.train(nr_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Java 3\n",
      "0 Big Data 3\n",
      "0 Hadoop 2\n",
      "0 programming languages 1\n",
      "0 HBase 1\n",
      "0 MapReduce 1\n",
      "0 Storm 1\n",
      "0 Cassandra 1\n",
      "0 C++ 1\n",
      "0 Spark 1\n",
      "0 deep learning 1\n",
      "1 Postgres 2\n",
      "1 HBase 2\n",
      "1 MongoDB 2\n",
      "1 neural networks 2\n",
      "1 machine learning 2\n",
      "1 artificial intelligence 1\n",
      "1 scipy 1\n",
      "1 NoSQL 1\n",
      "1 decision trees 1\n",
      "1 deep learning 1\n",
      "1 MySQL 1\n",
      "1 Cassandra 1\n",
      "1 databases 1\n",
      "1 numpy 1\n",
      "2 regression 3\n",
      "2 scikit-learn 2\n",
      "2 libsvm 2\n",
      "2 R 2\n",
      "2 Python 2\n",
      "2 Mahout 1\n",
      "2 mathematics 1\n",
      "2 Haskell 1\n",
      "2 support vector machines 1\n",
      "3 statistics 3\n",
      "3 probability 3\n",
      "3 pandas 2\n",
      "3 R 2\n",
      "3 statsmodels 2\n",
      "3 Python 2\n",
      "3 artificial intelligence 1\n",
      "3 theory 1\n",
      "3 C++ 1\n"
     ]
    }
   ],
   "source": [
    "lda_model.top_words_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hadoop', 'Big Data', 'HBase', 'Java', 'Spark', 'Storm', 'Cassandra']\n",
      "Big Data and programming languages 7\n",
      "\n",
      "['NoSQL', 'MongoDB', 'Cassandra', 'HBase', 'Postgres']\n",
      "Python and statistics 5\n",
      "\n",
      "['Python', 'scikit-learn', 'scipy', 'numpy', 'statsmodels', 'pandas']\n",
      "Python and statistics 2\n",
      "databases 2\n",
      "machine learning 2\n",
      "\n",
      "['R', 'Python', 'statistics', 'regression', 'probability']\n",
      "machine learning 3\n",
      "databases 2\n",
      "\n",
      "['machine learning', 'regression', 'decision trees', 'libsvm']\n",
      "Python and statistics 2\n",
      "databases 2\n",
      "\n",
      "['Python', 'R', 'Java', 'C++', 'Haskell', 'programming languages']\n",
      "Big Data and programming languages 3\n",
      "databases 3\n",
      "\n",
      "['statistics', 'probability', 'mathematics', 'theory']\n",
      "machine learning 3\n",
      "databases 1\n",
      "\n",
      "['machine learning', 'scikit-learn', 'Mahout', 'neural networks']\n",
      "Python and statistics 2\n",
      "databases 2\n",
      "\n",
      "['neural networks', 'deep learning', 'Big Data', 'artificial intelligence']\n",
      "Python and statistics 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['Hadoop', 'Java', 'MapReduce', 'Big Data']\n",
      "Big Data and programming languages 4\n",
      "\n",
      "['statistics', 'R', 'statsmodels']\n",
      "machine learning 3\n",
      "\n",
      "['C++', 'deep learning', 'artificial intelligence', 'probability']\n",
      "machine learning 3\n",
      "Big Data and programming languages 1\n",
      "\n",
      "['pandas', 'R', 'Python']\n",
      "machine learning 3\n",
      "\n",
      "['databases', 'HBase', 'Postgres', 'MySQL', 'MongoDB']\n",
      "Python and statistics 5\n",
      "\n",
      "['libsvm', 'regression', 'support vector machines']\n",
      "databases 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda_model.assign_topics(topic_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Counter({0: 7, 1: 0, 2: 0, 3: 0}),\n",
       " Counter({0: 0, 1: 5, 2: 0, 3: 0}),\n",
       " Counter({0: 0, 1: 2, 2: 2, 3: 2}),\n",
       " Counter({0: 0, 1: 0, 2: 2, 3: 3}),\n",
       " Counter({0: 0, 1: 2, 2: 2, 3: 0}),\n",
       " Counter({0: 3, 1: 0, 2: 3, 3: 0}),\n",
       " Counter({0: 0, 1: 0, 2: 1, 3: 3}),\n",
       " Counter({0: 0, 1: 2, 2: 2, 3: 0}),\n",
       " Counter({0: 1, 1: 3, 2: 0, 3: 0}),\n",
       " Counter({0: 4, 1: 0, 2: 0, 3: 0}),\n",
       " Counter({0: 0, 1: 0, 2: 0, 3: 3}),\n",
       " Counter({0: 1, 1: 0, 2: 0, 3: 3}),\n",
       " Counter({0: 0, 1: 0, 2: 0, 3: 3}),\n",
       " Counter({0: 0, 1: 5, 2: 0, 3: 0}),\n",
       " Counter({0: 0, 1: 0, 2: 3, 3: 0})]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.document_topic_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
