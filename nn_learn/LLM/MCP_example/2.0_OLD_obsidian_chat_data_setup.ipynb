{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9338839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import chromadb\n",
    "import openai\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd9d474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26d15e",
   "metadata": {},
   "source": [
    "## Setup vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4201bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "# For persistence to disk\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f949e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                model_name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da2bc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.01191489,  0.00940918,  0.00219249, ...,  0.02120901,\n",
       "         0.02228288, -0.04032141], shape=(1536,), dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_ef('tere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a31698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection with the custom embedding function\n",
    "# collection = client.create_collection(\n",
    "#     name=\"obsidian_notes\",\n",
    "#     embedding_function=openai_ef\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8a2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load if this has been setup\n",
    "collection = client.get_collection(\n",
    "    name=\"obsidian_notes\",\n",
    "    embedding_function=openai_ef  # Your embedding function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fc15d",
   "metadata": {},
   "source": [
    "## Upload documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cbd5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(file_path):\n",
    "    \"\"\"Get file metadata needed for change detection\"\"\"\n",
    "    stats = os.stat(file_path)\n",
    "    return {\n",
    "        \"modified_time\": stats.st_mtime,\n",
    "        \"size\": stats.st_size,\n",
    "        \"hash\": calculate_file_hash(file_path)\n",
    "    }\n",
    "\n",
    "def calculate_file_hash(file_path):\n",
    "    \"\"\"Calculate MD5 hash of file contents\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "def save_file_metadata(file_path):\n",
    "    \"\"\"Save metadata for a single file\"\"\"\n",
    "    file_info = get_file_info(file_path)\n",
    "    \n",
    "    # Create a unique filename for the metadata\n",
    "    metadata_filename = hashlib.md5(file_path.encode()).hexdigest() + \".json\"\n",
    "    metadata_path = os.path.join(METADATA_DIR, metadata_filename)\n",
    "    \n",
    "    # Save the metadata\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"file_path\": file_path,\n",
    "            \"info\": file_info,\n",
    "            \"last_processed\": time.time()\n",
    "        }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16e1cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tags_from_note(note_content):\n",
    "    \"\"\"Extract tags from an Obsidian note (inline hashtags)\"\"\"    \n",
    "    # This regex finds hashtags but ignores URLs and code blocks\n",
    "    inline_tags = re.findall(r'(?<!`|\\w)#([a-zA-Z0-9_/-]+)', note_content)\n",
    "\n",
    "    return list(inline_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c0af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_note(file_path, collection):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    if len(content)>5:\n",
    "        # Generate a unique ID based on file path\n",
    "        note_id = hashlib.md5(file_path.encode()).hexdigest()\n",
    "\n",
    "        # Extract title from filename\n",
    "        title = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        folder= file_path.replace('\\\\', '/').split('/')[1]\n",
    "        tags = extract_tags_from_note(content)\n",
    "\n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            ids=[note_id],\n",
    "            documents=[content],\n",
    "            metadatas=[{\"title\": title,\"folder\":folder, \"path\": file_path,\n",
    "                        \"last_updated\": time.time(),\n",
    "                        \"tags\": \",\".join(tags)}]\n",
    "        )\n",
    "\n",
    "        # Save individual file metadata\n",
    "        save_file_metadata(file_path)\n",
    "    else:\n",
    "        print(f\"file {file_path} has length of smaller than 5, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eb3707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store metadata about processed files\n",
    "METADATA_DIR = \"./note_metadata\"\n",
    "os.makedirs(METADATA_DIR, exist_ok=True)\n",
    "\n",
    "def process_notes_initially(vault_path, collection):\n",
    "    # Track all processed files\n",
    "    processed_files = {}\n",
    "    \n",
    "    for root, _, files in os.walk(vault_path):\n",
    "        for i, file in enumerate(files):\n",
    "            if i%10==0:\n",
    "                print(f\"working on file {i}\")\n",
    "            if file.endswith('.md'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    # Process and add the note\n",
    "                    process_single_note(file_path, collection)\n",
    "                    \n",
    "                    # Record this file as processed\n",
    "                    file_info = get_file_info(file_path)\n",
    "                    processed_files[file_path] = file_info\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Save metadata about all processed files\n",
    "    with open(os.path.join(METADATA_DIR, \"processed_files.json\"), \"w\") as f:\n",
    "        json.dump(processed_files, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "399a1748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 30\n",
      "working on file 40\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 30\n",
      "working on file 40\n",
      "working on file 50\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 10\n"
     ]
    }
   ],
   "source": [
    "process_notes_initially('Obsidian Vault/', collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521e905",
   "metadata": {},
   "source": [
    "### detect changes to change only this part in db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e4328bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_note_in_collection(file_path, collection):\n",
    "    \"\"\"Update an existing note in the collection\"\"\"\n",
    "    # Generate the consistent ID for this file\n",
    "    note_id = hashlib.md5(file_path.encode()).hexdigest()\n",
    "    \n",
    "    # Read the updated content\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract title from filename\n",
    "    title = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Update the note in collection\n",
    "    # First, check if it exists\n",
    "    results = collection.get(ids=[note_id])\n",
    "    \n",
    "    if len(results['ids']) > 0:\n",
    "        # Update existing entry\n",
    "        collection.update(\n",
    "            ids=[note_id],\n",
    "            documents=[content],\n",
    "            metadatas=[{\"title\": title, \"path\": file_path, \"last_updated\": time.time()}]\n",
    "        )\n",
    "    else:\n",
    "        # Add as new if not found (shouldn't happen normally)\n",
    "        collection.add(\n",
    "            ids=[note_id],\n",
    "            documents=[content],\n",
    "            metadatas=[{\"title\": title, \"path\": file_path, \"last_updated\": time.time()}]\n",
    "        )\n",
    "    \n",
    "    # Update file metadata\n",
    "    save_file_metadata(file_path)\n",
    "\n",
    "\n",
    "def remove_note_from_collection(file_path, collection):\n",
    "    \"\"\"Remove a note from the collection\"\"\"\n",
    "    # Generate the consistent ID for this file\n",
    "    note_id = hashlib.md5(file_path.encode()).hexdigest()\n",
    "    \n",
    "    # Remove from collection\n",
    "    collection.delete(ids=[note_id])\n",
    "    \n",
    "    # Remove metadata file\n",
    "    metadata_filename = hashlib.md5(file_path.encode()).hexdigest() + \".json\"\n",
    "    metadata_path = os.path.join(metadata_dir, metadata_filename)\n",
    "    \n",
    "    if os.path.exists(metadata_path):\n",
    "        os.remove(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11d1c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_changed_notes(vault_path, collection):\n",
    "    # Load previously processed files\n",
    "    try:\n",
    "        with open(os.path.join(metadata_dir, \"processed_files.json\"), \"r\") as f:\n",
    "            processed_files = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        processed_files = {}\n",
    "    \n",
    "    # Track current files\n",
    "    current_files = set()\n",
    "    \n",
    "    # Check all files in the vault\n",
    "    for root, _, files in os.walk(vault_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.md'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                current_files.add(file_path)\n",
    "                \n",
    "                # Get current file info\n",
    "                current_info = get_file_info(file_path)\n",
    "                \n",
    "                if file_path in processed_files:\n",
    "                    # File exists in our records, check if modified\n",
    "                    old_info = processed_files[file_path]\n",
    "                    \n",
    "                    if (current_info[\"modified_time\"] != old_info[\"modified_time\"] or\n",
    "                        current_info[\"size\"] != old_info[\"size\"] or\n",
    "                        current_info[\"hash\"] != old_info[\"hash\"]):\n",
    "                        \n",
    "                        print(f\"File changed: {file_path}\")\n",
    "                        \n",
    "                        # Update in collection\n",
    "                        update_note_in_collection(file_path, collection)\n",
    "                        \n",
    "                        # Update metadata\n",
    "                        processed_files[file_path] = current_info\n",
    "                else:\n",
    "                    # New file\n",
    "                    print(f\"New file: {file_path}\")\n",
    "                    process_single_note(file_path, collection)\n",
    "                    processed_files[file_path] = current_info\n",
    "    \n",
    "    # Check for deleted files\n",
    "    deleted_files = set(processed_files.keys()) - current_files\n",
    "    for file_path in deleted_files:\n",
    "        print(f\"File deleted: {file_path}\")\n",
    "        remove_note_from_collection(file_path, collection)\n",
    "        del processed_files[file_path]\n",
    "    \n",
    "    # Save updated metadata\n",
    "    with open(os.path.join(metadata_dir, \"processed_files.json\"), \"w\") as f:\n",
    "        json.dump(processed_files, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae82816",
   "metadata": {},
   "source": [
    "## MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faf6bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import datetime\n",
    "from contextlib import asynccontextmanager\n",
    "from collections.abc import AsyncIterator\n",
    "from dataclasses import dataclass\n",
    "from mcp.server.fastmcp import FastMCP, Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79661968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataclass to hold our dependencies\n",
    "@dataclass\n",
    "class AppContext:\n",
    "    collection: chromadb.Collection\n",
    "\n",
    "@asynccontextmanager\n",
    "async def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:\n",
    "    \"\"\"Set up and tear down the Chroma DB connection\"\"\"\n",
    "    # Initialize Chroma client\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    \n",
    "    # Get the collection\n",
    "    collection = client.get_collection(\"obsidian_notes\", embedding_function=openai_ef)\n",
    "    \n",
    "    try:\n",
    "        yield AppContext(collection=collection)\n",
    "    finally:\n",
    "        # Any cleanup if needed\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "034be191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MCP server with our lifespan\n",
    "mcp = FastMCP(\"ObsidianNotes\", lifespan=app_lifespan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5cb14",
   "metadata": {},
   "source": [
    "### retriever tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66dd1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.tool()\n",
    "def get_sample_note() -> str:\n",
    "    \"\"\"Get a single sample note to examine its structure\"\"\"\n",
    "    # Get the collection\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    collection = client.get_collection(\"obsidian_notes\", embedding_function=openai_ef)\n",
    "    \n",
    "    # Get just one document\n",
    "    results = collection.get(limit=1)\n",
    "    \n",
    "    if results[\"ids\"] and len(results[\"ids\"]) > 0:\n",
    "        sample_id = results[\"ids\"][0]\n",
    "        sample_doc = results[\"documents\"][0]\n",
    "        sample_metadata = results[\"metadatas\"][0] if \"metadatas\" in results and results[\"metadatas\"] else {}\n",
    "        \n",
    "        # Format the output to show structure\n",
    "        output = \"Sample Note Structure:\\n\\n\"\n",
    "        output += f\"ID: {sample_id}\\n\\n\"\n",
    "        \n",
    "        output += \"Metadata Fields:\\n\"\n",
    "        for key, value in sample_metadata.items():\n",
    "            output += f\"- {key}: {type(value).__name__} = {value}\\n\"\n",
    "        \n",
    "        output += \"\\nDocument Content (first 200 chars):\\n\"\n",
    "        output += sample_doc\n",
    "        \n",
    "        # If embeddings exist, show their shape\n",
    "        if \"embeddings\" in results and results[\"embeddings\"]:\n",
    "            embedding = results[\"embeddings\"][0]\n",
    "            output += f\"\\n\\nEmbedding: Vector of length {len(embedding)}\"\n",
    "        \n",
    "        return output\n",
    "    else:\n",
    "        return \"No documents found in the collection.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a4862be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_search_results(vector_results):\n",
    "    formatted_results = []\n",
    "    for i, doc in enumerate(vector_results[\"documents\"][0]):\n",
    "        doc_id = vector_results[\"ids\"][0][i]\n",
    "        folder = vector_results[\"metadatas\"][0][i]['folder']\n",
    "        title = vector_results[\"metadatas\"][0][i]['title']\n",
    "        # Convert distance to similarity score (closer to 1 is better)\n",
    "        similarity = 1.0 / (1.0 + vector_results[\"distances\"][0][i]) if \"distances\" in vector_results else \"N/A\"\n",
    "\n",
    "        # Add formatted result\n",
    "        formatted_results.append(\n",
    "            f\"Note ID: {doc_id}\\n\"\n",
    "            f\"Title: {title}\\n\"\n",
    "            f\"Folder: {folder}\\n\"\n",
    "            f\"Similarity: {similarity:.4f}\\n\\n\"\n",
    "            f\"{doc}\\n\"\n",
    "            f\"---\")\n",
    "    return \"\\n\".join(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac38029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if need to remove tool\n",
    "# if \"vector_search\" in mcp._tool_manager._tools:\n",
    "#     del mcp._tool_manager._tools[\"vector_search\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6068317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.tool()\n",
    "def vector_search(query: str, n_results: int = 5, folder: str = None, ) -> str:\n",
    "    \"\"\"\n",
    "    Search notes using both vector similarity\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        n_results: Number of results to return\n",
    "    \"\"\"\n",
    "    # Get the collection\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    collection = client.get_collection(\"obsidian_notes\", embedding_function=openai_ef)\n",
    "        \n",
    "    where_clause = {}\n",
    "    if folder:\n",
    "        where_clause[\"folder\"] = folder\n",
    "    \n",
    "    # 1. Vector search\n",
    "    vector_results = collection.query(\n",
    "        query_embeddings = openai_ef(query),\n",
    "        where=where_clause if where_clause else None,\n",
    "        n_results=n_results * 2  # Get more results to rerank\n",
    "    )\n",
    "    \n",
    "    #Format the results\n",
    "    if vector_results[\"documents\"] and len(vector_results[\"documents\"][0]) > 0:\n",
    "        formatted_results = format_search_results(vector_results)\n",
    "        return formatted_results\n",
    "    else:\n",
    "        return f\"No notes found for query: '{query}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80b50b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/21/25 14:57:25] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/embeddings</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 </span> <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/21/25 14:57:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/embeddings\u001b[0m \u001b[32m\"HTTP/1.1 200 \u001b[0m \u001b]8;id=88871;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=454740;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# res=vector_search('Transparency', folder='Learning')\n",
    "res=vector_search('Transparency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "34606271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note ID: 2266b46d92e2cad2bbd1cc30daf58540\n",
      "Title: Radical candor framework\n",
      "Folder: startup\n",
      "Similarity: 0.4454\n",
      "\n",
      "![[Pasted image 20231115205458.png]]\n",
      "\n",
      "Radical candor is a quadrant where you can be honest (give negative feedback) but at the same time be respectful. Being aggressive or false positive (giving empathy too much or being silent) is not going to help.\n",
      "\n",
      "\"Radical Candor: Be a Kickass Boss Without Losing Your Humanity, Kim Malone Scott, page 38\n",
      "\n",
      "[[Bill Campbell coaching style and ideas]]\n",
      "[[Google excellent team factors]]\n",
      "[[Intel operating style]]\n",
      "\n",
      "#psychological_safety \n",
      "#management \n",
      "#leadership \n",
      "#google \n",
      "\n",
      "---\n",
      "Note ID: 40329e717d71fe67393cefead7af1d4b\n",
      "Title: Algorithm governance questions\n",
      "Folder: AI\n",
      "Similarity: 0.4430\n",
      "\n",
      "I believe that the experience of kidney allocation may be able to shed some light:\n",
      "1. Participation by stakeholders\n",
      "2. Transparency measures\n",
      "3. Forecasting of system impacts\n",
      "4. Auditing of what actually happens once the system is turned on\n",
      "\n",
      "\"Voices in the Code: A Story about People, Their Values, and the Algorithm They Made\", David G. Robinson, page 37\n",
      "\n",
      "\n",
      "[[Automated hiring software is mistakenly rejecting millions of viable job candidates]]\n",
      "\n",
      "#AI \n",
      "#algorithms\n",
      "#governance\n",
      "#bias \n",
      "#fairness\n",
      "#openness \n",
      "---\n",
      "Note ID: 4ddc912de615f07ed2defc2c932e0bbd\n",
      "Title: Patron-client relationship is not welcomed in politics\n",
      "Folder: Learning\n",
      "Similarity: 0.4427\n",
      "\n",
      "When voters feel that patron-client relationship (big donations from private companies to some politician) is forming they feel that politicians are owing something to their patron and thus politician doesn't make good job.\n",
      "\n",
      "\"Worlds Hidden in Plain Sight: The Evolving Idea of Complexity at the Santa Fe Institute, 1984–2019\", David C. Krakauer, page 245\n",
      "\n",
      "[[Economic relationships are power relationships]]\n",
      "\n",
      "#patron_client\n",
      "#relationships \n",
      "#politics\n",
      "#lobby\n",
      "\n",
      "---\n",
      "Note ID: b18e35ead734361b7a9179c6dc0656fb\n",
      "Title: Deliberate polling, deliberate inclusion\n",
      "Folder: Society\n",
      "Similarity: 0.4401\n",
      "\n",
      "Fishkin and a colleague say that deliberation ideally aims to be:\n",
      "• Informed (and thus informative): Arguments should be supported by\n",
      "appropriate and reasonably accurate factual claims.\n",
      "• Balanced: Arguments should be met by contrary arguments.\n",
      "• Conscientious: The participants should be willing to talk and listen, with\n",
      "civility and respect.\n",
      "• Substantive: Arguments should be considered sincerely on their merits,\n",
      "not on how they are made or who is making them.\n",
      "• Comprehensive: All points of view held by significant portions of the population\n",
      "should receive attention.\n",
      "\n",
      "Fishkin, James S., and Robert C. Luskin. 2005. “Experimenting with a Democratic\n",
      "Ideal: Deliberative Polling and Public Opinion.” Acta Politica 40(3): 284–98. DOI:\n",
      "https://doi.org/10.1057/palgrave.ap.5500121.\n",
      "\n",
      "\"Voices in the Code: A Story about People, Their Values, and the Algorithm They Made\", David G. Robinson, page 38\n",
      "\n",
      "[[Why liberalism works]]\n",
      "\n",
      "\n",
      "#inclusion\n",
      "#democracy \n",
      "#critical_thinking \n",
      "#cooperation \n",
      "#governance \n",
      "---\n",
      "Note ID: 528778fd860e04ad3bde8b868ef444e8\n",
      "Title: People trust who understands them\n",
      "Folder: Marketing\n",
      "Similarity: 0.4352\n",
      "\n",
      "When we empathize with our customers’ dilemma, we create a bond of trust. People trust those who understand them, and they trust brands that understand them too.\n",
      "\n",
      "\"Building a StoryBrand: Clarify Your Message So Customers Will Listen\", Donald Miller, page 62\n",
      "\n",
      "[[Our success is coevolution]]\n",
      "\n",
      "\n",
      "#storybrand\n",
      "#relationships \n",
      "#trust\n",
      "\n",
      "---\n",
      "Note ID: b96a0389c3cac7dc4d28c23c050a0135\n",
      "Title: We should avoid tyranny narrow visions\n",
      "Folder: technology\n",
      "Similarity: 0.4340\n",
      "\n",
      "We should avoid that somebody who has a lot of power can set visions. They should include many views and feedbacks to vision and take in consideration impacts to the ones who have less power. Diversity and democracy is for the help.\n",
      "\n",
      " \"Power and Progress: Our Thousand-Year Struggle Over Technology and Prosperity\", Daron Acemoğlu, Simon Johnson, page 87\n",
      "[[Tech leaders are vision oligarchy]]\n",
      "[[Shared visions can trap us in using technology which might not be best]]\n",
      "\n",
      "\n",
      "#democracy\n",
      "#vision \n",
      "#vision_blindness \n",
      "#diversity\n",
      "\n",
      "---\n",
      "Note ID: f43b7a66c8f76ba6b187e1411f31e8a3\n",
      "Title: Continuous performance management and compensation\n",
      "Folder: startup\n",
      "Similarity: 0.4280\n",
      "\n",
      "![[Pasted image 20231028164756.png]]\n",
      "\n",
      "As companies transition to continuous performance management, OKRs and CFRs become mostly independent from compensation and formal evaluations.\n",
      "\n",
      "\"Measure What Matters: How Google, Bono, and the Gates Foundation Rock the World with OKRs\", John Doerr, page 147\n",
      "[[Two types of OKRs]]\n",
      "[[Andy Grove's OKR hygiene]]\n",
      "[[OKRs should be public]]\n",
      "\n",
      "\n",
      "#okr \n",
      "#feedback \n",
      "#performance \n",
      "#leadership \n",
      "#management \n",
      "\n",
      "---\n",
      "Note ID: 2e86067ee3480bd6f49a0e6a0033a615\n",
      "Title: bullshit is not a lie\n",
      "Folder: Society\n",
      "Similarity: 0.4266\n",
      "\n",
      "when you lie you care what is truth and try to hide it, you have to keep track what is lie and what is truth. on bullshitting you don't care. they just want to talk and hear themselves talking\n",
      "\n",
      "\"The AI Mirror: How to Reclaim Our Humanity in an Age of Machine Thinking\", Shannon Vallor, page 105\n",
      "\n",
      "[[Wealth of information creates a poverty of attention and a need to allocate that attention efficiently]]\n",
      "\n",
      "[[Information overload might create pancake people]]\n",
      "\n",
      "#information \n",
      "#information_consumption \n",
      "#free_information \n",
      "#bullshit\n",
      "\n",
      "\n",
      "---\n",
      "Note ID: 0195e1428fcc541742126cd5bc552e0d\n",
      "Title: Lack of trust slows economy down\n",
      "Folder: Economy\n",
      "Similarity: 0.4265\n",
      "\n",
      "Dr. Peter Kollock, associate professor and vice chair of the department of sociology at the University of California at Los Angeles, conducted a study in the mid-1990s that demonstrated this. He set up a trading game where people traded goods in two different environments. In one environment (low uncertainty), participants knew the value of what they were trading, and in the other (high uncertainty), they did not. Kollock’s study achieved two fascinating results: (1) people have a greater tendency to form interpersonal commitments in a high uncertainty environment, and (2) they tend to forgo potentially more profitable exchanges with untested partners in favor of trades with partners who have demonstrated their trustworthiness in previous transactions. In short, economic activity in times of high uncertainty slows down and traders become risk-averse.\n",
      "\n",
      "\"The Emergence of Exchange Structures: An Experimental Study of Uncertainty, Commitment, and Trust\", Peter Kollock, American Journal of Sociology, [Vol. 100, No. 2 (Sep., 1994)](https://www.jstor.org/stable/i329123), pp. 313-345 (33 pages)\n",
      "\n",
      "Referred from \"How: Why How We Do Anything Means Everything\", Dov Seidman, page 163\n",
      "\n",
      "[[People trust who understands them]]\n",
      "\n",
      "\n",
      "\n",
      "#trust \n",
      "#economy \n",
      "#exchange\n",
      "#trade \n",
      "---\n",
      "Note ID: 14ea58714ac430ad5b40ebb0c062c860\n",
      "Title: Intel operating style\n",
      "Folder: startup\n",
      "Similarity: 0.4248\n",
      "\n",
      "![[Pasted image 20231028170348.png]]\n",
      "\n",
      "\n",
      "[[Andy Grove's OKR hygiene]]\n",
      "[[Two types of OKRs]]\n",
      "[[OKRs should be public]]\n",
      "\n",
      "\n",
      "#andy_grove \n",
      "#okr \n",
      "#management \n",
      "#leadership \n",
      "#performance \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2f44175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Note Structure:\n",
      "\n",
      "ID: 40329e717d71fe67393cefead7af1d4b\n",
      "\n",
      "Metadata Fields:\n",
      "- folder: str = AI\n",
      "- tags: str = AI,algorithms,governance,bias,fairness,openness\n",
      "- path: str = Obsidian Vault/AI\\Algorithm governance questions.md\n",
      "- last_updated: float = 1747735517.90906\n",
      "- title: str = Algorithm governance questions\n",
      "\n",
      "Document Content (first 200 chars):\n",
      "I believe that the experience of kidney allocation may be able to shed some light:\n",
      "1. Participation by stakeholders\n",
      "2. Transparency measures\n",
      "3. Forecasting of system impacts\n",
      "4. Auditing of what actually happens once the system is turned on\n",
      "\n",
      "\"Voices in the Code: A Story about People, Their Values, and the Algorithm They Made\", David G. Robinson, page 37\n",
      "\n",
      "\n",
      "[[Automated hiring software is mistakenly rejecting millions of viable job candidates]]\n",
      "\n",
      "#AI \n",
      "#algorithms\n",
      "#governance\n",
      "#bias \n",
      "#fairness\n",
      "#openness \n"
     ]
    }
   ],
   "source": [
    "print(get_sample_note())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7050e34",
   "metadata": {},
   "source": [
    "### Browse notes tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c67079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.tool()\n",
    "def browse_notes(folder: str = None, tag: str = None, limit: int = 10, offset: int = 0) -> str:\n",
    "    \"\"\"\n",
    "    Browse notes by category or tag\n",
    "    \n",
    "    Args:\n",
    "        category: Optional category to filter by\n",
    "        tag: Optional tag to filter by (will match if tag string contains this value)\n",
    "        limit: Maximum number of notes to return\n",
    "        offset: Number of notes to skip (for pagination)\n",
    "    \"\"\"\n",
    "    # Get the collection\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    collection = client.get_collection(\"obsidian_notes\", embedding_function=openai_ef)\n",
    "    \n",
    "    # Build where clause for filtering\n",
    "    where_clause = {}\n",
    "    if folder:\n",
    "        where_clause[\"folder\"] = folder\n",
    "    \n",
    "    # For tag filtering, we'll need to handle it manually since we want substring matching\n",
    "    \n",
    "    # Get all notes that match the category filter (or all notes if no category filter)\n",
    "    results = collection.get(\n",
    "        where=where_clause if where_clause else None,\n",
    "        limit=1000  # Get a larger batch to filter manually\n",
    "    )\n",
    "    \n",
    "    # Filter by tag if specified\n",
    "    filtered_indices = []\n",
    "    if tag and results[\"metadatas\"]:\n",
    "        for i, metadata in enumerate(results[\"metadatas\"]):\n",
    "            # Check if tags field exists and contains the tag substring\n",
    "            if \"tags\" in metadata:\n",
    "                tags_str = metadata[\"tags\"]\n",
    "                if isinstance(tags_str, str) and tag.lower() in tags_str.lower():\n",
    "                    filtered_indices.append(i)\n",
    "    else:\n",
    "        # If no tag filter, use all results\n",
    "        filtered_indices = list(range(len(results[\"documents\"])))\n",
    "    \n",
    "    # Apply pagination\n",
    "    start_idx = min(offset, len(filtered_indices))\n",
    "    end_idx = min(start_idx + limit, len(filtered_indices))\n",
    "    page_indices = filtered_indices[start_idx:end_idx]\n",
    "    \n",
    "    # Format the results\n",
    "    if page_indices:\n",
    "        formatted_results = []\n",
    "        for idx in page_indices:\n",
    "            doc = results[\"documents\"][idx]\n",
    "            doc_id = results[\"ids\"][idx]\n",
    "            metadata = results[\"metadatas\"][idx]\n",
    "            \n",
    "            # Extract title and other metadata\n",
    "            title = metadata.get(\"title\", \"Untitled\")\n",
    "            tags = metadata.get(\"tags\", \"\")\n",
    "            category = metadata.get(\"category\", \"Uncategorized\")\n",
    "            \n",
    "            # Create a preview\n",
    "            preview = doc[:100] + \"...\" if len(doc) > 100 else doc\n",
    "            \n",
    "            # Format the result\n",
    "            formatted_results.append(\n",
    "                f\"## {title}\\n\"\n",
    "                f\"ID: {doc_id}\\n\"\n",
    "                f\"Category: {category}\\n\"\n",
    "                f\"Tags: {tags}\\n\\n\"\n",
    "                f\"Preview: {preview}\\n\"\n",
    "                f\"---\"\n",
    "            )\n",
    "        \n",
    "        # Add pagination info\n",
    "        pagination_info = f\"Showing results {start_idx+1}-{end_idx} of {len(filtered_indices)}. \"\n",
    "        if end_idx < len(filtered_indices):\n",
    "            pagination_info += f\"Use offset={end_idx} to see more.\"\n",
    "        \n",
    "        return pagination_info + \"\\n\\n\" + \"\\n\".join(formatted_results)\n",
    "    else:\n",
    "        return f\"No notes found with the specified filters.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61670c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Showing results 1-10 of 12. Use offset=10 to see more.\\n\\n## Algorithm governance questions\\nID: 40329e717d71fe67393cefead7af1d4b\\nCategory: Uncategorized\\nTags: AI,algorithms,governance,bias,fairness,openness\\n\\nPreview: I believe that the experience of kidney allocation may be able to shed some light:\\n1. Participation ...\\n---\\n## Automated hiring software is mistakenly rejecting millions of viable job candidates\\nID: 2ea949c8aef6a2dc251c678b020c7851\\nCategory: Uncategorized\\nTags: AI_hype,AI,ai_hiring,bias,society\\n\\nPreview: \"The study’s authors identify a number of factors blocking people from employment, but say automated...\\n---\\n## C. Clarke’s third law of prediction\\nID: b4cbf197a6f5e47aee89afa711fffb74\\nCategory: Uncategorized\\nTags: prediction,technology\\n\\nPreview: Arthur\\xa0C. Clarke’s third law of prediction is, famously, “Any sufficiently advanced technology is in...\\n---\\n## Can we create tools to augment our empathy\\nID: 585c1a57d10e93f11ead58f1656405ee\\nCategory: Uncategorized\\nTags: meaning,intelligence,AI,AI_hype\\n\\nPreview: \\n“We know how to create tools to augment our intelligence, but can we create tools to augment our em...\\n---\\n## Eetika printsiibib peavad olema seotud sisemiste väärtustega\\nID: aa69ac89dc6df99c1190979c79594188\\nCategory: Uncategorized\\nTags: eetika,AI\\n\\nPreview: Näiteks läbipaistvuse põhimõte peab olema seotud sisemiste väärtustega (sooviga olla läbipaistev, ee...\\n---\\n## ELIZA had already problems human considering it as a conscious entity\\nID: d8f425b5f135ead69b82774429f5f3ff\\nCategory: Uncategorized\\nTags: anthropomorphism,eetika,conscious,rationality\\n\\nPreview: ELIZA was a chatbot in 1960s, very simple system, but already then users who knew how it worked, ten...\\n---\\n## Humans surrender autonomy when we use autonomous machines\\nID: 9501bb55c2c54e944c804ad8ca179a4b\\nCategory: Uncategorized\\nTags: AI,AI_hype,ai_hiring,autonomy\\n\\nPreview:  \"... the computer scientist Joseph Weizenbaum, the inventor of the very first AI mirror, the ELIZA ...\\n---\\n## Language might help to create anthropomorphism\\nID: 972911915334f87718126f6a829f48a9\\nCategory: Uncategorized\\nTags: anthropomorphism,language,relationship_wtih_nature\\n\\nPreview: In English humans are considered living entities. Use word \"it\" and you turn living organism into su...\\n---\\n## Veebist andmete kraapimine masinõppe mudelite jaoks võib minna eetika piiride isegi kui on seaduslik\\nID: 07835ef18d95b70d649ab1ddb62e589b\\nCategory: Uncategorized\\nTags: eetika,ai\\n\\nPreview: Veebist andmete kraapimine ML jaoks on mugav ja odav viis andmete kogumiseks, võib olla seaduslik, k...\\n---\\n## We are so used to mimic states of other minds we interact\\nID: a0f7b458c2263ec75ac73f3e01c6ff4b\\nCategory: Uncategorized\\nTags: human_interaction,anthropomorphism\\n\\nPreview: This is a reason why we so easily attribute human qualities to machines, chatbots. It is our genes t...\\n---'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "browse_notes('AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7e32c",
   "metadata": {},
   "source": [
    "### get a single note tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bafa87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.resource(\"note://{note_id}\")\n",
    "def get_note(note_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the full content of a specific note by ID\n",
    "    \n",
    "    Args:\n",
    "        note_id: The ID of the note to retrieve\n",
    "    \"\"\"\n",
    "    # Get the collection\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    collection = client.get_collection(\"obsidian_notes\", embedding_function=openai_ef)\n",
    "    \n",
    "    # Query for the specific note\n",
    "    result = collection.get(\n",
    "        ids=[note_id]\n",
    "    )\n",
    "    \n",
    "    # Check if note was found\n",
    "    if result[\"documents\"] and len(result[\"documents\"]) > 0:\n",
    "        # Get the note content and metadata\n",
    "        content = result[\"documents\"][0]\n",
    "        metadata = result[\"metadatas\"][0] if result[\"metadatas\"] else {}\n",
    "        \n",
    "        # Format with metadata if available\n",
    "        title = metadata.get(\"title\", \"Untitled\")\n",
    "        tags = metadata.get(\"tags\", \"\")\n",
    "        \n",
    "        return f\"# {title}\\n\\nTags: {tags}\\n\\n{content}\"\n",
    "    else:\n",
    "        return f\"Note with ID '{note_id}' not found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86f3383b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Algorithm governance questions\\n\\nTags: AI,algorithms,governance,bias,fairness,openness\\n\\nI believe that the experience of kidney allocation may be able to shed some light:\\n1. Participation by stakeholders\\n2. Transparency measures\\n3. Forecasting of system impacts\\n4. Auditing of what actually happens once the system is turned on\\n\\n\"Voices in the Code: A Story about People, Their Values, and the Algorithm They Made\", David G. Robinson, page 37\\n\\n\\n[[Automated hiring software is mistakenly rejecting millions of viable job candidates]]\\n\\n#AI \\n#algorithms\\n#governance\\n#bias \\n#fairness\\n#openness '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_note('40329e717d71fe67393cefead7af1d4b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b012b",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "207c382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @mcp.prompt()\n",
    "# def explore_topic(topic: str) -> str:\n",
    "#     return f\"\"\"\n",
    "#     System: You are a helpful assistant that helps users explore topics (named \"folder\" in database) in their notes. \n",
    "#     Use the vector_search tool to find relevant notes and browse_notes to explore categories.\n",
    "#     When showing specific notes, use the note:// resource to get full content.\n",
    "    \n",
    "#     User: I want to learn more about {topic}\n",
    "    \n",
    "#     Assistant: I'd be happy to help you explore this topic. I'll search your notes to find relevant information.\n",
    "#     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2016ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp.server.fastmcp.prompts import base\n",
    "\n",
    "@mcp.prompt()\n",
    "def explore_topic(topic: str) -> list[base.Message]:\n",
    "    return [\n",
    "        # Include system instructions in the first user message instead\n",
    "        base.Message(\n",
    "            role=\"user\",\n",
    "            content=f\"I want to learn more about {topic}. Please act as a helpful assistant that helps me explore topics in my notes. \"\n",
    "                   f\"Use the vector_search tool to find relevant notes and browse_notes to explore categories (which are named as folder in database). \"\n",
    "                   f\"When showing specific notes, use the note:// resource to get full content.\"\n",
    "        ),\n",
    "        base.Message(\n",
    "            role=\"assistant\",\n",
    "            content=\"I'd be happy to help you explore this topic. I'll search your notes to find relevant information.\"\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9630e5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(role='user', content=TextContent(type='text', text='I want to learn more about AI. Please act as a helpful assistant that helps me explore topics in my notes. Use the vector_search tool to find relevant notes and browse_notes to explore categories (which are named as folder in database). When showing specific notes, use the note:// resource to get full content.', annotations=None)),\n",
       " Message(role='assistant', content=TextContent(type='text', text=\"I'd be happy to help you explore this topic. I'll search your notes to find relevant information.\", annotations=None))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_topic('AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5df005",
   "metadata": {},
   "source": [
    "## Run server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b50a4d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server running on default address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [14628]\n",
      "INFO:     Waiting for application startup.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/21/25 14:57:26] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> StreamableHTTP session manager started                  <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">streamable_http_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#109\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/21/25 14:57:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m StreamableHTTP session manager started                  \u001b]8;id=376529;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\u001b\\\u001b[2mstreamable_http_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=143234;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#109\u001b\\\u001b[2m109\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "def run_server():\n",
    "    # Check the supported parameters for your version\n",
    "    mcp.run(transport=\"streamable-http\")\n",
    "#     mcp.run(transport=\"http\")\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = threading.Thread(target=run_server)\n",
    "server_thread.daemon = True  # Allow the thread to exit when the notebook closes\n",
    "server_thread.start()\n",
    "\n",
    "print(\"Server running on default address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263f596",
   "metadata": {},
   "source": [
    "## Make request to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "77cb7f07",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Run the test function\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01masyncio\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m prompt_result = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexplore_topic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmachine learning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\runners.py:191\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "async def test_prompt(prompt_name, arguments):\n",
    "    # Connect to your local server\n",
    "    async with streamablehttp_client(\"http://localhost:8000/mcp\") as (read_stream, write_stream, _):\n",
    "        # Create a session\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            # Initialize the connection\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Get the prompt\n",
    "            prompt_result = await session.get_prompt(prompt_name, arguments)\n",
    "            \n",
    "            # Print the prompt content\n",
    "            print(\"Prompt content:\")\n",
    "            for message in prompt_result.messages:\n",
    "                print(f\"{message.role}: {message.content.text}\")\n",
    "            \n",
    "            return prompt_result\n",
    "\n",
    "# Run the test function\n",
    "import asyncio\n",
    "prompt_result = asyncio.run(test_prompt(\"explore_topic\", {\"topic\": \"machine learning\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "407030f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "def test_prompt_cli(prompt_name, arguments):\n",
    "    # Convert arguments to JSON string\n",
    "    args_json = json.dumps(arguments)\n",
    "    \n",
    "    # Run the MCP CLI command\n",
    "    result = subprocess.run(\n",
    "        [\"mcp\", \"prompt\", \"get\", prompt_name, \"--arguments\", args_json],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    return result.stdout\n",
    "\n",
    "# Test the prompt\n",
    "output = test_prompt_cli(\"explore_topic\", {\"topic\": \"AI\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24325df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/20/25 13:50:42] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Created new transport with session ID:                  <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">streamable_http_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#229\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">229</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         5fa6b50bc87b4321beecdebb3ba349e8                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/20/25 13:50:42]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Created new transport with session ID:                  \u001b]8;id=375782;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\u001b\\\u001b[2mstreamable_http_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=295823;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#229\u001b\\\u001b[2m229\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         5fa6b50bc87b4321beecdebb3ba349e8                        \u001b[2m                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": \"server-error\",\n",
      "  \"error\": {\n",
      "    \"code\": -32600,\n",
      "    \"message\": \"Not Acceptable: Client must accept both application/json and text/event-stream\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def test_prompt_direct(prompt_name, arguments):\n",
    "    # Endpoint for getting a prompt\n",
    "    url = \"http://localhost:8000/mcp/prompt\"\n",
    "    \n",
    "    # Prepare the request payload\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"id\": 1,\n",
    "        \"method\": \"prompt/get\",\n",
    "        \"params\": {\n",
    "            \"name\": prompt_name,\n",
    "            \"arguments\": arguments\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Make the request\n",
    "    response = requests.post(url, json=payload)\n",
    "    \n",
    "    # Return the response\n",
    "    return response.json()\n",
    "\n",
    "# Test the prompt\n",
    "result = test_prompt_direct(\"explore_topic\", {\"topic\": \"AI\"})\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "77381978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/20/25 13:54:35] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Created new transport with session ID:                  <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">streamable_http_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#229\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">229</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         12411ef6bb1f444aa28cc9e8d35e1cb9                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/20/25 13:54:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Created new transport with session ID:                  \u001b]8;id=730849;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\u001b\\\u001b[2mstreamable_http_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=715409;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#229\u001b\\\u001b[2m229\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         12411ef6bb1f444aa28cc9e8d35e1cb9                        \u001b[2m                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"jsonrpc\": \"2.0\",\n",
      "  \"id\": \"server-error\",\n",
      "  \"error\": {\n",
      "    \"code\": -32600,\n",
      "    \"message\": \"Bad Request: Missing session ID\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test a tool with proper headers\n",
    "def call_tool(tool_name, arguments):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json, text/event-stream\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/mcp/tool\",\n",
    "        headers=headers,\n",
    "        json={\n",
    "            \"jsonrpc\": \"2.0\",\n",
    "            \"id\": 1,\n",
    "            \"method\": \"tool/call\",\n",
    "            \"params\": {\n",
    "                \"name\": tool_name,\n",
    "                \"arguments\": arguments\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# Example: Call your vector_search tool\n",
    "result = call_tool(\"vector_search\", {\"query\": \"AI\", \"n_results\": 3})\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "522f5ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization response: 500\n",
      "<Response [500]>\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\python\\mcp\\.venv\\Lib\\site-packages\\requests\\models.py:974\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:345\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    341\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    342\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    343\u001b[39m \n\u001b[32m    344\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\json\\decoder.py:363\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     44\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.json()\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Example: Call your vector_search tool\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m result = \u001b[43mcall_mcp_api\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool/call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvector_search\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marguments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquery\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmachine learning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_results\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mprint\u001b[39m(json.dumps(result, indent=\u001b[32m2\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 44\u001b[39m, in \u001b[36mcall_mcp_api\u001b[39m\u001b[34m(method, params)\u001b[39m\n\u001b[32m     33\u001b[39m response = requests.post(\n\u001b[32m     34\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttp://localhost:8000/mcp\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     35\u001b[39m     headers=headers,\n\u001b[32m   (...)\u001b[39m\u001b[32m     41\u001b[39m     }\n\u001b[32m     42\u001b[39m )\n\u001b[32m     43\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\python\\mcp\\.venv\\Lib\\site-packages\\requests\\models.py:978\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    976\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    977\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m978\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# Create a session ID\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "def call_mcp_api(method, params):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json, text/event-stream\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-MCP-Session-ID\": session_id  # Include session ID in header\n",
    "    }\n",
    "    \n",
    "    # First initialize the session\n",
    "    if method != \"initialize\":\n",
    "        init_response = requests.post(\n",
    "            \"http://localhost:8000/mcp\",\n",
    "            headers=headers,\n",
    "            json={\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": 0,\n",
    "                \"method\": \"initialize\",\n",
    "                \"params\": {\n",
    "                    \"client_name\": \"python-test\",\n",
    "                    \"client_version\": \"1.0.0\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        print(\"Initialization response:\", init_response.status_code)\n",
    "    \n",
    "    # Then make the actual request\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/mcp\",\n",
    "        headers=headers,\n",
    "        json={\n",
    "            \"jsonrpc\": \"2.0\",\n",
    "            \"id\": 1,\n",
    "            \"method\": method,\n",
    "            \"params\": params\n",
    "        }\n",
    "    )\n",
    "    print(response)\n",
    "    return response.json()\n",
    "\n",
    "# Example: Call your vector_search tool\n",
    "result = call_mcp_api(\"tool/call\", {\n",
    "    \"name\": \"vector_search\", \n",
    "    \"arguments\": {\"query\": \"machine learning\", \"n_results\": 3}\n",
    "})\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d66dcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/21/25 09:00:21] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Connecting to StreamableHTTP endpoint:                          <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\client\\streamable_http.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">streamable_http.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\client\\streamable_http.py#461\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">461</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8000/mcp</span>                                       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                      </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/21/25 09:00:21]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Connecting to StreamableHTTP endpoint:                          \u001b]8;id=460684;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\client\\streamable_http.py\u001b\\\u001b[2mstreamable_http.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=125450;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\client\\streamable_http.py#461\u001b\\\u001b[2m461\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttp://localhost:8000/mcp\u001b[0m                                       \u001b[2m                      \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/21/25 09:00:22] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8000/mcp</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 307 Temporary </span>  <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">Redirect\"</span>                                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/21/25 09:00:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://localhost:8000/mcp\u001b[0m \u001b[32m\"HTTP/1.1 307 Temporary \u001b[0m  \u001b]8;id=75870;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=468379;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mRedirect\"\u001b[0m                                                              \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">http://localhost:8000/mcp/</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 500 Internal </span>  <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py#1740\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1740</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">Server Error\"</span>                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttp://localhost:8000/mcp/\u001b[0m \u001b[32m\"HTTP/1.1 500 Internal \u001b[0m  \u001b]8;id=628272;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=980804;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py#1740\u001b\\\u001b[2m1740\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mServer Error\"\u001b[0m                                                          \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: unhandled errors in a TaskGroup (1 sub-exception)\n"
     ]
    }
   ],
   "source": [
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "async def test_mcp():\n",
    "    try:\n",
    "        # Connect to your server\n",
    "        async with streamablehttp_client(\"http://localhost:8000/mcp\") as (read_stream, write_stream, _):\n",
    "            # Create a session\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                # Initialize\n",
    "                await session.initialize()\n",
    "                \n",
    "                # List tools\n",
    "                tools = await session.list_tools()\n",
    "                print(\"Available tools:\", [t.name for t in tools])\n",
    "                \n",
    "                # Call a tool\n",
    "                result = await session.call_tool(\"vector_search\", {\"query\": \"machine learning\", \"n_results\": 3})\n",
    "                print(\"Result:\", result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Run the async function properly\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    await test_mcp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98bbb743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP server started. Open http://localhost:8000 in your browser.\n",
      "Press Ctrl+C to stop the server when done.\n",
      "Starting MCP inspector...\n",
      "Server stopped.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# Function to run the MCP server in a subprocess\n",
    "def run_mcp_server(server_file):\n",
    "    process = subprocess.Popen(\n",
    "        [\"mcp\", \"dev\", server_file],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    # Print output in real-time\n",
    "    def print_output():\n",
    "        while True:\n",
    "            line = process.stdout.readline()\n",
    "            if not line and process.poll() is not None:\n",
    "                break\n",
    "            if line:\n",
    "                print(line.strip())\n",
    "    \n",
    "    # Start output thread\n",
    "    output_thread = threading.Thread(target=print_output)\n",
    "    output_thread.daemon = True\n",
    "    output_thread.start()\n",
    "    \n",
    "    return process\n",
    "\n",
    "# Save your server code to a temporary file\n",
    "import tempfile\n",
    "with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as f:\n",
    "    server_file = f.name\n",
    "    # Write your server code to the file\n",
    "    f.write(\"\"\"\n",
    "# Your MCP server code here\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"ObsidianNotes\")\n",
    "\n",
    "# Add your tools, resources, and prompts here\n",
    "@mcp.tool()\n",
    "def test_tool(message: str) -> str:\n",
    "    return f\"Test response: {message}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()\n",
    "\"\"\".encode())\n",
    "\n",
    "# Run the MCP server\n",
    "server_process = run_mcp_server(server_file)\n",
    "\n",
    "print(f\"MCP server started. Open http://localhost:8000 in your browser.\")\n",
    "print(\"Press Ctrl+C to stop the server when done.\")\n",
    "\n",
    "# Keep the server running until interrupted\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    server_process.terminate()\n",
    "    print(\"Server stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db62dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (venv)",
   "language": "python",
   "name": "venv-3.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
