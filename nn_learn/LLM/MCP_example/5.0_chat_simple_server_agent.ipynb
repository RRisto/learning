{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b96f309e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import BaseTool\n",
    "from typing import Optional\n",
    "from langchain_core.callbacks import CallbackManagerForToolRun, AsyncCallbackManagerForToolRun\n",
    "\n",
    "class MyCustomTool(BaseTool):\n",
    "    name: str = \"my_custom_tool\"\n",
    "    description: str = \"A custom tool that performs a specific task.\"\n",
    "\n",
    "    def _run(\n",
    "        self,\n",
    "        input_text: str,\n",
    "        run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        # Implement your tool's functionality here\n",
    "        return f\"Processed input: {input_text}\"\n",
    "\n",
    "    async def _arun(\n",
    "        self,\n",
    "        input_text: str,\n",
    "        run_manager: Optional[AsyncCallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        # Asynchronous implementation (optional)\n",
    "        return self._run(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "482d5ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Tool call: get_notes_for_summary({'query': 'complexity', 'max_notes': 5})\n",
      "✅ Tool result: [TextContent(type='text', text='Notes about \\'complexity\\' (showing 3449 results):\\n\\nNote ID: c139a9396e82239954f6b259cefb979c\\nTitle: Disorganized complexity\\nFolder: complex_systems\\nSimilarity: 0.5355\\n\\nIn Warren Weaver\\'s view, disorganized complexity results from the particular system having a very large number of parts, say millions of parts, or many more. Though the interactions of the parts in a \"disorganized complexity\" situation can be seen as largely random, the properties of the system as a whole can be understood by using probability and statistical methods. Individual fluctuations cancel out and system can be predictably described.\\n\\n\"Complex Adaptive Systems: An Introduction to Computational Models of Social Life\", John H. Miller,\\xa0Scott E. Page, page 47\\n\\n[[Complex system is greater than sum of its parts]]\\n\\n#complex_system \\n#complexity\\n#society \\n---\\nNote ID: 7c997db5dc86dc68b5892f1f3df50d38\\nTitle: Complexity is outcome of uncertainty minimization\\nFolder: Ecology\\nSimilarity: 0.5158\\n\\nKrakauer, Flack etc. research has shown that complexity and multiscale structures of biological systems is predictable outcome of evolutionary dynamics driven by uncertainty minimzation.\\n\\n\"Worlds Hidden in Plain Sight: The Evolving Idea of Complexity at the Santa Fe Institute, 1984–2019\", David C. Krakauer, page 161\\n\\n[[Organizing search spaces makes search easier]]\\n\\n#complexity \\n#uncertainty\\n#evolution\\n\\n\\n---\\nNote ID: 9343f36fda0e591f9daf865dfd65a9a4\\nTitle: Hierarchies are needed for efficient information processing\\nFolder: complex_systems\\nSimilarity: 0.4996\\n\\nComplex systems can evolve from simple systems only if there are stable intermediate forms. The resulting complex forms will naturally be hierarchic. \\n\\nHierarchies are brilliant systems inventions, not only because they give a system stability and resilience, but also because they reduce the amount of information that any part of the system has to keep track of.\\n\\n\"Thinking in Systems\", Donella H. Meadows, page 73\\n\\n[[Complex system is greater than sum of its parts]]\\n[[Complex systems wide view might need less information than detailed view]]\\n[[Complexity is outcome of uncertainty minimization]]\\n\\n#hierarchy\\n#information_processing\\n#resilience\\n#stability\\n\\n---\\nNote ID: bfb8bbd423da8fe1b9638345bcb4cd15\\nTitle: Random rewiring of the regular network will create a small world network\\nFolder: complex_systems\\nSimilarity: 0.4915\\n\\n\"Watts and Strogatz found that as the number of nodes increases, the effect becomes increasingly pronounced. For example, in a regular network with 1,000 nodes, the average path length is 250; in the same network with 5% of the links randomly rewired, the average path length will typically fall to around 20.\"\\n![[Pasted image 20230625190606.png]]\\nSmall things might have big impacts\\n\\n[[Complex system is greater than sum of its parts]]\\n\"Complexity: A Guided Tour\", Melanie Mithcell, page 495\\n\\n#complex_system \\n#network\\n#randomness\\n\\n---\\nNote ID: f04018282980c1f7024b150bfa51f417\\nTitle: non-transition of fitness\\nFolder: Ecology\\nSimilarity: 0.4838\\n\\nIf genotype A is more fit than B (in experiment involving only those 2 genotypes), and B is more fit than C (n experiment involving only those 2 genotypes), but in three-way competition C might beat A.\\n\\n\"Worlds Hidden in Plain Sight: The Evolving Idea of Complexity at the Santa Fe Institute, 1984–2019\", David C. Krakauer, page 88\\n\\n[[Complex system is greater than sum of its parts]]\\n[[Communication alters interactions in social systems]]\\n\\n#complex_system \\n#fitness\\n#transition\\n\\n\\n---', annotations=None)]...\n",
      "\n",
      "🎯 FINAL RESPONSE:\n",
      "return_values={'output': 'Here are the summaries of your notes about \\'complexity\\':\\n\\n1. **Disorganized complexity**: This concept, as per Warren Weaver\\'s view, results from a system having a very large number of parts, say millions or more. The interactions of the parts in a \"disorganized complexity\" situation can be seen as largely random, but the properties of the system as a whole can be understood by using probability and statistical methods. Individual fluctuations cancel out and the system can be predictably described.\\n\\n2. **Complexity is the outcome of uncertainty minimization**: Research by Krakauer, Flack, and others has shown that the complexity and multiscale structures of biological systems are predictable outcomes of evolutionary dynamics driven by uncertainty minimization.\\n\\n3. **Hierarchies are needed for efficient information processing**: Complex systems can evolve from simple systems only if there are stable intermediate forms. The resulting complex forms will naturally be hierarchic. Hierarchies are brilliant systems inventions, not only because they give a system stability and resilience, but also because they reduce the amount of information that any part of the system has to keep track of.\\n\\n4. **Random rewiring of the regular network will create a small world network**: Watts and Strogatz found that as the number of nodes increases, the effect becomes increasingly pronounced. For example, in a regular network with 1,000 nodes, the average path length is 250; in the same network with 5% of the links randomly rewired, the average path length will typically fall to around 20.\\n\\n5. **Non-transition of fitness**: If genotype A is more fit than B (in an experiment involving only those 2 genotypes), and B is more fit than C (in an experiment involving only those 2 genotypes), but in a three-way competition, C might beat A.'} log='Here are the summaries of your notes about \\'complexity\\':\\n\\n1. **Disorganized complexity**: This concept, as per Warren Weaver\\'s view, results from a system having a very large number of parts, say millions or more. The interactions of the parts in a \"disorganized complexity\" situation can be seen as largely random, but the properties of the system as a whole can be understood by using probability and statistical methods. Individual fluctuations cancel out and the system can be predictably described.\\n\\n2. **Complexity is the outcome of uncertainty minimization**: Research by Krakauer, Flack, and others has shown that the complexity and multiscale structures of biological systems are predictable outcomes of evolutionary dynamics driven by uncertainty minimization.\\n\\n3. **Hierarchies are needed for efficient information processing**: Complex systems can evolve from simple systems only if there are stable intermediate forms. The resulting complex forms will naturally be hierarchic. Hierarchies are brilliant systems inventions, not only because they give a system stability and resilience, but also because they reduce the amount of information that any part of the system has to keep track of.\\n\\n4. **Random rewiring of the regular network will create a small world network**: Watts and Strogatz found that as the number of nodes increases, the effect becomes increasingly pronounced. For example, in a regular network with 1,000 nodes, the average path length is 250; in the same network with 5% of the links randomly rewired, the average path length will typically fall to around 20.\\n\\n5. **Non-transition of fitness**: If genotype A is more fit than B (in an experiment involving only those 2 genotypes), and B is more fit than C (in an experiment involving only those 2 genotypes), but in a three-way competition, C might beat A.'\n"
     ]
    }
   ],
   "source": [
    "from fastmcp import Client\n",
    "from litellm.experimental_mcp_client.tools import transform_mcp_tool_to_openai_tool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain.agents.output_parsers.tools import ToolAgentAction\n",
    "from langchain.agents.format_scratchpad.openai_tools import format_to_openai_tool_messages\n",
    "from langchain_core.agents import AgentAction\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "async def main(input_text='Summarize my notes about complexity'):\n",
    "    mcp_client = Client(\"http://localhost:8000/mcp/\")\n",
    "    \n",
    "    async with mcp_client:\n",
    "        # Fetch and transform tools\n",
    "        tools = await mcp_client.list_tools()\n",
    "        openai_tools = [transform_mcp_tool_to_openai_tool(t) for t in tools]\n",
    "\n",
    "        # Initialize LLM with tools\n",
    "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0).bind_tools(openai_tools)\n",
    "\n",
    "        # Define the prompt template\n",
    "        prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are a helpful assistant that uses tools to answer user questions.\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "            MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "        ])\n",
    "\n",
    "        # Define the agent using the prompt, LLM, and output parser\n",
    "        agent_runnable: Runnable = prompt | llm | OpenAIToolsAgentOutputParser()\n",
    "\n",
    "        scratchpad_steps: list[tuple[AgentAction, str]] = []\n",
    "\n",
    "        # First reasoning step\n",
    "        step = await agent_runnable.ainvoke({\n",
    "            \"input\": input_text,\n",
    "            \"agent_scratchpad\": format_to_openai_tool_messages(scratchpad_steps),\n",
    "        })\n",
    "\n",
    "        # Loop until final output\n",
    "        while isinstance(step[0], AgentAction) or isinstance(step[0], ToolAgentAction):\n",
    "            print(f\"\\n🔧 Tool call: {step[0].tool}({step[0].tool_input})\")\n",
    "\n",
    "            # Call the tool via MCP\n",
    "            tool_output = await mcp_client.call_tool(step[0].tool, step[0].tool_input)\n",
    "\n",
    "            print(f\"✅ Tool result: {tool_output[:300]}...\")\n",
    "\n",
    "            # Record the (action, result) pair\n",
    "            scratchpad_steps.append((step[0], tool_output))\n",
    "\n",
    "            # Continue reasoning\n",
    "            step = await agent_runnable.ainvoke({\n",
    "                \"input\": input_text,\n",
    "                \"agent_scratchpad\": format_to_openai_tool_messages(scratchpad_steps),\n",
    "            })\n",
    "            # Final message (AgentFinish)\n",
    "            print(\"\\n🎯 FINAL RESPONSE:\")\n",
    "            print(step)\n",
    "            return step\n",
    "\n",
    "\n",
    "response=await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "efa22a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the summaries of your notes about 'complexity':\n",
      "\n",
      "1. **Disorganized complexity**: This concept, as per Warren Weaver's view, results from a system having a very large number of parts, say millions or more. The interactions of the parts in a \"disorganized complexity\" situation can be seen as largely random, but the properties of the system as a whole can be understood by using probability and statistical methods. Individual fluctuations cancel out and the system can be predictably described.\n",
      "\n",
      "2. **Complexity is the outcome of uncertainty minimization**: Research by Krakauer, Flack, and others has shown that the complexity and multiscale structures of biological systems are predictable outcomes of evolutionary dynamics driven by uncertainty minimization.\n",
      "\n",
      "3. **Hierarchies are needed for efficient information processing**: Complex systems can evolve from simple systems only if there are stable intermediate forms. The resulting complex forms will naturally be hierarchic. Hierarchies are brilliant systems inventions, not only because they give a system stability and resilience, but also because they reduce the amount of information that any part of the system has to keep track of.\n",
      "\n",
      "4. **Random rewiring of the regular network will create a small world network**: Watts and Strogatz found that as the number of nodes increases, the effect becomes increasingly pronounced. For example, in a regular network with 1,000 nodes, the average path length is 250; in the same network with 5% of the links randomly rewired, the average path length will typically fall to around 20.\n",
      "\n",
      "5. **Non-transition of fitness**: If genotype A is more fit than B (in an experiment involving only those 2 genotypes), and B is more fit than C (in an experiment involving only those 2 genotypes), but in a three-way competition, C might beat A.\n"
     ]
    }
   ],
   "source": [
    "print(response.return_values['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "05c1747b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 Tool call: get_notes_for_summary({'query': 'AI', 'max_notes': 5})\n",
      "✅ Tool result: [TextContent(type='text', text='Notes about \\'AI\\' (showing 3957 results):\\n\\nNote ID: 9501bb55c2c54e944c804ad8ca179a4b\\nTitle: Humans surrender autonomy when we use autonomous machines\\nFolder: AI\\nSimilarity: 0.4437\\n\\n \"... the computer scientist Joseph Weizenbaum, the inventor of the very first AI mirror, the ELIZA chatbot of the mid-1960s. A decade later, Weizenbaum noted the paradox of humans surrendering their belief in their own autonomy at the very moment that they chose to rely on autonomous machines that they themselves made.\"\\nSource: Weizenbaum, Joseph.\\xa0_Computer Power and Human Reason_. and \"The AI Mirror: How to Reclaim Our Humanity in an Age of Machine Thinking\", Shannon Vallor, page 170\\n\\n[[We shape our tools and thereafter they shape us]]\\n[[We should avoid tyranny narrow visions]]\\n\\n#AI \\n#AI_hype \\n#ai_hiring \\n#autonomy\\n\\n---\\nNote ID: a0f7b458c2263ec75ac73f3e01c6ff4b\\nTitle: We are so used to mimic states of other minds we interact\\nFolder: AI\\nSimilarity: 0.4387\\n\\nThis is a reason why we so easily attribute human qualities to machines, chatbots. It is our genes to interact and read others minds (even if they are not real). Idea from Jason Mitchell from Harvard\\'s Social Cognition and Affective Laboratory. ^bb6e7a\\n\\n\"The Shallows: What the Internet Is Doing to Our Brains\", Nicholas Carr, page 213\\n\\n[[ELIZA had already problems human considering it as a conscious entity]]\\n\\n#human_interaction\\n#anthropomorphism \\n\\n\\n\\n---\\nNote ID: aa69ac89dc6df99c1190979c79594188\\nTitle: Eetika printsiibib peavad olema seotud sisemiste väärtustega\\nFolder: AI\\nSimilarity: 0.4324\\n\\nNäiteks läbipaistvuse põhimõte peab olema seotud sisemiste väärtustega (sooviga olla läbipaistev, eetiline), et sellest printsiibist oleks kasu. Printsiibid ilma sisemise väärtuseta, on väline motivatsioon, mille vastupidavus on nõrk. AI arendamisel on lihtne vormiliselt järgida kontrollnimekirja, kuid see ei taga, et me sisuliselt tahame eetilist AI-d arendada.\\n\\n\"Stronger Together: on the Articulation of Ethical Charters, Legal Tools, and Technical Documentation in ML\" lk 4:\\n\"Transparency would have to be connected to an intrinsic value, such as accountability, in order for it to make sense to regard it as having a positive value.\"\\nhttp://arxiv.org/abs/2305.18615\\n[[Veebist andmete kraapimine masinõppe mudelite jaoks võib minna eetika piiride isegi kui on seaduslik]]\\n#eetika\\n#AI\\n\\n---\\nNote ID: 585c1a57d10e93f11ead58f1656405ee\\nTitle: Can we create tools to augment our empathy\\nFolder: AI\\nSimilarity: 0.4301\\n\\n\\n“We know how to create tools to augment our intelligence, but can we create tools to augment our empathy? Our communities? Our sense of meaning and purpose?”\\n\\nSource: \"Against Reduction: Designing a Human Future with Machines\", Noelani Arista, Sasha Costanza-Chock,\\xa0Vafa Ghazav,\\xa0Suzanne Kite, page 22\\n\\n[[We shape our tools and thereafter they shape us]]\\n[[We should avoid tyranny narrow visions]]\\n\\n\\n#meaning  \\n#intelligence \\n#AI \\n#AI_hype \\n\\n---\\nNote ID: 08da900731c1e2e3cee6409945579396\\nTitle: When humans are not as useless as sometimes presumed and intelligent machines not as intelligent as typically assumed, we get so-so automation\\nFolder: technology\\nSimilarity: 0.4288\\n\\nThe economic problem from this business strategy is clear: when humans are not as useless as sometimes presumed and intelligent machines not as intelligent as typically assumed, we get so-so automation—all the displacement and little of the promised productivity gains. In fact, even the companies themselves do not benefit much from this automation, and some of the AI adoption may be because of hype, as the former AI scientist Alberto Romero, whom we quoted earlier, noted: “The marketing power of AI is such that many companies use it without knowing why. Everyone wanted to get on the AI bandwagon.”\\n\\n\\n\"Power and Progress: Our Thousand-Year Struggle Over Technology and Prosperity\", Daron Acemoğlu,\\xa0Simon Johnson, page 272\\n\\n[[Machine usefulness should be measure of how useful machines are]]\\n\\n#productivity \\n#AI \\n#AI_hype \\n#intelligence\\n#automation\\n\\n---', annotations=None)]...\n",
      "\n",
      "🎯 FINAL RESPONSE:\n",
      "return_values={'output': \"Here are the summaries of your notes about AI:\\n\\n1. **Humans surrender autonomy when we use autonomous machines**: This note discusses the paradox of humans surrendering their belief in their own autonomy at the very moment that they chose to rely on autonomous machines that they themselves made. The note references the work of computer scientist Joseph Weizenbaum, the inventor of the ELIZA chatbot.\\n\\n2. **We are so used to mimic states of other minds we interact**: This note suggests that our tendency to attribute human qualities to machines and chatbots is due to our innate ability to interact and read others' minds. This idea is attributed to Jason Mitchell from Harvard's Social Cognition and Affective Laboratory.\\n\\n3. **Eetika printsiibib peavad olema seotud sisemiste väärtustega**: This note (in Estonian) discusses the importance of connecting principles like transparency to intrinsic values (like the desire to be transparent and ethical) in AI development. Without intrinsic value, principles serve as external motivation, which is weak and doesn't guarantee ethical AI development.\\n\\n4. **Can we create tools to augment our empathy**: This note poses a question about whether we can create tools to augment our empathy, communities, and sense of meaning and purpose, in addition to augmenting our intelligence.\\n\\n5. **When humans are not as useless as sometimes presumed and intelligent machines not as intelligent as typically assumed, we get so-so automation**: This note discusses the economic problem of so-so automation, where humans are not as useless as sometimes presumed and intelligent machines are not as intelligent as typically assumed. This results in all the displacement and little of the promised productivity gains. The note suggests that some AI adoption may be due to hype.\"} log=\"Here are the summaries of your notes about AI:\\n\\n1. **Humans surrender autonomy when we use autonomous machines**: This note discusses the paradox of humans surrendering their belief in their own autonomy at the very moment that they chose to rely on autonomous machines that they themselves made. The note references the work of computer scientist Joseph Weizenbaum, the inventor of the ELIZA chatbot.\\n\\n2. **We are so used to mimic states of other minds we interact**: This note suggests that our tendency to attribute human qualities to machines and chatbots is due to our innate ability to interact and read others' minds. This idea is attributed to Jason Mitchell from Harvard's Social Cognition and Affective Laboratory.\\n\\n3. **Eetika printsiibib peavad olema seotud sisemiste väärtustega**: This note (in Estonian) discusses the importance of connecting principles like transparency to intrinsic values (like the desire to be transparent and ethical) in AI development. Without intrinsic value, principles serve as external motivation, which is weak and doesn't guarantee ethical AI development.\\n\\n4. **Can we create tools to augment our empathy**: This note poses a question about whether we can create tools to augment our empathy, communities, and sense of meaning and purpose, in addition to augmenting our intelligence.\\n\\n5. **When humans are not as useless as sometimes presumed and intelligent machines not as intelligent as typically assumed, we get so-so automation**: This note discusses the economic problem of so-so automation, where humans are not as useless as sometimes presumed and intelligent machines are not as intelligent as typically assumed. This results in all the displacement and little of the promised productivity gains. The note suggests that some AI adoption may be due to hype.\"\n",
      "Here are the summaries of your notes about AI:\n",
      "\n",
      "1. **Humans surrender autonomy when we use autonomous machines**: This note discusses the paradox of humans surrendering their belief in their own autonomy at the very moment that they chose to rely on autonomous machines that they themselves made. The note references the work of computer scientist Joseph Weizenbaum, the inventor of the ELIZA chatbot.\n",
      "\n",
      "2. **We are so used to mimic states of other minds we interact**: This note suggests that our tendency to attribute human qualities to machines and chatbots is due to our innate ability to interact and read others' minds. This idea is attributed to Jason Mitchell from Harvard's Social Cognition and Affective Laboratory.\n",
      "\n",
      "3. **Eetika printsiibib peavad olema seotud sisemiste väärtustega**: This note (in Estonian) discusses the importance of connecting principles like transparency to intrinsic values (like the desire to be transparent and ethical) in AI development. Without intrinsic value, principles serve as external motivation, which is weak and doesn't guarantee ethical AI development.\n",
      "\n",
      "4. **Can we create tools to augment our empathy**: This note poses a question about whether we can create tools to augment our empathy, communities, and sense of meaning and purpose, in addition to augmenting our intelligence.\n",
      "\n",
      "5. **When humans are not as useless as sometimes presumed and intelligent machines not as intelligent as typically assumed, we get so-so automation**: This note discusses the economic problem of so-so automation, where humans are not as useless as sometimes presumed and intelligent machines are not as intelligent as typically assumed. This results in all the displacement and little of the promised productivity gains. The note suggests that some AI adoption may be due to hype.\n"
     ]
    }
   ],
   "source": [
    "response=await main('Summarize my notes about AI')\n",
    "print(response.return_values['output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a019c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (venv)",
   "language": "python",
   "name": "venv-3.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
