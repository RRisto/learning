{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9338839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import chromadb\n",
    "import openai\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd9d474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26d15e",
   "metadata": {},
   "source": [
    "## Setup vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4201bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "# For persistence to disk\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f949e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                model_name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da2bc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.01191489,  0.00940918,  0.00219249, ...,  0.02120901,\n",
       "         0.02228288, -0.04032141], shape=(1536,), dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_ef('tere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a31698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection with the custom embedding function\n",
    "# collection = client.create_collection(\n",
    "#     name=\"obsidian_notes\",\n",
    "#     embedding_function=openai_ef\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8a2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load if this has been setup\n",
    "collection = client.get_collection(\n",
    "    name=\"obsidian_notes\",\n",
    "    embedding_function=openai_ef  # Your embedding function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55fc15d",
   "metadata": {},
   "source": [
    "## Upload documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cbd5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_info(file_path):\n",
    "    \"\"\"Get file metadata needed for change detection\"\"\"\n",
    "    stats = os.stat(file_path)\n",
    "    return {\n",
    "        \"modified_time\": stats.st_mtime,\n",
    "        \"size\": stats.st_size,\n",
    "        \"hash\": calculate_file_hash(file_path)\n",
    "    }\n",
    "\n",
    "def calculate_file_hash(file_path):\n",
    "    \"\"\"Calculate MD5 hash of file contents\"\"\"\n",
    "    hash_md5 = hashlib.md5()\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "            hash_md5.update(chunk)\n",
    "    return hash_md5.hexdigest()\n",
    "\n",
    "def save_file_metadata(file_path):\n",
    "    \"\"\"Save metadata for a single file\"\"\"\n",
    "    file_info = get_file_info(file_path)\n",
    "    \n",
    "    # Create a unique filename for the metadata\n",
    "    metadata_filename = hashlib.md5(file_path.encode()).hexdigest() + \".json\"\n",
    "    metadata_path = os.path.join(METADATA_DIR, metadata_filename)\n",
    "    \n",
    "    # Save the metadata\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump({\n",
    "            \"file_path\": file_path,\n",
    "            \"info\": file_info,\n",
    "            \"last_processed\": time.time()\n",
    "        }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16e1cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tags_from_note(note_content):\n",
    "    \"\"\"Extract tags from an Obsidian note (inline hashtags)\"\"\"    \n",
    "    # This regex finds hashtags but ignores URLs and code blocks\n",
    "    inline_tags = re.findall(r'(?<!`|\\w)#([a-zA-Z0-9_/-]+)', note_content)\n",
    "\n",
    "    return list(inline_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1c0af61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_note(file_path, collection):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    if len(content)>5:\n",
    "        # Generate a unique ID based on file path\n",
    "        note_id = hashlib.md5(file_path.encode()).hexdigest()\n",
    "\n",
    "        # Extract title from filename\n",
    "        title = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        folder= file_path.replace('\\\\', '/').split('/')[1]\n",
    "        tags = extract_tags_from_note(content)\n",
    "\n",
    "        # Add to collection\n",
    "        collection.add(\n",
    "            ids=[note_id],\n",
    "            documents=[content],\n",
    "            metadatas=[{\"title\": title,\"folder\":folder, \"path\": file_path,\n",
    "                        \"last_updated\": time.time(),\n",
    "                        \"tags\": \",\".join(tags)}]\n",
    "        )\n",
    "\n",
    "        # Save individual file metadata\n",
    "        save_file_metadata(file_path)\n",
    "    else:\n",
    "        print(f\"file {file_path} has length of smaller than 5, skipping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6eb3707e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory to store metadata about processed files\n",
    "METADATA_DIR = \"./note_metadata\"\n",
    "os.makedirs(METADATA_DIR, exist_ok=True)\n",
    "\n",
    "def process_notes_initially(vault_path, collection):\n",
    "    # Track all processed files\n",
    "    processed_files = {}\n",
    "    \n",
    "    for root, _, files in os.walk(vault_path):\n",
    "        for i, file in enumerate(files):\n",
    "            if i%10==0:\n",
    "                print(f\"working on file {i}\")\n",
    "            if file.endswith('.md'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                try:\n",
    "                    # Process and add the note\n",
    "                    process_single_note(file_path, collection)\n",
    "                    \n",
    "                    # Record this file as processed\n",
    "                    file_info = get_file_info(file_path)\n",
    "                    processed_files[file_path] = file_info\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    # Save metadata about all processed files\n",
    "    with open(os.path.join(METADATA_DIR, \"processed_files.json\"), \"w\") as f:\n",
    "        json.dump(processed_files, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a1748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 30\n",
      "working on file 40\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 0\n",
      "working on file 10\n",
      "working on file 20\n",
      "working on file 0\n",
      "working on file 0\n"
     ]
    }
   ],
   "source": [
    "process_notes_initially('Obsidian Vault/', collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7521e905",
   "metadata": {},
   "source": [
    "### detect changes to change only this part in db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4328bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_note_in_collection(file_path, collection):\n",
    "    \"\"\"Update an existing note in the collection\"\"\"\n",
    "    # Generate the consistent ID for this file\n",
    "    note_id = hashlib.md5(file_path.encode()).hexdigest()\n",
    "    \n",
    "    # Read the updated content\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    \n",
    "    # Extract title from filename\n",
    "    title = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    \n",
    "    # Update the note in collection\n",
    "    # First, check if it exists\n",
    "    results = collection.get(ids=[note_id])\n",
    "    \n",
    "    if len(results['ids']) > 0:\n",
    "        # Update existing entry\n",
    "        collection.update(\n",
    "            ids=[note_id],\n",
    "            documents=[content],\n",
    "            metadatas=[{\"title\": title, \"path\": file_path, \"last_updated\": time.time()}]\n",
    "        )\n",
    "    else:\n",
    "        # Add as new if not found (shouldn't happen normally)\n",
    "        collection.add(\n",
    "            ids=[note_id],\n",
    "            documents=[content],\n",
    "            metadatas=[{\"title\": title, \"path\": file_path, \"last_updated\": time.time()}]\n",
    "        )\n",
    "    \n",
    "    # Update file metadata\n",
    "    save_file_metadata(file_path)\n",
    "\n",
    "\n",
    "def remove_note_from_collection(file_path, collection):\n",
    "    \"\"\"Remove a note from the collection\"\"\"\n",
    "    # Generate the consistent ID for this file\n",
    "    note_id = hashlib.md5(file_path.encode()).hexdigest()\n",
    "    \n",
    "    # Remove from collection\n",
    "    collection.delete(ids=[note_id])\n",
    "    \n",
    "    # Remove metadata file\n",
    "    metadata_filename = hashlib.md5(file_path.encode()).hexdigest() + \".json\"\n",
    "    metadata_path = os.path.join(metadata_dir, metadata_filename)\n",
    "    \n",
    "    if os.path.exists(metadata_path):\n",
    "        os.remove(metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d1c2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_changed_notes(vault_path, collection):\n",
    "    # Load previously processed files\n",
    "    try:\n",
    "        with open(os.path.join(metadata_dir, \"processed_files.json\"), \"r\") as f:\n",
    "            processed_files = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        processed_files = {}\n",
    "    \n",
    "    # Track current files\n",
    "    current_files = set()\n",
    "    \n",
    "    # Check all files in the vault\n",
    "    for root, _, files in os.walk(vault_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.md'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                current_files.add(file_path)\n",
    "                \n",
    "                # Get current file info\n",
    "                current_info = get_file_info(file_path)\n",
    "                \n",
    "                if file_path in processed_files:\n",
    "                    # File exists in our records, check if modified\n",
    "                    old_info = processed_files[file_path]\n",
    "                    \n",
    "                    if (current_info[\"modified_time\"] != old_info[\"modified_time\"] or\n",
    "                        current_info[\"size\"] != old_info[\"size\"] or\n",
    "                        current_info[\"hash\"] != old_info[\"hash\"]):\n",
    "                        \n",
    "                        print(f\"File changed: {file_path}\")\n",
    "                        \n",
    "                        # Update in collection\n",
    "                        update_note_in_collection(file_path, collection)\n",
    "                        \n",
    "                        # Update metadata\n",
    "                        processed_files[file_path] = current_info\n",
    "                else:\n",
    "                    # New file\n",
    "                    print(f\"New file: {file_path}\")\n",
    "                    process_single_note(file_path, collection)\n",
    "                    processed_files[file_path] = current_info\n",
    "    \n",
    "    # Check for deleted files\n",
    "    deleted_files = set(processed_files.keys()) - current_files\n",
    "    for file_path in deleted_files:\n",
    "        print(f\"File deleted: {file_path}\")\n",
    "        remove_note_from_collection(file_path, collection)\n",
    "        del processed_files[file_path]\n",
    "    \n",
    "    # Save updated metadata\n",
    "    with open(os.path.join(metadata_dir, \"processed_files.json\"), \"w\") as f:\n",
    "        json.dump(processed_files, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (venv)",
   "language": "python",
   "name": "venv-3.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
