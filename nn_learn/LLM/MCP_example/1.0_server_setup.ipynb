{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9338839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import chromadb\n",
    "import openai\n",
    "import hashlib\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abd9d474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e26d15e",
   "metadata": {},
   "source": [
    "## Setup vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4201bcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.Client()\n",
    "# For persistence to disk\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98f949e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "                api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "                model_name=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da2bc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.01191489,  0.00940918,  0.00219249, ...,  0.02120901,\n",
       "         0.02228288, -0.04032141], shape=(1536,), dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openai_ef('tere')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a31698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a collection with the custom embedding function\n",
    "# collection = client.create_collection(\n",
    "#     name=\"obsidian_notes\",\n",
    "#     embedding_function=openai_ef\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8a2efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load if this has been setup\n",
    "collection = client.get_collection(\n",
    "    name=\"obsidian_notes\",\n",
    "    embedding_function=openai_ef  # Your embedding function\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae82816",
   "metadata": {},
   "source": [
    "## MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf6bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "import datetime\n",
    "from contextlib import asynccontextmanager\n",
    "from collections.abc import AsyncIterator\n",
    "from dataclasses import dataclass\n",
    "from mcp.server.fastmcp import FastMCP, Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79661968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataclass to hold our dependencies\n",
    "@dataclass\n",
    "class AppContext:\n",
    "    collection: chromadb.Collection\n",
    "\n",
    "@asynccontextmanager\n",
    "async def app_lifespan(server: FastMCP) -> AsyncIterator[AppContext]:\n",
    "    \"\"\"Set up and tear down the Chroma DB connection\"\"\"\n",
    "    # Initialize Chroma client\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    \n",
    "    # Get the collection\n",
    "    collection = client.get_collection(\"obsidian_notes\", embedding_function=openai_ef)\n",
    "    \n",
    "    try:\n",
    "        yield AppContext(collection=collection)\n",
    "    finally:\n",
    "        # Any cleanup if needed\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "034be191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the MCP server with our lifespan\n",
    "mcp = FastMCP(\"ObsidianNotes\", lifespan=app_lifespan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca5cb14",
   "metadata": {},
   "source": [
    "### retriever tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66dd1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.tool()\n",
    "def get_sample_note() -> str:\n",
    "    \"\"\"Get a single sample note to examine its structure\"\"\"\n",
    "    # Get the collection\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    collection = client.get_collection(\"obsidian_notes\", embedding_function=openai_ef)\n",
    "    \n",
    "    # Get just one document\n",
    "    results = collection.get(limit=1)\n",
    "    \n",
    "    if results[\"ids\"] and len(results[\"ids\"]) > 0:\n",
    "        sample_id = results[\"ids\"][0]\n",
    "        sample_doc = results[\"documents\"][0]\n",
    "        sample_metadata = results[\"metadatas\"][0] if \"metadatas\" in results and results[\"metadatas\"] else {}\n",
    "        \n",
    "        # Format the output to show structure\n",
    "        output = \"Sample Note Structure:\\n\\n\"\n",
    "        output += f\"ID: {sample_id}\\n\\n\"\n",
    "        \n",
    "        output += \"Metadata Fields:\\n\"\n",
    "        for key, value in sample_metadata.items():\n",
    "            output += f\"- {key}: {type(value).__name__} = {value}\\n\"\n",
    "        \n",
    "        output += \"\\nDocument Content (first 200 chars):\\n\"\n",
    "        output += sample_doc\n",
    "        \n",
    "        # If embeddings exist, show their shape\n",
    "        if \"embeddings\" in results and results[\"embeddings\"]:\n",
    "            embedding = results[\"embeddings\"][0]\n",
    "            output += f\"\\n\\nEmbedding: Vector of length {len(embedding)}\"\n",
    "        \n",
    "        return output\n",
    "    else:\n",
    "        return \"No documents found in the collection.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a4862be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_search_results(vector_results):\n",
    "    formatted_results = []\n",
    "    for i, doc in enumerate(vector_results[\"documents\"][0]):\n",
    "        doc_id = vector_results[\"ids\"][0][i]\n",
    "        folder = vector_results[\"metadatas\"][0][i]['folder']\n",
    "        title = vector_results[\"metadatas\"][0][i]['title']\n",
    "        # Convert distance to similarity score (closer to 1 is better)\n",
    "        similarity = 1.0 / (1.0 + vector_results[\"distances\"][0][i]) if \"distances\" in vector_results else \"N/A\"\n",
    "\n",
    "        # Add formatted result\n",
    "        formatted_results.append(\n",
    "            f\"Note ID: {doc_id}\\n\"\n",
    "            f\"Title: {title}\\n\"\n",
    "            f\"Folder: {folder}\\n\"\n",
    "            f\"Similarity: {similarity:.4f}\\n\\n\"\n",
    "            f\"{doc}\\n\"\n",
    "            f\"---\")\n",
    "    return \"\\n\".join(formatted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac38029d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if need to remove tool\n",
    "# if \"vector_search\" in mcp._tool_manager._tools:\n",
    "#     del mcp._tool_manager._tools[\"vector_search\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6068317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.tool()\n",
    "def vector_search(query: str, n_results: int = 5, folder: str = None, ) -> str:\n",
    "    \"\"\"\n",
    "    Search notes using both vector similarity\n",
    "    \n",
    "    Args:\n",
    "        query: The search query\n",
    "        n_results: Number of results to return\n",
    "    \"\"\"\n",
    "    # Get the collection\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    collection = client.get_collection(\"obsidian_notes\", embedding_function=openai_ef)\n",
    "        \n",
    "    where_clause = {}\n",
    "    if folder:\n",
    "        where_clause[\"folder\"] = folder\n",
    "    \n",
    "    # 1. Vector search\n",
    "    vector_results = collection.query(\n",
    "        query_embeddings = openai_ef(query),\n",
    "        where=where_clause if where_clause else None,\n",
    "        n_results=n_results * 2  # Get more results to rerank\n",
    "    )\n",
    "    \n",
    "    #Format the results\n",
    "    if vector_results[\"documents\"] and len(vector_results[\"documents\"][0]) > 0:\n",
    "        formatted_results = format_search_results(vector_results)\n",
    "        return formatted_results\n",
    "    else:\n",
    "        return f\"No notes found for query: '{query}'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80b50b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/21/25 15:01:33] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/embeddings</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 </span> <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/21/25 15:01:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/embeddings\u001b[0m \u001b[32m\"HTTP/1.1 200 \u001b[0m \u001b]8;id=565025;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=18154;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# res=vector_search('Transparency', folder='Learning')\n",
    "res=vector_search('Transparency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34606271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note ID: 2266b46d92e2cad2bbd1cc30daf58540\n",
      "Title: Radical candor framework\n",
      "Folder: startup\n",
      "Similarity: 0.4454\n",
      "\n",
      "![[Pasted image 20231115205458.png]]\n",
      "\n",
      "Radical candor is a quadrant where you can be honest (give negative feedback) but at the same time be respectful. Being aggressive or false positive (giving empathy too much or being silent) is not going to help.\n",
      "\n",
      "\"Radical Candor: Be a Kickass Boss Without Losing Your Humanity, Kim Malone Scott, page 38\n",
      "\n",
      "[[Bill Campbell coaching style and ideas]]\n",
      "[[Google excellent team factors]]\n",
      "[[Intel operating style]]\n",
      "\n",
      "#psychological_safety \n",
      "#management \n",
      "#leadership \n",
      "#google \n",
      "\n",
      "---\n",
      "Note ID: 40329e717d71fe67393cefead7af1d4b\n",
      "Title: Algorithm governance questions\n",
      "Folder: AI\n",
      "Similarity: 0.4430\n",
      "\n",
      "I believe that the experience of kidney allocation may be able to shed some light:\n",
      "1. Participation by stakeholders\n",
      "2. Transparency measures\n",
      "3. Forecasting of system impacts\n",
      "4. Auditing of what actually happens once the system is turned on\n",
      "\n",
      "\"Voices in the Code: A Story about People, Their Values, and the Algorithm They Made\", David G. Robinson, page 37\n",
      "\n",
      "\n",
      "[[Automated hiring software is mistakenly rejecting millions of viable job candidates]]\n",
      "\n",
      "#AI \n",
      "#algorithms\n",
      "#governance\n",
      "#bias \n",
      "#fairness\n",
      "#openness \n",
      "---\n",
      "Note ID: 4ddc912de615f07ed2defc2c932e0bbd\n",
      "Title: Patron-client relationship is not welcomed in politics\n",
      "Folder: Learning\n",
      "Similarity: 0.4427\n",
      "\n",
      "When voters feel that patron-client relationship (big donations from private companies to some politician) is forming they feel that politicians are owing something to their patron and thus politician doesn't make good job.\n",
      "\n",
      "\"Worlds Hidden in Plain Sight: The Evolving Idea of Complexity at the Santa Fe Institute, 1984–2019\", David C. Krakauer, page 245\n",
      "\n",
      "[[Economic relationships are power relationships]]\n",
      "\n",
      "#patron_client\n",
      "#relationships \n",
      "#politics\n",
      "#lobby\n",
      "\n",
      "---\n",
      "Note ID: b18e35ead734361b7a9179c6dc0656fb\n",
      "Title: Deliberate polling, deliberate inclusion\n",
      "Folder: Society\n",
      "Similarity: 0.4401\n",
      "\n",
      "Fishkin and a colleague say that deliberation ideally aims to be:\n",
      "• Informed (and thus informative): Arguments should be supported by\n",
      "appropriate and reasonably accurate factual claims.\n",
      "• Balanced: Arguments should be met by contrary arguments.\n",
      "• Conscientious: The participants should be willing to talk and listen, with\n",
      "civility and respect.\n",
      "• Substantive: Arguments should be considered sincerely on their merits,\n",
      "not on how they are made or who is making them.\n",
      "• Comprehensive: All points of view held by significant portions of the population\n",
      "should receive attention.\n",
      "\n",
      "Fishkin, James S., and Robert C. Luskin. 2005. “Experimenting with a Democratic\n",
      "Ideal: Deliberative Polling and Public Opinion.” Acta Politica 40(3): 284–98. DOI:\n",
      "https://doi.org/10.1057/palgrave.ap.5500121.\n",
      "\n",
      "\"Voices in the Code: A Story about People, Their Values, and the Algorithm They Made\", David G. Robinson, page 38\n",
      "\n",
      "[[Why liberalism works]]\n",
      "\n",
      "\n",
      "#inclusion\n",
      "#democracy \n",
      "#critical_thinking \n",
      "#cooperation \n",
      "#governance \n",
      "---\n",
      "Note ID: 528778fd860e04ad3bde8b868ef444e8\n",
      "Title: People trust who understands them\n",
      "Folder: Marketing\n",
      "Similarity: 0.4352\n",
      "\n",
      "When we empathize with our customers’ dilemma, we create a bond of trust. People trust those who understand them, and they trust brands that understand them too.\n",
      "\n",
      "\"Building a StoryBrand: Clarify Your Message So Customers Will Listen\", Donald Miller, page 62\n",
      "\n",
      "[[Our success is coevolution]]\n",
      "\n",
      "\n",
      "#storybrand\n",
      "#relationships \n",
      "#trust\n",
      "\n",
      "---\n",
      "Note ID: b96a0389c3cac7dc4d28c23c050a0135\n",
      "Title: We should avoid tyranny narrow visions\n",
      "Folder: technology\n",
      "Similarity: 0.4340\n",
      "\n",
      "We should avoid that somebody who has a lot of power can set visions. They should include many views and feedbacks to vision and take in consideration impacts to the ones who have less power. Diversity and democracy is for the help.\n",
      "\n",
      " \"Power and Progress: Our Thousand-Year Struggle Over Technology and Prosperity\", Daron Acemoğlu, Simon Johnson, page 87\n",
      "[[Tech leaders are vision oligarchy]]\n",
      "[[Shared visions can trap us in using technology which might not be best]]\n",
      "\n",
      "\n",
      "#democracy\n",
      "#vision \n",
      "#vision_blindness \n",
      "#diversity\n",
      "\n",
      "---\n",
      "Note ID: f43b7a66c8f76ba6b187e1411f31e8a3\n",
      "Title: Continuous performance management and compensation\n",
      "Folder: startup\n",
      "Similarity: 0.4280\n",
      "\n",
      "![[Pasted image 20231028164756.png]]\n",
      "\n",
      "As companies transition to continuous performance management, OKRs and CFRs become mostly independent from compensation and formal evaluations.\n",
      "\n",
      "\"Measure What Matters: How Google, Bono, and the Gates Foundation Rock the World with OKRs\", John Doerr, page 147\n",
      "[[Two types of OKRs]]\n",
      "[[Andy Grove's OKR hygiene]]\n",
      "[[OKRs should be public]]\n",
      "\n",
      "\n",
      "#okr \n",
      "#feedback \n",
      "#performance \n",
      "#leadership \n",
      "#management \n",
      "\n",
      "---\n",
      "Note ID: 2e86067ee3480bd6f49a0e6a0033a615\n",
      "Title: bullshit is not a lie\n",
      "Folder: Society\n",
      "Similarity: 0.4266\n",
      "\n",
      "when you lie you care what is truth and try to hide it, you have to keep track what is lie and what is truth. on bullshitting you don't care. they just want to talk and hear themselves talking\n",
      "\n",
      "\"The AI Mirror: How to Reclaim Our Humanity in an Age of Machine Thinking\", Shannon Vallor, page 105\n",
      "\n",
      "[[Wealth of information creates a poverty of attention and a need to allocate that attention efficiently]]\n",
      "\n",
      "[[Information overload might create pancake people]]\n",
      "\n",
      "#information \n",
      "#information_consumption \n",
      "#free_information \n",
      "#bullshit\n",
      "\n",
      "\n",
      "---\n",
      "Note ID: 0195e1428fcc541742126cd5bc552e0d\n",
      "Title: Lack of trust slows economy down\n",
      "Folder: Economy\n",
      "Similarity: 0.4265\n",
      "\n",
      "Dr. Peter Kollock, associate professor and vice chair of the department of sociology at the University of California at Los Angeles, conducted a study in the mid-1990s that demonstrated this. He set up a trading game where people traded goods in two different environments. In one environment (low uncertainty), participants knew the value of what they were trading, and in the other (high uncertainty), they did not. Kollock’s study achieved two fascinating results: (1) people have a greater tendency to form interpersonal commitments in a high uncertainty environment, and (2) they tend to forgo potentially more profitable exchanges with untested partners in favor of trades with partners who have demonstrated their trustworthiness in previous transactions. In short, economic activity in times of high uncertainty slows down and traders become risk-averse.\n",
      "\n",
      "\"The Emergence of Exchange Structures: An Experimental Study of Uncertainty, Commitment, and Trust\", Peter Kollock, American Journal of Sociology, [Vol. 100, No. 2 (Sep., 1994)](https://www.jstor.org/stable/i329123), pp. 313-345 (33 pages)\n",
      "\n",
      "Referred from \"How: Why How We Do Anything Means Everything\", Dov Seidman, page 163\n",
      "\n",
      "[[People trust who understands them]]\n",
      "\n",
      "\n",
      "\n",
      "#trust \n",
      "#economy \n",
      "#exchange\n",
      "#trade \n",
      "---\n",
      "Note ID: 14ea58714ac430ad5b40ebb0c062c860\n",
      "Title: Intel operating style\n",
      "Folder: startup\n",
      "Similarity: 0.4248\n",
      "\n",
      "![[Pasted image 20231028170348.png]]\n",
      "\n",
      "\n",
      "[[Andy Grove's OKR hygiene]]\n",
      "[[Two types of OKRs]]\n",
      "[[OKRs should be public]]\n",
      "\n",
      "\n",
      "#andy_grove \n",
      "#okr \n",
      "#management \n",
      "#leadership \n",
      "#performance \n",
      "---\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f44175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Note Structure:\n",
      "\n",
      "ID: 40329e717d71fe67393cefead7af1d4b\n",
      "\n",
      "Metadata Fields:\n",
      "- tags: str = AI,algorithms,governance,bias,fairness,openness\n",
      "- folder: str = AI\n",
      "- last_updated: float = 1747735517.90906\n",
      "- path: str = Obsidian Vault/AI\\Algorithm governance questions.md\n",
      "- title: str = Algorithm governance questions\n",
      "\n",
      "Document Content (first 200 chars):\n",
      "I believe that the experience of kidney allocation may be able to shed some light:\n",
      "1. Participation by stakeholders\n",
      "2. Transparency measures\n",
      "3. Forecasting of system impacts\n",
      "4. Auditing of what actually happens once the system is turned on\n",
      "\n",
      "\"Voices in the Code: A Story about People, Their Values, and the Algorithm They Made\", David G. Robinson, page 37\n",
      "\n",
      "\n",
      "[[Automated hiring software is mistakenly rejecting millions of viable job candidates]]\n",
      "\n",
      "#AI \n",
      "#algorithms\n",
      "#governance\n",
      "#bias \n",
      "#fairness\n",
      "#openness \n"
     ]
    }
   ],
   "source": [
    "print(get_sample_note())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7050e34",
   "metadata": {},
   "source": [
    "### Browse notes tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c67079d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.tool()\n",
    "def browse_notes(folder: str = None, tag: str = None, limit: int = 10, offset: int = 0) -> str:\n",
    "    \"\"\"\n",
    "    Browse notes by category or tag\n",
    "    \n",
    "    Args:\n",
    "        category: Optional category to filter by\n",
    "        tag: Optional tag to filter by (will match if tag string contains this value)\n",
    "        limit: Maximum number of notes to return\n",
    "        offset: Number of notes to skip (for pagination)\n",
    "    \"\"\"\n",
    "    # Get the collection\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    collection = client.get_collection(\"obsidian_notes\", embedding_function=openai_ef)\n",
    "    \n",
    "    # Build where clause for filtering\n",
    "    where_clause = {}\n",
    "    if folder:\n",
    "        where_clause[\"folder\"] = folder\n",
    "    \n",
    "    # For tag filtering, we'll need to handle it manually since we want substring matching\n",
    "    \n",
    "    # Get all notes that match the category filter (or all notes if no category filter)\n",
    "    results = collection.get(\n",
    "        where=where_clause if where_clause else None,\n",
    "        limit=1000  # Get a larger batch to filter manually\n",
    "    )\n",
    "    \n",
    "    # Filter by tag if specified\n",
    "    filtered_indices = []\n",
    "    if tag and results[\"metadatas\"]:\n",
    "        for i, metadata in enumerate(results[\"metadatas\"]):\n",
    "            # Check if tags field exists and contains the tag substring\n",
    "            if \"tags\" in metadata:\n",
    "                tags_str = metadata[\"tags\"]\n",
    "                if isinstance(tags_str, str) and tag.lower() in tags_str.lower():\n",
    "                    filtered_indices.append(i)\n",
    "    else:\n",
    "        # If no tag filter, use all results\n",
    "        filtered_indices = list(range(len(results[\"documents\"])))\n",
    "    \n",
    "    # Apply pagination\n",
    "    start_idx = min(offset, len(filtered_indices))\n",
    "    end_idx = min(start_idx + limit, len(filtered_indices))\n",
    "    page_indices = filtered_indices[start_idx:end_idx]\n",
    "    \n",
    "    # Format the results\n",
    "    if page_indices:\n",
    "        formatted_results = []\n",
    "        for idx in page_indices:\n",
    "            doc = results[\"documents\"][idx]\n",
    "            doc_id = results[\"ids\"][idx]\n",
    "            metadata = results[\"metadatas\"][idx]\n",
    "            \n",
    "            # Extract title and other metadata\n",
    "            title = metadata.get(\"title\", \"Untitled\")\n",
    "            tags = metadata.get(\"tags\", \"\")\n",
    "            category = metadata.get(\"category\", \"Uncategorized\")\n",
    "            \n",
    "            # Create a preview\n",
    "            preview = doc[:100] + \"...\" if len(doc) > 100 else doc\n",
    "            \n",
    "            # Format the result\n",
    "            formatted_results.append(\n",
    "                f\"## {title}\\n\"\n",
    "                f\"ID: {doc_id}\\n\"\n",
    "                f\"Category: {category}\\n\"\n",
    "                f\"Tags: {tags}\\n\\n\"\n",
    "                f\"Preview: {preview}\\n\"\n",
    "                f\"---\"\n",
    "            )\n",
    "        \n",
    "        # Add pagination info\n",
    "        pagination_info = f\"Showing results {start_idx+1}-{end_idx} of {len(filtered_indices)}. \"\n",
    "        if end_idx < len(filtered_indices):\n",
    "            pagination_info += f\"Use offset={end_idx} to see more.\"\n",
    "        \n",
    "        return pagination_info + \"\\n\\n\" + \"\\n\".join(formatted_results)\n",
    "    else:\n",
    "        return f\"No notes found with the specified filters.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61670c22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Showing results 1-10 of 12. Use offset=10 to see more.\\n\\n## Algorithm governance questions\\nID: 40329e717d71fe67393cefead7af1d4b\\nCategory: Uncategorized\\nTags: AI,algorithms,governance,bias,fairness,openness\\n\\nPreview: I believe that the experience of kidney allocation may be able to shed some light:\\n1. Participation ...\\n---\\n## Automated hiring software is mistakenly rejecting millions of viable job candidates\\nID: 2ea949c8aef6a2dc251c678b020c7851\\nCategory: Uncategorized\\nTags: AI_hype,AI,ai_hiring,bias,society\\n\\nPreview: \"The study’s authors identify a number of factors blocking people from employment, but say automated...\\n---\\n## C. Clarke’s third law of prediction\\nID: b4cbf197a6f5e47aee89afa711fffb74\\nCategory: Uncategorized\\nTags: prediction,technology\\n\\nPreview: Arthur\\xa0C. Clarke’s third law of prediction is, famously, “Any sufficiently advanced technology is in...\\n---\\n## Can we create tools to augment our empathy\\nID: 585c1a57d10e93f11ead58f1656405ee\\nCategory: Uncategorized\\nTags: meaning,intelligence,AI,AI_hype\\n\\nPreview: \\n“We know how to create tools to augment our intelligence, but can we create tools to augment our em...\\n---\\n## Eetika printsiibib peavad olema seotud sisemiste väärtustega\\nID: aa69ac89dc6df99c1190979c79594188\\nCategory: Uncategorized\\nTags: eetika,AI\\n\\nPreview: Näiteks läbipaistvuse põhimõte peab olema seotud sisemiste väärtustega (sooviga olla läbipaistev, ee...\\n---\\n## ELIZA had already problems human considering it as a conscious entity\\nID: d8f425b5f135ead69b82774429f5f3ff\\nCategory: Uncategorized\\nTags: anthropomorphism,eetika,conscious,rationality\\n\\nPreview: ELIZA was a chatbot in 1960s, very simple system, but already then users who knew how it worked, ten...\\n---\\n## Humans surrender autonomy when we use autonomous machines\\nID: 9501bb55c2c54e944c804ad8ca179a4b\\nCategory: Uncategorized\\nTags: AI,AI_hype,ai_hiring,autonomy\\n\\nPreview:  \"... the computer scientist Joseph Weizenbaum, the inventor of the very first AI mirror, the ELIZA ...\\n---\\n## Language might help to create anthropomorphism\\nID: 972911915334f87718126f6a829f48a9\\nCategory: Uncategorized\\nTags: anthropomorphism,language,relationship_wtih_nature\\n\\nPreview: In English humans are considered living entities. Use word \"it\" and you turn living organism into su...\\n---\\n## Veebist andmete kraapimine masinõppe mudelite jaoks võib minna eetika piiride isegi kui on seaduslik\\nID: 07835ef18d95b70d649ab1ddb62e589b\\nCategory: Uncategorized\\nTags: eetika,ai\\n\\nPreview: Veebist andmete kraapimine ML jaoks on mugav ja odav viis andmete kogumiseks, võib olla seaduslik, k...\\n---\\n## We are so used to mimic states of other minds we interact\\nID: a0f7b458c2263ec75ac73f3e01c6ff4b\\nCategory: Uncategorized\\nTags: human_interaction,anthropomorphism\\n\\nPreview: This is a reason why we so easily attribute human qualities to machines, chatbots. It is our genes t...\\n---'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "browse_notes('AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc7e32c",
   "metadata": {},
   "source": [
    "### get a single note tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bafa87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mcp.resource(\"note://{note_id}\")\n",
    "def get_note(note_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Get the full content of a specific note by ID\n",
    "    \n",
    "    Args:\n",
    "        note_id: The ID of the note to retrieve\n",
    "    \"\"\"\n",
    "    # Get the collection\n",
    "    client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "    collection = client.get_collection(\"obsidian_notes\", embedding_function=openai_ef)\n",
    "    \n",
    "    # Query for the specific note\n",
    "    result = collection.get(\n",
    "        ids=[note_id]\n",
    "    )\n",
    "    \n",
    "    # Check if note was found\n",
    "    if result[\"documents\"] and len(result[\"documents\"]) > 0:\n",
    "        # Get the note content and metadata\n",
    "        content = result[\"documents\"][0]\n",
    "        metadata = result[\"metadatas\"][0] if result[\"metadatas\"] else {}\n",
    "        \n",
    "        # Format with metadata if available\n",
    "        title = metadata.get(\"title\", \"Untitled\")\n",
    "        tags = metadata.get(\"tags\", \"\")\n",
    "        \n",
    "        return f\"# {title}\\n\\nTags: {tags}\\n\\n{content}\"\n",
    "    else:\n",
    "        return f\"Note with ID '{note_id}' not found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86f3383b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Algorithm governance questions\\n\\nTags: AI,algorithms,governance,bias,fairness,openness\\n\\nI believe that the experience of kidney allocation may be able to shed some light:\\n1. Participation by stakeholders\\n2. Transparency measures\\n3. Forecasting of system impacts\\n4. Auditing of what actually happens once the system is turned on\\n\\n\"Voices in the Code: A Story about People, Their Values, and the Algorithm They Made\", David G. Robinson, page 37\\n\\n\\n[[Automated hiring software is mistakenly rejecting millions of viable job candidates]]\\n\\n#AI \\n#algorithms\\n#governance\\n#bias \\n#fairness\\n#openness '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_note('40329e717d71fe67393cefead7af1d4b')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b012b",
   "metadata": {},
   "source": [
    "## Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "207c382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @mcp.prompt()\n",
    "# def explore_topic(topic: str) -> str:\n",
    "#     return f\"\"\"\n",
    "#     System: You are a helpful assistant that helps users explore topics (named \"folder\" in database) in their notes. \n",
    "#     Use the vector_search tool to find relevant notes and browse_notes to explore categories.\n",
    "#     When showing specific notes, use the note:// resource to get full content.\n",
    "    \n",
    "#     User: I want to learn more about {topic}\n",
    "    \n",
    "#     Assistant: I'd be happy to help you explore this topic. I'll search your notes to find relevant information.\n",
    "#     \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2016ff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp.server.fastmcp.prompts import base\n",
    "\n",
    "@mcp.prompt()\n",
    "def explore_topic(topic: str) -> list[base.Message]:\n",
    "    return [\n",
    "        # Include system instructions in the first user message instead\n",
    "        base.Message(\n",
    "            role=\"user\",\n",
    "            content=f\"I want to learn more about {topic}. Please act as a helpful assistant that helps me explore topics in my notes. \"\n",
    "                   f\"Use the vector_search tool to find relevant notes and browse_notes to explore categories (which are named as folder in database). \"\n",
    "                   f\"When showing specific notes, use the note:// resource to get full content.\"\n",
    "        ),\n",
    "        base.Message(\n",
    "            role=\"assistant\",\n",
    "            content=\"I'd be happy to help you explore this topic. I'll search your notes to find relevant information.\"\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9630e5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Message(role='user', content=TextContent(type='text', text='I want to learn more about AI. Please act as a helpful assistant that helps me explore topics in my notes. Use the vector_search tool to find relevant notes and browse_notes to explore categories (which are named as folder in database). When showing specific notes, use the note:// resource to get full content.', annotations=None)),\n",
       " Message(role='assistant', content=TextContent(type='text', text=\"I'd be happy to help you explore this topic. I'll search your notes to find relevant information.\", annotations=None))]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explore_topic('AI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5df005",
   "metadata": {},
   "source": [
    "## Run server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b50a4d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server running on default address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [30404]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:63903 - \"POST /mcp/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:63905 - \"POST /mcp/ HTTP/1.1\" 400 Bad Request\n",
      "INFO:     127.0.0.1:64020 - \"POST /mcp/ HTTP/1.1\" 200 OK\n",
      "INFO:     127.0.0.1:64024 - \"POST /mcp/ HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:      + Exception Group Traceback (most recent call last):\n",
      "  |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\starlette\\routing.py\", line 693, in lifespan\n",
      "  |     async with self.lifespan_context(app) as maybe_state:\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "  |   File \"C:\\Users\\RistoHinno\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py\", line 235, in __aexit__\n",
      "  |     await self.gen.athrow(value)\n",
      "  |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\", line 106, in run\n",
      "  |     async with anyio.create_task_group() as tg:\n",
      "  |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 772, in __aexit__\n",
      "  |     raise BaseExceptionGroup(\n",
      "  |         \"unhandled errors in a TaskGroup\", self._exceptions\n",
      "  |     ) from None\n",
      "  | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "  +-+---------------- 1 ----------------\n",
      "    | Exception Group Traceback (most recent call last):\n",
      "    |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\", line 235, in run_server\n",
      "    |     async with http_transport.connect() as streams:\n",
      "    |                ~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "    |   File \"C:\\Users\\RistoHinno\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py\", line 235, in __aexit__\n",
      "    |     await self.gen.athrow(value)\n",
      "    |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http.py\", line 843, in connect\n",
      "    |     async with anyio.create_task_group() as tg:\n",
      "    |                ~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "    |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 772, in __aexit__\n",
      "    |     raise BaseExceptionGroup(\n",
      "    |         \"unhandled errors in a TaskGroup\", self._exceptions\n",
      "    |     ) from None\n",
      "    | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "    +-+---------------- 1 ----------------\n",
      "      | Exception Group Traceback (most recent call last):\n",
      "      |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http.py\", line 916, in connect\n",
      "      |     yield read_stream, write_stream\n",
      "      |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\", line 238, in run_server\n",
      "      |     await self.app.run(\n",
      "      |     ...<4 lines>...\n",
      "      |     )\n",
      "      |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\lowlevel\\server.py\", line 495, in run\n",
      "      |     async with AsyncExitStack() as stack:\n",
      "      |                ~~~~~~~~~~~~~~^^\n",
      "      |   File \"C:\\Users\\RistoHinno\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py\", line 768, in __aexit__\n",
      "      |     raise exc\n",
      "      |   File \"C:\\Users\\RistoHinno\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\contextlib.py\", line 751, in __aexit__\n",
      "      |     cb_suppress = await cb(*exc_details)\n",
      "      |                   ^^^^^^^^^^^^^^^^^^^^^^\n",
      "      |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\shared\\session.py\", line 220, in __aexit__\n",
      "      |     return await self._task_group.__aexit__(exc_type, exc_val, exc_tb)\n",
      "      |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "      |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 772, in __aexit__\n",
      "      |     raise BaseExceptionGroup(\n",
      "      |         \"unhandled errors in a TaskGroup\", self._exceptions\n",
      "      |     ) from None\n",
      "      | ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n",
      "      +-+---------------- 1 ----------------\n",
      "        | Traceback (most recent call last):\n",
      "        |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\session.py\", line 147, in _receive_loop\n",
      "        |     await super()._receive_loop()\n",
      "        |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\shared\\session.py\", line 370, in _receive_loop\n",
      "        |     await self._received_request(responder)\n",
      "        |   File \"C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\session.py\", line 175, in _received_request\n",
      "        |     raise RuntimeError(\n",
      "        |         \"Received request before initialization was complete\"\n",
      "        |     )\n",
      "        | RuntimeError: Received request before initialization was complete\n",
      "        +------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "def run_server():\n",
    "    # Check the supported parameters for your version\n",
    "    mcp.run(transport=\"streamable-http\")\n",
    "#     mcp.run(transport=\"http\")\n",
    "\n",
    "# Start the server in a separate thread\n",
    "server_thread = threading.Thread(target=run_server)\n",
    "server_thread.daemon = True  # Allow the thread to exit when the notebook closes\n",
    "server_thread.start()\n",
    "\n",
    "print(\"Server running on default address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263f596",
   "metadata": {},
   "source": [
    "## Make request to server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "77cb7f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/21/25 15:01:34] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> StreamableHTTP session manager started                  <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">streamable_http_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#109\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">109</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/21/25 15:01:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m StreamableHTTP session manager started                  \u001b]8;id=223938;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\u001b\\\u001b[2mstreamable_http_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=926787;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#109\u001b\\\u001b[2m109\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Run the test function\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01masyncio\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m prompt_result = \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_prompt\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexplore_topic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtopic\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmachine learning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\asyncio\\runners.py:191\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(main, debug, loop_factory)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[32m    162\u001b[39m \n\u001b[32m    163\u001b[39m \u001b[33;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m \u001b[33;03m    asyncio.run(main())\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m events._get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# fail fast with short traceback\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    192\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    194\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Runner(debug=debug, loop_factory=loop_factory) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m runner.run(main)\n",
      "\u001b[31mRuntimeError\u001b[39m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "async def test_prompt(prompt_name, arguments):\n",
    "    # Connect to your local server\n",
    "    async with streamablehttp_client(\"http://localhost:8000/mcp\") as (read_stream, write_stream, _):\n",
    "        # Create a session\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            # Initialize the connection\n",
    "            await session.initialize()\n",
    "            \n",
    "            # Get the prompt\n",
    "            prompt_result = await session.get_prompt(prompt_name, arguments)\n",
    "            \n",
    "            # Print the prompt content\n",
    "            print(\"Prompt content:\")\n",
    "            for message in prompt_result.messages:\n",
    "                print(f\"{message.role}: {message.content.text}\")\n",
    "            \n",
    "            return prompt_result\n",
    "\n",
    "# Run the test function\n",
    "import asyncio\n",
    "prompt_result = asyncio.run(test_prompt(\"explore_topic\", {\"topic\": \"machine learning\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407030f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import json\n",
    "\n",
    "def test_prompt_cli(prompt_name, arguments):\n",
    "    # Convert arguments to JSON string\n",
    "    args_json = json.dumps(arguments)\n",
    "    \n",
    "    # Run the MCP CLI command\n",
    "    result = subprocess.run(\n",
    "        [\"mcp\", \"prompt\", \"get\", prompt_name, \"--arguments\", args_json],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    return result.stdout\n",
    "\n",
    "# Test the prompt\n",
    "output = test_prompt_cli(\"explore_topic\", {\"topic\": \"AI\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24325df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def test_prompt_direct(prompt_name, arguments):\n",
    "    # Endpoint for getting a prompt\n",
    "    url = \"http://localhost:8000/mcp/prompt\"\n",
    "    \n",
    "    # Prepare the request payload\n",
    "    payload = {\n",
    "        \"jsonrpc\": \"2.0\",\n",
    "        \"id\": 1,\n",
    "        \"method\": \"prompt/get\",\n",
    "        \"params\": {\n",
    "            \"name\": prompt_name,\n",
    "            \"arguments\": arguments\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Make the request\n",
    "    response = requests.post(url, json=payload)\n",
    "    \n",
    "    # Return the response\n",
    "    return response.json()\n",
    "\n",
    "# Test the prompt\n",
    "result = test_prompt_direct(\"explore_topic\", {\"topic\": \"AI\"})\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77381978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test a tool with proper headers\n",
    "def call_tool(tool_name, arguments):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json, text/event-stream\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/mcp/tool\",\n",
    "        headers=headers,\n",
    "        json={\n",
    "            \"jsonrpc\": \"2.0\",\n",
    "            \"id\": 1,\n",
    "            \"method\": \"tool/call\",\n",
    "            \"params\": {\n",
    "                \"name\": tool_name,\n",
    "                \"arguments\": arguments\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# Example: Call your vector_search tool\n",
    "result = call_tool(\"vector_search\", {\"query\": \"AI\", \"n_results\": 3})\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522f5ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "# Create a session ID\n",
    "session_id = str(uuid.uuid4())\n",
    "\n",
    "def call_mcp_api(method, params):\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json, text/event-stream\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-MCP-Session-ID\": session_id  # Include session ID in header\n",
    "    }\n",
    "    \n",
    "    # First initialize the session\n",
    "    if method != \"initialize\":\n",
    "        init_response = requests.post(\n",
    "            \"http://localhost:8000/mcp\",\n",
    "            headers=headers,\n",
    "            json={\n",
    "                \"jsonrpc\": \"2.0\",\n",
    "                \"id\": 0,\n",
    "                \"method\": \"initialize\",\n",
    "                \"params\": {\n",
    "                    \"client_name\": \"python-test\",\n",
    "                    \"client_version\": \"1.0.0\"\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        print(\"Initialization response:\", init_response.status_code)\n",
    "    \n",
    "    # Then make the actual request\n",
    "    response = requests.post(\n",
    "        \"http://localhost:8000/mcp\",\n",
    "        headers=headers,\n",
    "        json={\n",
    "            \"jsonrpc\": \"2.0\",\n",
    "            \"id\": 1,\n",
    "            \"method\": method,\n",
    "            \"params\": params\n",
    "        }\n",
    "    )\n",
    "    print(response)\n",
    "    return response.json()\n",
    "\n",
    "# Example: Call your vector_search tool\n",
    "result = call_mcp_api(\"tool/call\", {\n",
    "    \"name\": \"vector_search\", \n",
    "    \"arguments\": {\"query\": \"machine learning\", \"n_results\": 3}\n",
    "})\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d66dcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mcp import ClientSession\n",
    "from mcp.client.streamable_http import streamablehttp_client\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "\n",
    "\n",
    "async def test_mcp():\n",
    "    try:\n",
    "        # Connect to your server\n",
    "        async with streamablehttp_client(\"http://localhost:8000/mcp\") as (read_stream, write_stream, _):\n",
    "            # Create a session\n",
    "            async with ClientSession(read_stream, write_stream) as session:\n",
    "                # Initialize\n",
    "                await session.initialize()\n",
    "                \n",
    "                # List tools\n",
    "                tools = await session.list_tools()\n",
    "                print(\"Available tools:\", [t.name for t in tools])\n",
    "                \n",
    "                # Call a tool\n",
    "                result = await session.call_tool(\"vector_search\", {\"query\": \"machine learning\", \"n_results\": 3})\n",
    "                print(\"Result:\", result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Run the async function properly\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    await test_mcp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98bbb743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/21/25 15:01:57] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Created new transport with session ID:                  <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">streamable_http_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#229\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">229</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         2e76dc97e96e41f395b3d998c1ffc9b6                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/21/25 15:01:57]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Created new transport with session ID:                  \u001b]8;id=904862;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\u001b\\\u001b[2mstreamable_http_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=53175;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#229\u001b\\\u001b[2m229\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         2e76dc97e96e41f395b3d998c1ffc9b6                        \u001b[2m                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/21/25 15:01:59] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Created new transport with session ID:                  <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">streamable_http_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#229\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">229</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         aa1fb1e0e3174f8bb62d2bbdfc29259f                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/21/25 15:01:59]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Created new transport with session ID:                  \u001b]8;id=228548;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\u001b\\\u001b[2mstreamable_http_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=336914;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#229\u001b\\\u001b[2m229\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         aa1fb1e0e3174f8bb62d2bbdfc29259f                        \u001b[2m                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/21/25 15:03:20] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Created new transport with session ID:                  <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">streamable_http_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#229\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">229</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         a3bcb7d340454ced8cf8cf22ffdbcec6                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/21/25 15:03:20]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Created new transport with session ID:                  \u001b]8;id=996807;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\u001b\\\u001b[2mstreamable_http_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=993133;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#229\u001b\\\u001b[2m229\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         a3bcb7d340454ced8cf8cf22ffdbcec6                        \u001b[2m                              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[05/21/25 15:03:22] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> StreamableHTTP session manager shutting down            <a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">streamable_http_manager.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#113\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[05/21/25 15:03:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m StreamableHTTP session manager shutting down            \u001b]8;id=695982;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py\u001b\\\u001b[2mstreamable_http_manager.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=511814;file://C:\\Users\\RistoHinno\\python\\mcp\\.venv\\Lib\\site-packages\\mcp\\server\\streamable_http_manager.py#113\u001b\\\u001b[2m113\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import threading\n",
    "\n",
    "# Function to run the MCP server in a subprocess\n",
    "def run_mcp_server(server_file):\n",
    "    process = subprocess.Popen(\n",
    "        [\"mcp\", \"dev\", server_file],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True\n",
    "    )\n",
    "    \n",
    "    # Print output in real-time\n",
    "    def print_output():\n",
    "        while True:\n",
    "            line = process.stdout.readline()\n",
    "            if not line and process.poll() is not None:\n",
    "                break\n",
    "            if line:\n",
    "                print(line.strip())\n",
    "    \n",
    "    # Start output thread\n",
    "    output_thread = threading.Thread(target=print_output)\n",
    "    output_thread.daemon = True\n",
    "    output_thread.start()\n",
    "    \n",
    "    return process\n",
    "\n",
    "# Save your server code to a temporary file\n",
    "import tempfile\n",
    "with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as f:\n",
    "    server_file = f.name\n",
    "    # Write your server code to the file\n",
    "    f.write(\"\"\"\n",
    "# Your MCP server code here\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"ObsidianNotes\")\n",
    "\n",
    "# Add your tools, resources, and prompts here\n",
    "@mcp.tool()\n",
    "def test_tool(message: str) -> str:\n",
    "    return f\"Test response: {message}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()\n",
    "\"\"\".encode())\n",
    "\n",
    "# Run the MCP server\n",
    "server_process = run_mcp_server(server_file)\n",
    "\n",
    "print(f\"MCP server started. Open http://localhost:8000 in your browser.\")\n",
    "print(\"Press Ctrl+C to stop the server when done.\")\n",
    "\n",
    "# Keep the server running until interrupted\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    server_process.terminate()\n",
    "    print(\"Server stopped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db62dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.13 (venv)",
   "language": "python",
   "name": "venv-3.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
