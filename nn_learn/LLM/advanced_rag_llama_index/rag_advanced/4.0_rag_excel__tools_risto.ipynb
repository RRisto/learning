{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ca90316-1742-47f4-90dc-28234f07ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install weaviate-client\n",
    "# !pip install llama_index\n",
    "# !pip install openai\n",
    "# !pip install --upgrade pydantic==1.10.0 typing-extensions==4.5.0\n",
    "# !pip install fastcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f464652e-e611-4ea7-894c-5ff78094a5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores import WeaviateVectorStore\n",
    "from llama_index.schema import TextNode, NodeRelationship, RelatedNodeInfo\n",
    "from llama_index import VectorStoreIndex, StorageContext\n",
    "from llama_index import get_response_synthesizer\n",
    "from llama_index.indices.postprocessor import SimilarityPostprocessor, PrevNextNodePostprocessor\n",
    "from llama_index.retrievers import VectorIndexRetriever\n",
    "from llama_index.query_engine import RetrieverQueryEngine\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "import weaviate, os\n",
    "from weaviate import EmbeddedOptions\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from llama_index import ServiceContext\n",
    "from llama_index.llms import OpenAI as OpenAILLama\n",
    "from llama_index.embeddings import OpenAIEmbedding\n",
    "from llama_index import download_loader\n",
    "from configparser import ConfigParser\n",
    "import openai\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc637b6-857c-42fa-bc7e-2c3552efb62b",
   "metadata": {},
   "source": [
    "## OpenAI conifg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df25723-b7bb-4d49-a275-ea6f8be9097e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=ConfigParser()\n",
    "config.read('conf/conf.ini')\n",
    "os.environ[\"OPENAI_API_KEY\"] = config['openai']['apikey']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eac88f-38be-46d1-a517-67ae4af4a8b2",
   "metadata": {},
   "source": [
    "## Load docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa1c4064-50e9-4bac-97f4-2c71aa8409b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/pkg_resources/_vendor/jaraco/text/__init__.py:593: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.9/dist-packages/llama_index/download/llamahub_modules/requirements.txt' mode='r' encoding='UTF-8'>\n",
      "  for item in lines:\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "PandasExcelReader = download_loader(\"PandasExcelReader\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b176cd2e-72c5-41d9-b4d5-e449d0f54572",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PandasExcelReader(pandas_config={\"header\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eedfa7f0-6538-4c71-8271-091f226acf4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/openpyxl/compat/numbers.py:41: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  numpy.float,\n"
     ]
    }
   ],
   "source": [
    "documents = loader.load_data(file=Path('data/riigikogu/ems_subset.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c5359d5-00d7-4aa5-9080-9863b88551f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fbd363-b00a-4ce4-9388-c28d973f98be",
   "metadata": {},
   "source": [
    "## Vector store index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84769ed3-d53b-44b4-a878-e2255aeebfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /tmp/llama_index...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "#this could be added to vectorstoreindex\n",
    "embed_model=HuggingFaceEmbedding(model_name='intfloat/multilingual-e5-base')\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=embed_model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62668e76-3cf9-4b2d-ba36-98dad778dc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NB! as embedding not set from previous model defaultsto OpenAi embedding model!\n",
    "index = VectorStoreIndex.from_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4764d602-94f4-4099-8602-72635e1bfca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3fbd1f22-f0fd-444b-bbb5-a450c43482a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=query_engine.query('Kas makse tuleb tõsta?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50f482d1-d10c-42fc-a858-85f732341446",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Makse tuleb tõsta.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "80625872-a237-4f99-8177-469d06f74991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resp.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b53d280-bc13-462c-8945-2283052b2395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Makse tuleb tõsta selleks, et tagada riigi rahaline stabiilsus ja võimaldada riigil täita oma ülesandeid ja kohustusi. Maksumäärade tõstmine võib olla vajalik, et katta riigi kulutused erinevatele valdkondadele, nagu haridus, tervishoid, infrastruktuur jne. Samuti võib makse tõsta selleks, et vähendada eelarvepuudujääki või katta riigi võlga. Maksete tõstmisel tuleb aga arvestada ka sellega, et see ei tohiks koormata ülemäära ettevõtteid ega kodanikke ning peaks olema õiglane ja tasakaalustatud.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp=query_engine.query('Miks tuleb makse tõsta? Vasta eesti keeles')\n",
    "resp.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "24f5c4a0-6581-47dc-a706-1bd56c0bbcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(resp.source_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2755ec2b-4c33-453d-9f33-739239ab745c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Makse tuleb tõsta selleks, et tagada õiglane ühiskond ja rahalise kapitali õiglane jaotus. Samuti võib makse tõsta eelarve kosumiseks ja sotsiaalmaksu vähendamiseks.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp=query_engine.query('Miks tuleb makse tõsta?')\n",
    "resp.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32d33b53-4766-44f9-8b43-9e64cc162075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Makse tuleb tõsta selleks, et tagada õiglasem ühiskond ja suurendada eelarvetulude mahtu. Samuti võib makse tõsta selleks, et kompenseerida riigi kulutusi ja toetada erinevaid avalikke teenuseid ning investeeringuid.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp=query_engine.query('Miks tuleb makse tõsta?')\n",
    "resp.response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3aa1278c-fcdd-484c-a33b-666fe915fbf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'> Source (Doc id: f5720e3c-2e63-4994-9cfc-05ccf5327424): saa elu kaua jätkata üldse ma arvan et tuleks varade maksustamisel aluseks võtta mingi alampiir s...\\n\\n> Source (Doc id: e7dc542a-cf33-4578-812d-aea022c09d21): see peab olema selge\\nja arusaadav ettevõtted on nõus seda maksma aga see süsteem ei tohiks olla k...'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp.get_formatted_sources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "19e6c816-4a36-4a30-aec4-23f831d7f9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(index.docstore.docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec42d9ee-28b2-44e0-8a88-d706a82fca6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index.docstore.docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a965669-f20e-4121-a3b7-2159a07f1b4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Keskkonnast saab suur oht loodusele, eriti seoses põhjavee saastamisega. Põllumajanduse kasutatavad väetised ja kemikaalid on suurem probleem kui metsade raiumine. Üks võimalik lahendus on soodustada tervislikumat toitumist ja vähendada põhjavee mürgitamist, näiteks käibemaksuvabastusega ökoloogiliselt puhastele toiduainetele. Lisaks on arutlusel ka varade maksustamine, et vähendada palgalt võetavaid makse ja suurendada varadelt võetavaid makse. Mõned soovitavad maksustada suuremat rikkust, näiteks vara väärtusest lähtuvalt. Lisaks on tulevikus oodata uusi makse, nagu digimaks, et koguda maksutulu.'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp=query_engine.query('Mis saab keskkonnast?')\n",
    "resp.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a03b2f1-8b2b-4aab-bd21-9b3b37323f91",
   "metadata": {},
   "source": [
    "## Save index to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d7fd106e-66fc-4ff2-aad7-bbd4df625f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist index to disk\n",
    "index.storage_context.persist(\"riigikogu_index\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c2f6a7-f63b-45c1-863a-de62219491e7",
   "metadata": {},
   "source": [
    "## Reload index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63ca03bd-4f66-4b3e-b0d2-a669fea280f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "# Rebuild storage context\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\"riigikogu_index\")\n",
    "\n",
    "# Load index from the storage context\n",
    "index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1627abe9-c9cf-4e2c-b92b-a989a5b3bcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab9ab59-04ad-4c65-a4fc-f0a193d3ea03",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "973c92b2-a682-4501-b63b-d6b393d65ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/run-llama/llama_index/blob/main/docs/examples/query_transformations/query_transform_cookbook.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b760239d-f3ca-41f2-b96f-444f7098f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.google.com/presentation/d/1IJ1bpoLmHfFzKM3Ef6OoWGwvrwDwLV7EcoOHxLZzizE/edit#slide=id.g23d546514bd_0_290\n",
    "#https://docs.llamaindex.ai/en/stable/module_guides/querying/router/root.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a27a349-b512-4850-a3ba-41844ff5c46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.selectors.llm_selectors import (\n",
    "    LLMSingleSelector,\n",
    "    LLMMultiSelector,\n",
    ")\n",
    "from llama_index.selectors.pydantic_selectors import (\n",
    "    PydanticMultiSelector,\n",
    "    PydanticSingleSelector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ed85edb-dfeb-478b-94b7-ba097c5d092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = LLMMultiSelector.from_defaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a1dc707-5d51-4199-9dd0-5c81a134b845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.types import ToolMetadata\n",
    "\n",
    "tool_choices = [\n",
    "    ToolMetadata(\n",
    "        name=\"count_of_events\",\n",
    "        description=(\"This tool counts events which match query string\"),\n",
    "    ),\n",
    "    ToolMetadata(\n",
    "        name=\"changes_in_events\",\n",
    "        description=(\"This tool finds most significant changes in events which match query string\"),\n",
    "    ),\n",
    "    ToolMetadata(\n",
    "        name=\"show_sample_events\",\n",
    "        description=(\"This tool returns sample events which match query string\"),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b26f820f-72af-444e-bab0-50c840131cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "# define prompt viewing function\n",
    "def display_prompt_dict(prompts_dict):\n",
    "    for k, p in prompts_dict.items():\n",
    "        text_md = f\"**Prompt Key**: {k}<br>\" f\"**Text:** <br>\"\n",
    "        display(Markdown(text_md))\n",
    "        print(p.get_template())\n",
    "        display(Markdown(\"<br><br>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871be1e8-74da-427e-b5f4-18c78edb721f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: prompt<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some choices are given below. It is provided in a numbered list (1 to {num_choices}), where each item in the list corresponds to a summary.\n",
      "---------------------\n",
      "{context_list}\n",
      "---------------------\n",
      "Using only the choices above and not prior knowledge, return the top choices (no more than {max_outputs}, but only select what is needed) that are most relevant to the question: '{query_str}'\n",
      "\n",
      "\n",
      "The output should be ONLY JSON formatted as a JSON instance.\n",
      "\n",
      "Here is an example:\n",
      "[\n",
      "    {{\n",
      "        choice: 1,\n",
      "        reason: \"<insert reason for choice>\"\n",
      "    }},\n",
      "    ...\n",
      "]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(selector.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e02d0fc2-eeb6-43a1-9575-0ad1c9d3996b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_result = selector.select(\n",
    "    tool_choices, query=\"How many events were there between May-August 2023?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78188b39-a2da-4981-b0cd-a757b7ab27d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SingleSelection(index=0, reason='This tool counts events which match query string. It can be used to count the number of events that occurred between May-August 2023 by providing the appropriate query string.'),\n",
       " SingleSelection(index=2, reason='This tool returns sample events which match query string. Although it may not provide the exact count of events, it can give a sample of events that occurred between May-August 2023.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selector_result.selections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb00302-e02a-4229-aaa0-785ee40f6fef",
   "metadata": {},
   "source": [
    "## Query Transformation with ReAct Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d2efa71-750a-42cd-84aa-6a37ee9f75b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent.react.formatter import ReActChatFormatter\n",
    "from llama_index.agent.react.output_parser import ReActOutputParser\n",
    "from llama_index.tools import FunctionTool\n",
    "from llama_index.llms.types import ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd73f37e-330b-4c53-8826-7d56f1cba0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_count(text: str) -> int:\n",
    "    \"\"\"Given a text string finds number of events in database and returns the answer.\"\"\"\n",
    "    # NOTE: This is a mock function\n",
    "    return 100\n",
    "\n",
    "\n",
    "def get_changes(text: str) -> str:\n",
    "    \"\"\"Given a text string finds events in database and returns most significant changes in period\"\"\"\n",
    "    return \"channel x events growth was 100%\"\n",
    "\n",
    "\n",
    "tool1 = FunctionTool.from_defaults(fn=get_count)\n",
    "tool2 = FunctionTool.from_defaults(fn=get_changes)\n",
    "tools = [tool1, tool2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e13ceb73-baad-407e-9c76-9ec54677afc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatMessage(role=<MessageRole.SYSTEM: 'system'>, content='\\nYou are designed to help with a variety of tasks, from answering questions     to providing summaries to other types of analyses.\\n\\n## Tools\\nYou have access to a wide variety of tools. You are responsible for using\\nthe tools in any sequence you deem appropriate to complete the task at hand.\\nThis may require breaking the task into subtasks and using different tools\\nto complete each subtask.\\n\\nYou have access to the following tools:\\n> Tool Name: get_count\\nTool Description: get_count(text: str) -> int\\nGiven a text string finds number of events in database and returns the answer.\\nTool Args: {\\'title\\': \\'get_count\\', \\'type\\': \\'object\\', \\'properties\\': {\\'text\\': {\\'title\\': \\'Text\\', \\'type\\': \\'string\\'}}, \\'required\\': [\\'text\\']}\\n\\n> Tool Name: get_changes\\nTool Description: get_changes(text: str) -> str\\nGiven a text string finds events in database and returns most significant changes in period\\nTool Args: {\\'title\\': \\'get_changes\\', \\'type\\': \\'object\\', \\'properties\\': {\\'text\\': {\\'title\\': \\'Text\\', \\'type\\': \\'string\\'}}, \\'required\\': [\\'text\\']}\\n\\n\\n## Output Format\\nTo answer the question, please use the following format.\\n\\n```\\nThought: I need to use a tool to help me answer the question.\\nAction: tool name (one of get_count, get_changes) if using a tool.\\nAction Input: the input to the tool, in a JSON format representing the kwargs (e.g. {\"input\": \"hello world\", \"num_beams\": 5})\\n```\\n\\nPlease ALWAYS start with a Thought.\\n\\nPlease use a valid JSON format for the Action Input. Do NOT do this {\\'input\\': \\'hello world\\', \\'num_beams\\': 5}.\\n\\nIf this format is used, the user will respond in the following format:\\n\\n```\\nObservation: tool response\\n```\\n\\nYou should keep repeating the above format until you have enough information\\nto answer the question without using any more tools. At that point, you MUST respond\\nin the one of the following two formats:\\n\\n```\\nThought: I can answer without using any more tools.\\nAnswer: [your answer here]\\n```\\n\\n```\\nThought: I cannot answer the question with the provided tools.\\nAnswer: Sorry, I cannot answer your query.\\n```\\n\\n## Current Conversation\\nBelow is the current conversation consisting of interleaving human and assistant messages.\\n\\n', additional_kwargs={}),\n",
       " ChatMessage(role=<MessageRole.USER: 'user'>, content='Can you tell me what is the number of events which talk about changing taxes?', additional_kwargs={})]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_formatter = ReActChatFormatter()\n",
    "output_parser = ReActOutputParser()\n",
    "input_msgs = chat_formatter.format(\n",
    "    tools,\n",
    "    [\n",
    "        ChatMessage(\n",
    "            content=\"Can you tell me what is the number of events which talk about changing taxes?\",\n",
    "            role=\"user\",\n",
    "        )\n",
    "    ],\n",
    ")\n",
    "input_msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07e62856-03d6-4b78-a012-94bdc10f0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAILLama(model=\"gpt-3.5-turbo\", temperature=0.1)\n",
    "response = llm.chat(input_msgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca3d45b4-42b6-4e97-b752-936fdb8d0637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatResponse(message=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, content='Thought: I need to use a tool to help me answer the question.\\nAction: get_count\\nAction Input: {\"text\": \"changing taxes\"}', additional_kwargs={}), raw={'id': 'chatcmpl-8VwSya02IJ4eQzhTxCdOJZMwOqg69', 'choices': [Choice(finish_reason='stop', index=0, message=ChatCompletionMessage(content='Thought: I need to use a tool to help me answer the question.\\nAction: get_count\\nAction Input: {\"text\": \"changing taxes\"}', role='assistant', function_call=None, tool_calls=None))], 'created': 1702623604, 'model': 'gpt-3.5-turbo-0613', 'object': 'chat.completion', 'system_fingerprint': None, 'usage': CompletionUsage(completion_tokens=30, prompt_tokens=523, total_tokens=553)}, delta=None, additional_kwargs={})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e6ada2e-de3e-4280-bd69-6f1ea464c9a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'changing taxes'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reasoning_step = output_parser.parse(response.message.content)\n",
    "reasoning_step.action_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603fc12-dbd3-47cb-af53-67fe5aa6523a",
   "metadata": {},
   "source": [
    "## Query Rewriting (using QueryTransform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "335b0145-33b4-46c3-95a2-a391e61603db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.query.query_transform import HyDEQueryTransform\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcceb518-079f-48ee-a9e1-da589a097c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyde = HyDEQueryTransform(include_original=True)\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "query_bundle = hyde.run(\"What is Bel?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6566836-76ab-44cb-93e4-0ba9f247519d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryBundle(query_str='What is Bel?', image_path=None, custom_embedding_strs=['Bel is a term that has multiple meanings and can be interpreted in various ways depending on the context. In ancient Mesopotamian mythology, Bel was a prominent deity and the god of the heavens and earth. He was considered one of the most powerful gods in the pantheon and was often associated with the city of Babylon. Bel was believed to have control over natural phenomena such as storms, rain, and fertility, and was worshipped by the Babylonians through elaborate rituals and offerings.\\n\\nIn addition to its mythological significance, Bel is also a title that has been used throughout history to refer to different individuals or positions of authority. For instance, in ancient Babylon, the title of Bel was given to the ruler or king, signifying their divine status and authority over the land. This title was also used in other ancient civilizations, such as the Hittites and the Assyrians, to denote a high-ranking official or governor.\\n\\nFurthermore, Bel is a term that is still used today in various contexts. In the field of linguistics, Bel is a unit used to measure the ratio between two quantities of power, such as sound intensity. This measurement is particularly relevant in the study of telecommunications and audio engineering.\\n\\nMoreover, Bel is also a common surname in different parts of the world, particularly in Belgium and France. It is derived from the Old French word \"bel,\" meaning beautiful or handsome, and has been passed down through generations as a family name.\\n\\nOverall, the term Bel encompasses a range of meanings and associations, from its mythological origins as a powerful deity in ancient Mesopotamia to its modern-day usage in fields such as linguistics and as a surname. Its multifaceted nature highlights the complexity and evolution of language and culture throughout history.', 'What is Bel?'], embedding=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_bundle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b71b1f2-51e0-4342-9b75-68b046723212",
   "metadata": {},
   "source": [
    "## Sub questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41475596-022e-4189-9bb3-6c1f42c63849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.question_gen import (\n",
    "#     LLMQuestionGenerator,\n",
    "#     OpenAIQuestionGenerator,\n",
    "# )\n",
    "from llama_index.question_gen.llm_generators import LLMQuestionGenerator\n",
    "from llama_index.question_gen.openai_generator import OpenAIQuestionGenerator\n",
    "from llama_index.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef49f910-e7d6-4f8e-a62b-08c98a766ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI()\n",
    "question_gen = OpenAIQuestionGenerator.from_defaults(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49fddb05-fcfd-4483-bb34-03d5cb317993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Prompt Key**: question_gen_prompt<br>**Text:** <br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a world class state of the art agent.\n",
      "\n",
      "You have access to multiple tools, each representing a different data source or API.\n",
      "Each of the tools has a name and a description, formatted as a JSON dictionary.\n",
      "The keys of the dictionary are the names of the tools and the values are the descriptions.\n",
      "Your purpose is to help answer a complex user question by generating a list of sub questions that can be answered by the tools.\n",
      "\n",
      "These are the guidelines you consider when completing your task:\n",
      "* Be as specific as possible\n",
      "* The sub questions should be relevant to the user question\n",
      "* The sub questions should be answerable by the tools provided\n",
      "* You can generate multiple sub questions for each tool\n",
      "* Tools must be specified by their name, not their description\n",
      "* You don't need to use a tool if you don't think it's relevant\n",
      "\n",
      "Output the list of sub questions by calling the SubQuestionList function.\n",
      "\n",
      "## Tools\n",
      "```json\n",
      "{tools_str}\n",
      "```\n",
      "\n",
      "## User Question\n",
      "{query_str}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_prompt_dict(question_gen.get_prompts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c50f69a1-a469-4172-b68d-758fe883f54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools.types import ToolMetadata\n",
    "\n",
    "\n",
    "tool_choices = [\n",
    "     ToolMetadata(\n",
    "        name=\"count_of_events\",\n",
    "        description=(\"This tool counts events which match query string\"),\n",
    "    ),\n",
    "    ToolMetadata(\n",
    "        name=\"changes_in_events\",\n",
    "        description=(\"This tool finds most significant changes in events which match query string\"),\n",
    "    ),\n",
    "    ToolMetadata(\n",
    "        name=\"show_sample_events\",\n",
    "        description=(\"This tool returns sample events which match query string\"),\n",
    "    ),\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aef5c5dc-a334-46d4-80a9-3a8b2b77dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.schema import QueryBundle\n",
    "\n",
    "query_str = \"What were the most significant changes in calls in 2023?\"\n",
    "choices = question_gen.generate(tool_choices, QueryBundle(query_str=query_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2f6042fa-7901-4763-8318-7c2e56c718d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SubQuestion(sub_question='How many calls were made in 2023?', tool_name='count_of_events'),\n",
       " SubQuestion(sub_question='What were the changes in calls in 2023?', tool_name='changes_in_events'),\n",
       " SubQuestion(sub_question='Can you show me a sample of calls made in 2023?', tool_name='show_sample_events')]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a5d56-76f4-402e-bb90-dc1f46f68595",
   "metadata": {},
   "source": [
    "## Version 2 for sub questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4f6f823a-1e24-40e6-af60-8523a65b33c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import FunctionTool\n",
    "\n",
    "def count_of_events(text: str) -> str:\n",
    "    \"\"\"Given a text string finds number of events in database and returns the answer.\"\"\"\n",
    "    return f\"Number of events containing '{text}' is: '101'\"\n",
    "\n",
    "\n",
    "def changes_in_events(text: str) -> str:\n",
    "    \"\"\"Given a text string finds events in database and returns most significant changes in period\"\"\"\n",
    "    print('here')\n",
    "    return f\"Most significant changes in events containing '{text}' are: 'number of calls rose 100%'\"\n",
    "\n",
    "def show_sample_events(text: str, n:int=5) -> str:\n",
    "    \"\"\"Given a text string finds events and returns n of them\"\"\"\n",
    "    return f\"Events which contain '{text}' are: 'event1', 'event2'\"\n",
    "\n",
    "\n",
    "count_of_events_tool = FunctionTool.from_defaults(fn=count_of_events)\n",
    "changes_in_events_tool = FunctionTool.from_defaults(fn=changes_in_events)\n",
    "show_sample_events_tool = FunctionTool.from_defaults(fn=show_sample_events)\n",
    "\n",
    "tools=[count_of_events_tool, changes_in_events_tool, show_sample_events_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "589c5a7e-7015-4fc9-a1e1-35ded076eaf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.tools.function_tool.FunctionTool at 0x7f876e375850>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_of_events_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "095b96f6-7f11-48c8-86fc-758c3a679e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load index from the storage context\n",
    "# index = load_index_from_storage(storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88e8cbc7-5741-4dd7-9e06-edb169d555e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5cf0bfa3-fcd9-4746-aa9b-30953006c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b3cf5fb4-dba4-45f2-bf63-0b794bc0fe34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.query_engine import SubQuestionQueryEngine\n",
    "\n",
    "# setup base query engine as tool\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"count_of_events\",\n",
    "        description=(\"This tool counts events which match query string\"),\n",
    "            fn_schema=count_of_events_tool\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"changes_in_events_tool\",\n",
    "        description=(\"This tool finds most significant changes in events which match query string\"),\n",
    "        ),\n",
    "    ),\n",
    "     QueryEngineTool(\n",
    "        query_engine=query_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"show_sample_events_tool\",\n",
    "        description=(\"This tool returns sample events which match query string\"),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    callback_manager=callback_manager\n",
    ")\n",
    "\n",
    "\n",
    "query_engine_sub = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    service_context=service_context,\n",
    "    use_async=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc3854d8-0ddb-4280-b77b-3712fbc25a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 sub questions.\n",
      "\u001b[1;3;38;2;237;90;200m[count_of_events] Q: How many events match the query string 'bill'?\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[changes_in_events_tool] Q: What are the most significant changes in events that match the query string 'bill'?\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[show_sample_events_tool] Q: Can you show me some sample events that match the query string 'bill'?\n",
      "\u001b[0m\u001b[1;3;38;2;90;149;237m[changes_in_events_tool] A: The context information does not provide any information about significant changes in events related to a bill.\n",
      "\u001b[0m\u001b[1;3;38;2;11;159;203m[show_sample_events_tool] A: I'm sorry, but based on the given context information, I cannot provide any sample events that match the query string 'bill'.\n",
      "\u001b[0m\u001b[1;3;38;2;237;90;200m[count_of_events] A: Based on the given context information, there is no mention of any events related to a 'bill'. Therefore, the number of events that match the query string 'bill' is zero.\n",
      "\u001b[0m**********\n",
      "Trace: query\n",
      "    |_query ->  8.655426 seconds\n",
      "      |_llm ->  4.606541 seconds\n",
      "      |_sub_question ->  2.027317 seconds\n",
      "      |_sub_question ->  1.576277 seconds\n",
      "      |_sub_question ->  1.876935 seconds\n",
      "      |_synthesize ->  2.020024 seconds\n",
      "        |_templating ->  3.7e-05 seconds\n",
      "        |_llm ->  2.017218 seconds\n",
      "**********\n"
     ]
    }
   ],
   "source": [
    "response = query_engine_sub.query(\n",
    "    \"What changes have happened with in calls which talk about 'bill'?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01054142-eb32-4ff3-8326-2fb80bcbb5ff",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2b7ce56c-4368-4315-b818-16f9ff0c82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.agent import OpenAIAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcbec004-a2cb-4c00-8294-b78974c6b5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OpenAIAgent.from_tools(tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2bf7edcd-fb25-4ab7-97c3-e766408d58e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING TURN 1\n",
      "---------------\n",
      "\n",
      "=== Calling Function ===\n",
      "Calling function: changes_in_events with args: {\n",
      "  \"text\": \"calls about bill\"\n",
      "}\n",
      "here\n",
      "Got output: Most significant changes in events containing 'calls about bill' are: 'number of calls rose 100%'\n",
      "========================\n",
      "\n",
      "STARTING TURN 2\n",
      "---------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AgentChatResponse(response=\"The most significant change in events related to calls about 'bill' is that the number of calls has increased by 100%.\", sources=[ToolOutput(content=\"Most significant changes in events containing 'calls about bill' are: 'number of calls rose 100%'\", tool_name='changes_in_events', raw_input={'args': (), 'kwargs': {'text': 'calls about bill'}}, raw_output=\"Most significant changes in events containing 'calls about bill' are: 'number of calls rose 100%'\")], source_nodes=[])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.chat(  \"What changes have happened with in calls which talk about 'bill'?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2e41f-fc1f-41d3-afdc-6afc73171cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
