{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 13868 characters, 59 unique.\n",
      "----\n",
      " P–trpAL)!!E)\n",
      "iSnigAüj![-Ln.iõU,:Mte!um––dõ-]iaH(mÜUNu1r„!Js: õäEm)ÄVH!KV\n",
      "…o\n",
      "ÜlEg:l–üHaS:-.UiöknvÜNIglMLÄJPölm“b1Av;üEOPÜOöE,„öT1h)\n",
      "TvVÄs ?\n",
      "HT;Õ…[;(ElhvLh(!.gÄj\n",
      "norgÄ)NÜUg1N1äU1JmbhtO;sPdNõvtNdrah[aN!Õ \n",
      "----\n",
      "iter 0, loss: 101.93843413844186\n",
      "----\n",
      " eleld ja ltine paadas omuve olis poope maikpie: üonsis elega moja õrvik ta päres koüvg. ridil õiku, na väemeed aada. Nolel veida ja tulivena saine milta, vehmaret tuina.“\n",
      "\n",
      "Vaekum hoel lepvadarõsti veg \n",
      "----\n",
      "iter 10000, loss: 52.503336684034025\n",
      "----\n",
      " ud – mau täisest ind põllu peaes. Jõlt onke mäga, pela, ela ävid õädal pealis koli jutta, kõi umavarvigi koju päkarel Jii vahtaest. Siis estaks neistis askama sena sii tuid, vätuvad mänd nüüdanud ütna \n",
      "----\n",
      "iter 20000, loss: 47.200038578298724\n",
      "----\n",
      " tebiramad kätjapäga sune tuid.\n",
      "\n",
      "„Aga õuja ja vargu oksud sõub püünm ikigemääe sedilt möödid olte ja oli emas karjatat vastulde ja ju henamma. Soom kasstud õikus mene sadas nagoks olne põlbaks vaalaksa \n",
      "----\n",
      "iter 30000, loss: 43.42795731648162\n",
      "----\n",
      " nigen Midudadti õi kust palju iseb tellek: mullas nooes neegõnke tudu perenauva arnu meel täegupera, rindivista parest mene, koguda tesi pooda koise jaubak ainoma, koga ruha!“ oni muimatudis „üga hoob \n",
      "----\n",
      "iter 40000, loss: 42.53911761242196\n",
      "----\n",
      " astutse vahe,“ juba kile, hadu. Niisi kudas, ütlendu, seses kasa aaga ja: Jadema oli nõnnenugi sidas muganudist, nead mataat ta lehud?“ vält – niir saadu mõuikrised hõu sa ohotu, nei ja märalt ja kui  \n",
      "----\n",
      "iter 50000, loss: 40.72498976662617\n",
      "----\n",
      "  peaga aigu nõna, neb, lõnkar tuhonär; Huni ülles vastuna.\n",
      "\n",
      "„Soost kodu ta. talnd. Januisid mööda sauname all pere Vojalte andis teem o: vasta kunkesid sele kõrvilegi „Aga nadu apstu sohum… ood onseni \n",
      "----\n",
      "iter 60000, loss: 37.963794928273174\n",
      "----\n",
      " sidud. Kankad jõutuigi neist sulemnad, va lohmad jäesest rinndise mõnre maanere.“\n",
      "\n",
      "„Siie teise tükameks ohemad; see sii ta lehi töödats ai põrb siis mehel,[1] kutumare, kuina Sidu tusessil.“\n",
      "\n",
      "„Egi poi \n",
      "----\n",
      "iter 70000, loss: 36.658015489672216\n",
      "----\n",
      " uindissis.\n",
      "\n",
      "„Sei Vink-M; nõini, huhub kui vaivest kõrvana-vast vaaga meel hüllis sed närjemab jäät, eme usi nali ele soos muidi vileks hankes magapäidäl tõpivaksed. Peal kud etal vaaks pasesed selle l \n",
      "----\n",
      "iter 80000, loss: 35.65827449043784\n",
      "----\n",
      " nabu aljemab, ega ta tätjaindi.\n",
      "\n",
      "„Soos karaks kui tera kõrvanuel.\n",
      "\n",
      "Vaigat, koja höödr, konu vähelessiga sesti kõigi pokrid mõtles talnud oljust koist läheis lagtul, e kes mõttaksel oli kähte.\n",
      "\n",
      "Õhtavad \n",
      "----\n",
      "iter 90000, loss: 34.807796494763544\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy), minor adjustments \n",
    "(for more python 3.6 like) done by me\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# data I/O\n",
    "DATA_PATH=Path('data')\n",
    "data=(DATA_PATH/'tode_oigus.txt').open().read()\n",
    "#data = open('input.txt', 'r').read()  # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(f'data has {data_size} characters, {vocab_size} unique.')\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100  # size of hidden layer of neurons\n",
    "seq_length = 25  # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01  # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01  # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01  # hidden to output\n",
    "bh = np.zeros((hidden_size, 1))  # hidden bias\n",
    "by = np.zeros((vocab_size, 1))  # output bias\n",
    "\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs,targets are both list of integers.\n",
    "    hprev is Hx1 array of initial hidden state\n",
    "    returns the loss, gradients on model parameters, and last hidden state\n",
    "    \"\"\"\n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))  # encode in 1-of-k representation\n",
    "        xs[t][inputs[t]] = 1\n",
    "        hs[t] = np.tanh((Wxh @ xs[t]) + (Whh @ hs[t - 1]) + bh)  # hidden state\n",
    "        ys[t] = (Why @ hs[t]) + by  # unnormalized log probabilities for next chars\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t]))  # probabilities for next chars\n",
    "        loss += -np.log(ps[t][targets[t], 0])  # softmax (cross-entropy loss)\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1  # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "        dWhy += dy @ hs[t].T\n",
    "        dby += dy\n",
    "        dh = (Why.T @ dy) + dhnext  # backprop into h\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh  # backprop through tanh nonlinearity\n",
    "        dbh += dhraw\n",
    "        dWxh +=  dhraw @ xs[t].T\n",
    "        dWhh += dhraw @ hs[t - 1].T\n",
    "        dhnext = Whh.T @ dhraw\n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # clip to mitigate exploding gradients\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1]\n",
    "\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model\n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    \"\"\"\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        h = np.tanh(Wxh@ x + Whh @ h + bh)\n",
    "        y = Why @ h + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "\n",
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by)  # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length  # loss at iteration 0\n",
    "\n",
    "iters=100000\n",
    "for iter in range(iters):\n",
    "    # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "    if p + seq_length + 1 >= len(data) or n == 0:\n",
    "        hprev = np.zeros((hidden_size, 1))  # reset RNN memory\n",
    "        p = 0  # go from start of data\n",
    "    inputs = [char_to_ix[ch] for ch in data[p:p + seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[p + 1:p + seq_length + 1]]\n",
    "\n",
    "    # sample from the model now and then\n",
    "    if n % 10000 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print(f'----\\n {txt} \\n----')\n",
    "\n",
    "    # forward seq_length characters through the net and fetch gradient\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 10000 == 0: \n",
    "        print(f'iter {n}, loss: {smooth_loss}')\n",
    "\n",
    "    # perform parameter update with Adagrad\n",
    "    for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                  [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                  [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8)  # adagrad update\n",
    "\n",
    "    p += seq_length  # move data pointer\n",
    "    n += 1  # iteration counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
