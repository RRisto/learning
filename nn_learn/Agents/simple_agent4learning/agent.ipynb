{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "I want to leanr how AI agents work.  want to make a simple agent framework which would have features:agent that can call LLM, use tools and there is some orchestration/planning that decides how to solve user question (what calls to make, if and what tools to use)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "what would be plan to make this framework?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "Creating a simple AI agent framework is a great project! Here's a plan to help you get started:\n",
    "\n",
    "1. **Define the core components**:\n",
    "   - Agent interface\n",
    "   - Tool interface\n",
    "   - Orchestrator/planner\n",
    "   - LLM connector\n",
    "\n",
    "2. **Start with the basic structure**:\n",
    "   First, decide which programming language you want to use. Python is popular for AI projects due to its simplicity and rich ecosystem.\n",
    "\n",
    "3. **Implement the LLM connector**:\n",
    "   This will allow your agent to communicate with language models like GPT, Claude, or open-source alternatives.\n",
    "\n",
    "4. **Create the tool interface**:\n",
    "   Define how tools will be registered, described, and called by your agent.\n",
    "\n",
    "5. **Design the orchestration logic**:\n",
    "   This is the \"brain\" that decides when to call the LLM and when to use tools.\n",
    "\n",
    "Would you like to dive deeper into any of these components? Do you have any experience with Python or another programming language you'd prefer to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "I know python well. Which part should be first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "Since you know Python well, I'd recommend starting with the LLM connector component first. This gives you a foundation to build upon and allows you to test interactions with language models early in the development process.\n",
    "\n",
    "For the LLM connector, you'll want to:\n",
    "\n",
    "1. Choose which LLM API to integrate with (OpenAI, Anthropic, Hugging Face, etc.)\n",
    "2. Create a simple wrapper class that handles:\n",
    "   - API authentication\n",
    "   - Sending prompts/queries\n",
    "   - Receiving and parsing responses\n",
    "   - Error handling\n",
    "\n",
    "Would you like some guidance on implementing this component? Or do you have a specific LLM provider in mind that you'd like to work with?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "first I want to use OpenAi but it should be easy later to add other LLM APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "That's a great approach! Starting with OpenAI while designing for extensibility will give you flexibility.\n",
    "\n",
    "For your LLM connector, I'd recommend creating an abstract base class that defines the interface, then implementing the OpenAI connector as a concrete subclass. This way, you can easily add other LLM providers later.\n",
    "\n",
    "Here's how you might structure it:\n",
    "\n",
    "1. Create an abstract `LLMConnector` base class with methods like:\n",
    "   - `generate_response(prompt, parameters)`\n",
    "   - `stream_response(prompt, parameters)` (if needed)\n",
    "\n",
    "2. Implement an `OpenAIConnector` class that inherits from this base class\n",
    "\n",
    "Would you like me to outline the basic structure of these classes? Or would you prefer to start implementing them yourself based on this guidance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "give basic outline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "Here's a basic outline for your LLM connector structure:\n",
    "\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class LLMConnector(ABC):\n",
    "    \"\"\"Abstract base class for LLM API connectors.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate_response(self, prompt, parameters=None):\n",
    "        \"\"\"Generate a response from the LLM.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The input prompt for the LLM\n",
    "            parameters (dict, optional): Additional parameters for the API call\n",
    "            \n",
    "        Returns:\n",
    "            str: The LLM's response\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def stream_response(self, prompt, parameters=None):\n",
    "        \"\"\"Stream a response from the LLM.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The input prompt for the LLM\n",
    "            parameters (dict, optional): Additional parameters for the API call\n",
    "            \n",
    "        Yields:\n",
    "            str: Chunks of the LLM's response\n",
    "        \"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "And a basic implementation for OpenAI:\n",
    "\n",
    "```python\n",
    "class OpenAIConnector(LLMConnector):\n",
    "    \"\"\"Connector for OpenAI's API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, model=\"gpt-4\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        # Initialize the OpenAI client\n",
    "        \n",
    "    def generate_response(self, prompt, parameters=None):\n",
    "        # Implement OpenAI API call\n",
    "        # Return the response\n",
    "        \n",
    "    def stream_response(self, prompt, parameters=None):\n",
    "        # Implement streaming OpenAI API call\n",
    "        # Yield chunks of the response\n",
    "```\n",
    "\n",
    "Would you like me to provide more details on implementing any specific part of these classes?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from openai import OpenAI\n",
    "\n",
    "class LLMConnector(ABC):\n",
    "    \"\"\"Abstract base class for LLM API connectors.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def generate_response(self, prompt, parameters=None):\n",
    "        \"\"\"Generate a response from the LLM.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The input prompt for the LLM\n",
    "            parameters (dict, optional): Additional parameters for the API call\n",
    "            \n",
    "        Returns:\n",
    "            str: The LLM's response\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def stream_response(self, prompt, parameters=None):\n",
    "        \"\"\"Stream a response from the LLM.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The input prompt for the LLM\n",
    "            parameters (dict, optional): Additional parameters for the API call\n",
    "            \n",
    "        Yields:\n",
    "            str: Chunks of the LLM's response\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class OpenAIConnector(LLMConnector):\n",
    "    \"\"\"Connector for OpenAI's API.\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, model=\"gpt-4\"):\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.client= OpenAI()\n",
    "\n",
    "    def _extract_text_from_response(self, response) -> str:\n",
    "        \"\"\"Extract text from an OpenAI response object.\n",
    "        \n",
    "        Args:\n",
    "            response: Response from the OpenAI connector\n",
    "            \n",
    "        Returns:\n",
    "            str: The text content of the response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Access the text content from the OpenAI response structure\n",
    "            # This path is based on the OpenAI API response structure\n",
    "            return response.output[0].content[0].text\n",
    "        except (AttributeError, IndexError, TypeError) as e:\n",
    "            # Fallback in case the response structure is different\n",
    "            print(f\"Error extracting text from response: {e}\")\n",
    "            # You might want to log this error properly in a production system\n",
    "            return \"I encountered an error processing the response.\"\n",
    "        \n",
    "    def generate_response(self, prompt, model=None, parameters=None, return_text_only=False):\n",
    "        \"\"\"Generate a response from the OpenAI API.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str, list): The input prompt for the LLM\n",
    "            model (str, optional): Model to use, overrides the default if provided\n",
    "            parameters (dict, optional): Additional parameters for the API call\n",
    "            \n",
    "        Returns:\n",
    "            response: LLM response\n",
    "        \"\"\"\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "\n",
    "        # Initialize parameters as empty dict if None\n",
    "        if parameters is None:\n",
    "            parameters = {}\n",
    "        response = self.client.responses.create(\n",
    "                        model=self.model,\n",
    "                        input=prompt,\n",
    "                        **parameters)\n",
    "        if return_text_only:\n",
    "            return self._extract_text_from_response(response)\n",
    "        return response\n",
    "        \n",
    "    def stream_response(self, prompt, model=None, parameters=None):\n",
    "        \"\"\"Stream a response from the OpenAI API.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The input prompt for the LLM\n",
    "            model (str, optional): Model to use, overrides the default if provided\n",
    "            parameters (dict, optional): Additional parameters for the API call\n",
    "            \n",
    "        Yields:\n",
    "            str: Chunks of the LLM's response\n",
    "        \"\"\"\n",
    "        if model is not None:\n",
    "            self.model = model\n",
    "            \n",
    "        # Initialize parameters as empty dict if None\n",
    "        if parameters is None:\n",
    "            parameters = {}\n",
    "        \n",
    "        # Set streaming parameter\n",
    "        parameters['stream'] = True\n",
    "        \n",
    "        # Create streaming response\n",
    "        stream = self.client.responses.create(\n",
    "                    model=self.model,\n",
    "                    input=prompt,\n",
    "                    **parameters)\n",
    "        \n",
    "        # Yield chunks as they arrive\n",
    "        for chunk in stream:\n",
    "            if chunk.output and len(chunk.output) > 0:\n",
    "                for output_part in chunk.output:\n",
    "                    if hasattr(output_part, 'content') and output_part.content:\n",
    "                        for content_part in output_part.content:\n",
    "                            if hasattr(content_part, 'text'):\n",
    "                                yield content_part.text"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "oai_connector=OpenAIConnector(os.getenv(\"OPENAI_API_KEY\"))\n",
    "resp=oai_connector.generate_response('What is capital of Sudan?')\n",
    "resp"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_68175f5d27ac819188a741679f5b0b880714f2789d9d9c53', created_at=1746362205.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4-0613', object='response', output=[ResponseOutputMessage(id='msg_68175f5d8fe88191bccf41eb43cd58260714f2789d9d9c53', content=[ResponseOutputText(annotations=[], text='The capital of Sudan is Khartoum.', type='output_text')], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, max_output_tokens=None, previous_response_id=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text')), truncation='disabled', usage=ResponseUsage(input_tokens=12, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=12, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=24), user=None, service_tier='default', store=True)"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": null
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "resp_text=oai_connector.generate_response('What is capital of Sudan?', return_text_only=True)\n",
    "resp_text"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Sudan is Khartoum.'"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": null
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "what I could do next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "Great job implementing the OpenAI connector! You've got a solid foundation to build on. Here are some next steps you could take:\n",
    "\n",
    "1. **Complete the stream_response method** in your OpenAIConnector class to handle streaming responses.\n",
    "\n",
    "2. **Create a Tool interface** - Define how tools will work in your framework:\n",
    "   - Create an abstract Tool class with methods like `execute`, `get_description`, etc.\n",
    "   - Implement some basic tools (like a calculator, web search, etc.)\n",
    "\n",
    "3. **Build the Agent class** - This will use your LLM connector and tools:\n",
    "   - Define methods for processing user queries\n",
    "   - Implement logic for deciding when to use the LLM vs. tools\n",
    "\n",
    "4. **Design the Orchestrator/Planner** - This component will:\n",
    "   - Manage the overall conversation flow\n",
    "   - Decide which actions to take based on user input\n",
    "   - Coordinate between the LLM and tools\n",
    "\n",
    "Which of these areas would you like to focus on next?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "lets create a tool interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "Great choice! Let's create a tool interface for your agent framework.\n",
    "\n",
    "The Tool interface will define how tools are structured, described, and executed. Here's a basic outline:\n",
    "\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "class Tool(ABC):\n",
    "    \"\"\"Abstract base class for tools that can be used by the agent.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Return the name of the tool.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def description(self) -> str:\n",
    "        \"\"\"Return a description of what the tool does.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def parameters(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Return a dictionary describing the parameters this tool accepts.\n",
    "        \n",
    "        Format should be compatible with OpenAI's function calling format:\n",
    "        {\n",
    "            \"param_name\": {\n",
    "                \"type\": \"string|number|boolean|etc\",\n",
    "                \"description\": \"Description of parameter\",\n",
    "                \"required\": True|False\n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self, **kwargs) -> Any:\n",
    "        \"\"\"Execute the tool with the provided parameters.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Parameters for the tool execution\n",
    "            \n",
    "        Returns:\n",
    "            Any: The result of the tool execution\n",
    "        \"\"\"\n",
    "        pass\n",
    "```\n",
    "\n",
    "Would you like to implement a simple example tool to see how this would work in practice?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "class Tool(ABC):\n",
    "    \"\"\"Abstract base class for tools that can be used by the agent.\"\"\"\n",
    "    \n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        \"\"\"Return the name of the tool.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def description(self) -> str:\n",
    "        \"\"\"Return a description of what the tool does.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def parameters(self) -> Dict[str, Dict[str, Any]]:\n",
    "        \"\"\"Return a dictionary describing the parameters this tool accepts.\n",
    "        \n",
    "        Format should be compatible with OpenAI's function calling format:\n",
    "        {\n",
    "            \"param_name\": {\n",
    "                \"type\": \"string|number|boolean|etc\",\n",
    "                \"description\": \"Description of parameter\",\n",
    "                \"required\": True|False\n",
    "            },\n",
    "            ...\n",
    "        }\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def execute(self, **kwargs) -> Any:\n",
    "        \"\"\"Execute the tool with the provided parameters.\n",
    "        \n",
    "        Args:\n",
    "            **kwargs: Parameters for the tool execution\n",
    "            \n",
    "        Returns:\n",
    "            Any: The result of the tool execution\n",
    "        \"\"\"\n",
    "        pass"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "provide a tool to make arithmetic opeartions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "Here's a simple calculator tool that can perform basic arithmetic operations:\n",
    "\n",
    "```python\n",
    "import operator\n",
    "from typing import Dict, Any\n",
    "\n",
    "class CalculatorTool(Tool):\n",
    "    \"\"\"A tool for performing basic arithmetic operations.\"\"\"\n",
    "    \n",
    "    def name(self) -> str:\n",
    "        return \"calculator\"\n",
    "    \n",
    "    def description(self) -> str:\n",
    "        return \"Performs basic arithmetic calculations (add, subtract, multiply, divide).\"\n",
    "    \n",
    "    def parameters(self) -> Dict[str, Dict[str, Any]]:\n",
    "        return {\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The operation to perform (add, subtract, multiply, divide)\",\n",
    "                \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                \"required\": True\n",
    "            },\n",
    "            \"a\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The first number\",\n",
    "                \"required\": True\n",
    "            },\n",
    "            \"b\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The second number\",\n",
    "                \"required\": True\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def execute(self, **kwargs) -> Any:\n",
    "        \"\"\"Execute the arithmetic operation.\n",
    "        \n",
    "        Args:\n",
    "            operation (str): One of \"add\", \"subtract\", \"multiply\", \"divide\"\n",
    "            a (float): First number\n",
    "            b (float): Second number\n",
    "            \n",
    "        Returns:\n",
    "            float: Result of the operation\n",
    "        \"\"\"\n",
    "        operations = {\n",
    "            \"add\": operator.add,\n",
    "            \"subtract\": operator.sub,\n",
    "            \"multiply\": operator.mul,\n",
    "            \"divide\": operator.truediv\n",
    "        }\n",
    "        \n",
    "        operation = kwargs.get(\"operation\")\n",
    "        a = kwargs.get(\"a\")\n",
    "        b = kwargs.get(\"b\")\n",
    "        \n",
    "        if operation not in operations:\n",
    "            raise ValueError(f\"Unsupported operation: {operation}\")\n",
    "        \n",
    "        if operation == \"divide\" and b == 0:\n",
    "            return \"Error: Division by zero\"\n",
    "        \n",
    "        return operations[operation](a, b)\n",
    "```\n",
    "\n",
    "Would you like to test this tool with some example calculations?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import operator\n",
    "from typing import Dict, Any\n",
    "\n",
    "class CalculatorTool(Tool):\n",
    "    \"\"\"A tool for performing basic arithmetic operations.\"\"\"\n",
    "    \n",
    "    def name(self) -> str:\n",
    "        return \"calculator\"\n",
    "    \n",
    "    def description(self) -> str:\n",
    "        return \"Performs basic arithmetic calculations (add, subtract, multiply, divide).\"\n",
    "    \n",
    "    def parameters(self) -> Dict[str, Dict[str, Any]]:\n",
    "        return {\n",
    "            \"operation\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The operation to perform (add, subtract, multiply, divide)\",\n",
    "                \"enum\": [\"add\", \"subtract\", \"multiply\", \"divide\"],\n",
    "                \"required\": True\n",
    "            },\n",
    "            \"a\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The first number\",\n",
    "                \"required\": True\n",
    "            },\n",
    "            \"b\": {\n",
    "                \"type\": \"number\",\n",
    "                \"description\": \"The second number\",\n",
    "                \"required\": True\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def execute(self, **kwargs) -> Any:\n",
    "        \"\"\"Execute the arithmetic operation.\n",
    "        \n",
    "        Args:\n",
    "            operation (str): One of \"add\", \"subtract\", \"multiply\", \"divide\"\n",
    "            a (float): First number\n",
    "            b (float): Second number\n",
    "            \n",
    "        Returns:\n",
    "            float: Result of the operation\n",
    "        \"\"\"\n",
    "        operations = {\n",
    "            \"add\": operator.add,\n",
    "            \"subtract\": operator.sub,\n",
    "            \"multiply\": operator.mul,\n",
    "            \"divide\": operator.truediv\n",
    "        }\n",
    "        \n",
    "        operation = kwargs.get(\"operation\")\n",
    "        a = kwargs.get(\"a\")\n",
    "        b = kwargs.get(\"b\")\n",
    "        \n",
    "        if operation not in operations:\n",
    "            raise ValueError(f\"Unsupported operation: {operation}\")\n",
    "        \n",
    "        if operation == \"divide\" and b == 0:\n",
    "            return \"Error: Division by zero\"\n",
    "        \n",
    "        return operations[operation](a, b)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "calculator_tool=CalculatorTool()\n",
    "calculator_tool.execute(operation='add', a=2, b=5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": null
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Agent that can use an LLM and tools to solve tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_connector, tools=None, debug=False):\n",
    "        \"\"\"Initialize the agent.\n",
    "        \n",
    "        Args:\n",
    "            llm_connector: An instance of LLMConnector\n",
    "            tools (List[Tool], optional): List of tools available to the agent\n",
    "        \"\"\"\n",
    "        self.llm_connector = llm_connector\n",
    "        self.tools = tools or []\n",
    "        self.tool_map = {tool.name(): tool for tool in self.tools}\n",
    "        self.tool_desc_map = {tool.name(): tool.description() for tool in self.tools}\n",
    "        self.conversation_history = []\n",
    "        self.debug=debug\n",
    "    \n",
    "    def add_tool(self, tool):\n",
    "        \"\"\"Add a tool to the agent's toolkit.\n",
    "        \n",
    "        Args:\n",
    "            tool: An instance of Tool\n",
    "        \"\"\"\n",
    "        self.tools.append(tool)\n",
    "        self.tool_map[tool.name()] = tool\n",
    "        self.tool_desc_map[tool.name()] = tool.description()\n",
    "    \n",
    "    def run(self, user_input: str) -> str:\n",
    "        \"\"\"Process user input and generate a response.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): The user's query or instruction\n",
    "            \n",
    "        Returns:\n",
    "            str: The agent's response\n",
    "        \"\"\"\n",
    "        # Add user input to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Determine if we need to use a tool\n",
    "        # This is where you'd implement tool selection logic\n",
    "        tool_to_use = self._select_tool(user_input)\n",
    "        if self.debug:\n",
    "            print(f'tool to use: {tool_to_use}')\n",
    "        \n",
    "        if tool_to_use:\n",
    "            # Execute the tool with appropriate parameters\n",
    "            tool_params = self._extract_tool_parameters(user_input, tool_to_use)\n",
    "            if self.debug:\n",
    "                print(f'tool params: {tool_params}')\n",
    "            tool_result = self.tool_map[tool_to_use].execute(**tool_params)\n",
    "            if self.debug:\n",
    "                print(f'tool result: {tool_result}')\n",
    "            \n",
    "            # Use the LLM to create a response based on the tool result\n",
    "            resp = self._generate_response_with_tool_result(user_input, tool_to_use, tool_result)\n",
    "        else:\n",
    "            # Just use the LLM directly\n",
    "            resp = self.llm_connector.generate_response(\n",
    "                self._format_conversation_for_llm(), return_text_only=True\n",
    "            )\n",
    "        \n",
    "        # Add response to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": resp})\n",
    "        \n",
    "        return resp\n",
    "    \n",
    "    def _select_tool(self, user_input: str) -> Optional[str]:\n",
    "        \"\"\"Determine which tool to use based on user input.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): The user's query\n",
    "            \n",
    "        Returns:\n",
    "            Optional[str]: Name of the tool to use, or None if no tool is needed\n",
    "        \"\"\"\n",
    "        tools_descriptions=self.tool_desc_map\n",
    "        tool_choice_prompt=f\"\"\"Decide if and what tool is used to answer user input. Return \"none\" if no tool is needed.\n",
    "                                Tools are provided with descriptions. Do not change any name or description.\n",
    "                                User input: {user_input}\n",
    "                                \n",
    "                                Tools available: {tools_descriptions}\n",
    "                                \n",
    "                                Return only a tool name\"\"\"\n",
    "        resp=self.llm_connector.generate_response(tool_choice_prompt, return_text_only=True)\n",
    "        if self.debug:\n",
    "            print(f'LLM response to select tool {resp}')\n",
    "        if resp not in self.tool_desc_map:\n",
    "            return None\n",
    "        return resp\n",
    "    \n",
    "    def _extract_tool_parameters(self, user_input: str, tool_name: str) -> Dict[str, Any]:\n",
    "        \"\"\"Extract parameters for a tool from user input.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): The user's query\n",
    "            tool_name (str): The name of the tool to use\n",
    "            \n",
    "        Returns:\n",
    "            Dict[str, Any]: Parameters to pass to the tool\n",
    "        \"\"\"\n",
    "        tool_description=self.tool_desc_map[tool_name]\n",
    "        tool_params=self.tool_map[tool_name].parameters()\n",
    "        \n",
    "        tool_params_prompt=f\"\"\"Extract tool parameters from the uer input. Parameters tool expects to have are given\n",
    "                                Tools are provided with descriptions. Do not change any name or description.\n",
    "                                User input: {user_input}\n",
    "                                Tool to be used: {tool_name}\n",
    "                                Tool description: {tool_description}\n",
    "                                Tool parameters: {tool_params}\n",
    "                                \n",
    "                                Return ONLY a valid JSON object with the parameter names and their values. Example: {{\"operation\": \"add\", \"a\": 5, \"b\": 3}}. If some parameters are missing or with wrong values return text only information what is wrong or missing\"\"\"\n",
    "\n",
    "        resp=self.llm_connector.generate_response(tool_params_prompt, return_text_only=True)\n",
    "        if self.debug:\n",
    "            print(f'LLM response to selec tool {resp}')\n",
    "        try:\n",
    "            params = json.loads(resp)\n",
    "            return params\n",
    "        except json.JSONDecodeError:\n",
    "            if self.debug:\n",
    "                print(f\"Failed to parse parameters as JSON: {resp}\")\n",
    "        return {}\n",
    "    \n",
    "    def _generate_response_with_tool_result(self, user_input: str, tool_name: str, tool_result: Any) -> str:\n",
    "        \"\"\"Generate a response that incorporates the tool result.\n",
    "        \n",
    "        Args:\n",
    "            user_input (str): The user's query\n",
    "            tool_name (str): The name of the tool that was used\n",
    "            tool_result (Any): The result from the tool\n",
    "            \n",
    "        Returns:\n",
    "            str: A natural language response incorporating the tool result\n",
    "        \"\"\"\n",
    "        final_answer_prompt=f\"\"\"Generate final response to user based on user input and tool response\n",
    "                                User input: {user_input}\n",
    "                                Tool used: {tool_name}\n",
    "                                Tool result: {tool_result}\n",
    "                                \n",
    "                                Return only text with repsonse to user question\"\"\"\n",
    "\n",
    "        resp=self.llm_connector.generate_response(final_answer_prompt, return_text_only=True)\n",
    "        return resp\n",
    "    \n",
    "    def _format_conversation_for_llm(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Format the conversation history for the LLM.\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict[str, str]]: Formatted conversation history\n",
    "        \"\"\"\n",
    "        return self.conversation_history"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "agent=Agent(oai_connector, tools=[calculator_tool], debug=True)\n",
    "agent.run('What is 23 plus 56')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response to select tool calculator\n",
      "tool to use: calculator\n",
      "LLM response to selec tool {\"operation\": \"add\", \"a\": 23, \"b\": 56}\n",
      "tool params: {'operation': 'add', 'a': 23, 'b': 56}\n",
      "tool result: 79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sum of 23 and 56 is 79.'"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": null
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "agent.run('My name is John')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response to select tool none\n",
      "tool to use: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Nice to meet you, John! How can I assist you today?'"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": null
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "agent.run('What is my name?')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response to select tool none\n",
      "tool to use: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is John.'"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": null
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "agent.conversation_history"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is 23 plus 56'},\n",
       " {'role': 'assistant', 'content': 'The sum of 23 and 56 is 79.'},\n",
       " {'role': 'user', 'content': 'My name is John'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'Nice to meet you, John! How can I assist you today?'},\n",
       " {'role': 'user', 'content': 'What is my name?'},\n",
       " {'role': 'assistant', 'content': 'Your name is John.'}]"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": null
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "now what Design the Orchestrator/Planner would need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "For the Orchestrator/Planner component, you'll need to create a more sophisticated decision-making system that can handle complex tasks requiring multiple steps. Here's what this component would need:\n",
    "\n",
    "1. **Task Decomposition**:\n",
    "   - Break down complex tasks into smaller, manageable subtasks\n",
    "   - Determine dependencies between subtasks\n",
    "\n",
    "2. **Strategy Selection**:\n",
    "   - Choose between different approaches based on the task requirements\n",
    "   - Decide when to use tools vs. when to rely on the LLM's knowledge\n",
    "\n",
    "3. **Execution Monitoring**:\n",
    "   - Track progress of the overall plan\n",
    "   - Handle failures and adapt the plan as needed\n",
    "\n",
    "4. **Memory Management**:\n",
    "   - Maintain context across multiple steps\n",
    "   - Store intermediate results for later use\n",
    "\n",
    "5. **Feedback Loop**:\n",
    "   - Evaluate the effectiveness of actions\n",
    "   - Learn from successes and failures\n",
    "\n",
    "Would you like me to outline a basic structure for the Orchestrator/Planner class that incorporates these capabilities?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "from enum import Enum\n",
    "from datetime import datetime\n",
    "\n",
    "class TaskStatus(Enum):\n",
    "    PENDING = \"pending\"\n",
    "    IN_PROGRESS = \"in_progress\"\n",
    "    COMPLETED = \"completed\"\n",
    "    FAILED = \"failed\"\n",
    "\n",
    "class Task:\n",
    "    \"\"\"Represents a single task or subtask to be executed.\"\"\"\n",
    "    \n",
    "    def __init__(self, task_id: str, description: str, dependencies: List[str] = None):\n",
    "        self.task_id = task_id\n",
    "        self.description = description\n",
    "        self.dependencies = dependencies or []\n",
    "        self.status = TaskStatus.PENDING\n",
    "        self.result = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f\"Task({self.task_id}, {self.description}, {self.status.value})\"\n",
    "\n",
    "class Orchestrator:\n",
    "    \"\"\"Manages the planning and execution of complex tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent, llm_connector, debug=False):\n",
    "        self.agent = agent\n",
    "        self.llm_connector = llm_connector\n",
    "        self.tasks = {}  # Dictionary of task_id -> Task\n",
    "        self.execution_history = []\n",
    "        self.debug=debug\n",
    "        \n",
    "    def plan(self, user_query: str) -> List[Task]:\n",
    "        \"\"\"Break down a complex query into subtasks.\n",
    "        \n",
    "        Args:\n",
    "            user_query: The user's query or request\n",
    "            \n",
    "        Returns:\n",
    "            List of Task objects representing the plan\n",
    "        \"\"\"\n",
    "        \"\"\"Generate a prompt for the LLM to decompose a query into tasks.\"\"\"\n",
    "        tools=self.agent.tool_desc_map\n",
    "        self.user_query=user_query\n",
    "        prompt = f\"\"\"\n",
    "            I need to break down the following user query into a series of logical subtasks:\n",
    "            \n",
    "            USER QUERY: {user_query}\n",
    "            \n",
    "            Please analyze this query and:\n",
    "            1. Break it down into 2-6 distinct subtasks that would be needed to fully answer it. There are following tools available (use them if possbile to solve tasks): {tools}\n",
    "            2. For each subtask, specify which other subtasks (if any) must be completed before it can begin\n",
    "            3. Provide a clear, actionable description for each subtask\n",
    "            \n",
    "            Return your answer as a JSON array with this format:\n",
    "            [\n",
    "            {{\n",
    "                \"task_id\": 'task1',\n",
    "                \"description\": \"Detailed description of what this subtask should accomplish\",\n",
    "                \"dependencies\": []  // List of task_ids that must be completed before this one\n",
    "            }},\n",
    "            {{\n",
    "                \"task_id\": 'task2',\n",
    "                \"description\": \"Description of second subtask\",\n",
    "                \"dependencies\": [\"task1\"]  // This task depends on task1\n",
    "            }},\n",
    "            // Additional tasks...\n",
    "            ]\n",
    "            \n",
    "            Ensure that:\n",
    "            - Each task has a unique task_id\n",
    "            - Dependencies form a valid graph (no circular dependencies)\n",
    "            - The complete set of tasks fully addresses the user's query\n",
    "            - Tasks are specific and actionable\n",
    "            \n",
    "            RETURN ONLY THE JSON ARRAY, NO OTHER TEXT.\n",
    "            \"\"\"\n",
    "\n",
    "        resp=self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "        try:\n",
    "            tasks_json=json.loads(resp)\n",
    "            if len(tasks_json)>0:\n",
    "                for task in tasks_json:\n",
    "                    self.tasks[task['task_id']]=Task(task_id=task['task_id'],\n",
    "                                                    description=task['description'], \n",
    "                                                    dependencies=task['dependencies'])\n",
    "        except json.JSONDecodeError:\n",
    "            if self.debug:\n",
    "                print(f\"Failed to parse parameters as JSON: {resp}\")\n",
    "    \n",
    "    def execute_plan(self) -> str:\n",
    "        \"\"\"Execute all tasks in the plan in the correct order based on dependencies.\n",
    "        \n",
    "        Returns:\n",
    "            str: Final response synthesizing the results\n",
    "        \"\"\"\n",
    "        # Keep track of which tasks are completed\n",
    "        completed_tasks = set()\n",
    "        \n",
    "        # Continue until all tasks are either completed or failed\n",
    "        while len(completed_tasks) < len(self.tasks):\n",
    "            # Find tasks that are ready to execute (all dependencies satisfied)\n",
    "            ready_tasks = []\n",
    "            for task_id, task in self.tasks.items():\n",
    "                if (task.status == TaskStatus.PENDING and \n",
    "                    all(dep in completed_tasks for dep in task.dependencies)):\n",
    "                    ready_tasks.append(task)\n",
    "            \n",
    "            # If no tasks are ready but we haven't completed all tasks, we have a problem\n",
    "            if not ready_tasks and len(completed_tasks) < len(self.tasks):\n",
    "                return \"Error: Unable to complete the plan. Possible circular dependency.\"\n",
    "            \n",
    "            # Execute all ready tasks\n",
    "            for task in ready_tasks:\n",
    "                if self.debug:\n",
    "                    print(f\"Executing task: {task.task_id} - {task.description}\")\n",
    "                \n",
    "                result = self._execute_task(task)\n",
    "                \n",
    "                if task.status == TaskStatus.COMPLETED:\n",
    "                    completed_tasks.add(task.task_id)\n",
    "                elif task.status == TaskStatus.FAILED:\n",
    "                    # Handle failure - could implement retry logic here\n",
    "                    if self.debug:\n",
    "                        print(f\"Task failed: {task.task_id} - {result}\")\n",
    "                        print(f\"Adapting plan\")\n",
    "                    results_so_far={t.description: t.result for _, t in self.tasks.items() if t.status!=TaskStatus.PENDING}\n",
    "                    self._adapt_plan(results_so_far)\n",
    "        \n",
    "        # Generate final response based on completed tasks\n",
    "        return self._generate_final_response()\n",
    "\n",
    "    def _generate_final_response(self) -> str:\n",
    "        \"\"\"Generate a final response based on the results of all tasks.\n",
    "        \n",
    "        Returns:\n",
    "            str: Synthesized response for the user\n",
    "        \"\"\"\n",
    "        # Collect all task results\n",
    "        results = {task_id: task.result for task_id, task in self.tasks.items() \n",
    "                if task.status == TaskStatus.COMPLETED}\n",
    "        \n",
    "        # Create a prompt for the LLM to synthesize the results\n",
    "        prompt = f\"\"\"\n",
    "            Based on the following results from completed tasks, provide a comprehensive \n",
    "            answer to the original query.\n",
    "            Original query: {self.user_query}\n",
    "            \n",
    "            Results:\n",
    "            {json.dumps(results, indent=2)}\n",
    "            \n",
    "            Please synthesize these results into a clear, concise response that fully\n",
    "            addresses the original query.\n",
    "            \"\"\"\n",
    "        \n",
    "        # Get the final response from the LLM\n",
    "        return self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "    \n",
    "    def _is_executable(self, task: Task) -> bool:\n",
    "        \"\"\"Check if a task's dependencies are satisfied.\"\"\"\n",
    "        for dep_id in task.dependencies:\n",
    "            if dep_id not in self.tasks or self.tasks[dep_id].status != TaskStatus.COMPLETED:\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    def _execute_task(self, task: Task) -> Any:\n",
    "        \"\"\"Execute a single task and return its result.\n",
    "        \n",
    "        Args:\n",
    "            task: The task to execute\n",
    "            \n",
    "        Returns:\n",
    "            The result of the task execution\n",
    "        \"\"\"\n",
    "        # Mark task as in progress\n",
    "        task.status = TaskStatus.IN_PROGRESS\n",
    "        \n",
    "        try:\n",
    "            # Prepare context with results from dependencies\n",
    "            context = {}\n",
    "            for dep_id in task.dependencies:\n",
    "                if dep_id in self.tasks:\n",
    "                    context[dep_id] = self.tasks[dep_id].result\n",
    "            \n",
    "            # Create a prompt that includes the task description and context\n",
    "            prompt = f\"\"\"\n",
    "                Execute the following task:\n",
    "                TASK: {task.description}\n",
    "                \n",
    "                Context from previous tasks:\n",
    "                {json.dumps(context, indent=2)}\n",
    "                \n",
    "                Please execute this task and return only the result.\n",
    "                \"\"\"\n",
    "            \n",
    "            # Use the agent to execute the task\n",
    "            result = self.agent.run(prompt)\n",
    "            \n",
    "            # Store the result\n",
    "            task.result = result\n",
    "            task.status = TaskStatus.COMPLETED\n",
    "            \n",
    "            # Log the execution\n",
    "            self.execution_history.append({\n",
    "                \"task_id\": task.task_id,\n",
    "                \"description\": task.description,\n",
    "                \"result\": result,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            task.status = TaskStatus.FAILED\n",
    "            task.result = f\"Error: {str(e)}\"\n",
    "            return task.result\n",
    "    \n",
    "    def _adapt_plan(self, results_so_far: Dict[str, Any]) -> None:\n",
    "        \"\"\"Adapt the plan based on results so far.\n",
    "        \n",
    "        Args:\n",
    "            results_so_far: Dictionary mapping task_ids to their results\n",
    "        \"\"\"\n",
    "        # Create a prompt that includes the original query and results so far\n",
    "        prompt = f\"\"\"\n",
    "            I'm working on answering this query: {self.user_query}\n",
    "            \n",
    "            My original plan included these tasks:\n",
    "            {json.dumps({t_id: t.description for t_id, t in self.tasks.items()}, indent=2)}\n",
    "            \n",
    "            So far, I've completed these tasks with these results:\n",
    "            {json.dumps(results_so_far, indent=2)}\n",
    "            \n",
    "            Based on these results, should I:\n",
    "            1. Continue with the original plan\n",
    "            2. Modify the plan by adding, changing, or removing tasks\n",
    "            \n",
    "            If option 2, provide the complete new plan as a JSON array with this format:\n",
    "            [\n",
    "            {{\n",
    "                \"task_id\": \"task_id\",\n",
    "                \"description\": \"task description\",\n",
    "                \"dependencies\": []\n",
    "            }},\n",
    "            ...\n",
    "            ]\n",
    "            \n",
    "            Only include tasks that haven't been executed yet, plus any new tasks.\n",
    "            Return \"CONTINUE\" if no changes are needed, or the JSON array if changes are needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Get recommendation from LLM\n",
    "        response = self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "        \n",
    "        # If the plan needs to change\n",
    "        if response != \"CONTINUE\" and not response.strip().startswith(\"CONTINUE\"):\n",
    "            try:\n",
    "                new_tasks_json = json.loads(response)\n",
    "                \n",
    "                # Create new task objects\n",
    "                new_tasks = {}\n",
    "                for task_data in new_tasks_json:\n",
    "                    task_id = task_data[\"task_id\"]\n",
    "                    # Skip already completed tasks\n",
    "                    if task_id in self.tasks and self.tasks[task_id].status == TaskStatus.COMPLETED:\n",
    "                        continue\n",
    "                        \n",
    "                    new_tasks[task_id] = Task(\n",
    "                        task_id=task_id,\n",
    "                        description=task_data[\"description\"],\n",
    "                        dependencies=task_data[\"dependencies\"]\n",
    "                    )\n",
    "                \n",
    "                # Update the plan with new tasks\n",
    "                for task_id, task in new_tasks.items():\n",
    "                    self.tasks[task_id] = task\n",
    "                    \n",
    "                if self.debug:\n",
    "                    print(\"Plan adapted with new tasks:\", new_tasks)\n",
    "                    \n",
    "            except json.JSONDecodeError:\n",
    "                if self.debug:\n",
    "                    print(f\"Failed to parse updated plan: {response}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "what should plan be doing? ask LLM to divide input into tasks and return list of tasks and I make finally task dict?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "Yes, that's a good approach for the `plan` method. Here's what it should be doing:\n",
    "\n",
    "1. Ask the LLM to analyze the user query and break it down into logical subtasks\n",
    "2. Request the LLM to identify dependencies between these subtasks (which ones need to happen before others)\n",
    "3. Parse the LLM's response to create Task objects\n",
    "4. Organize these tasks into your task dictionary and determine execution order\n",
    "\n",
    "The LLM prompt would need to request structured output that you can parse, such as JSON with task descriptions and dependencies. You'd then convert this into your Task objects.\n",
    "\n",
    "This approach lets the LLM handle the complex reasoning about how to break down problems, while your code manages the execution flow and tracking.\n",
    "\n",
    "Would you like me to sketch out how this implementation might look in more detail?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "orchestrator=Orchestrator(agent, oai_connector, debug=True)\n",
    "orchestrator.plan('What is 2 plus 4 plus 10 plus 25')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "orchestrator.tasks"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task1': Task(task1, Identify the numbers to be added from the user's query: 2, 4, 10, 25, pending),\n",
       " 'task2': Task(task2, Use the calculator tool to add the first two numbers identified in task1, 2 and 4, pending),\n",
       " 'task3': Task(task3, Add the result from task2 with the third number identified in task1, 10, using the calculator tool, pending),\n",
       " 'task4': Task(task4, Add the result from task3 with the final number from task1, 25, using the calculator tool, pending),\n",
       " 'task5': Task(task5, Formulate the response to the user's query using the final calculated result, pending)}"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": null
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "orchestrator.execute_plan()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing task: task1 - Identify the numbers to be added from the user's query: 2, 4, 10, 25\n",
      "LLM response to select tool calculator\n",
      "tool to use: calculator\n",
      "LLM response to selec tool {\"operation\": \"add\", \"a\": 2, \"b\": 41}\n",
      "tool params: {'operation': 'add', 'a': 2, 'b': 41}\n",
      "tool result: 43\n",
      "Executing task: task2 - Use the calculator tool to add the first two numbers identified in task1, 2 and 4\n",
      "LLM response to select tool calculator\n",
      "tool to use: calculator\n",
      "LLM response to selec tool {\"operation\": \"add\", \"a\": 2, \"b\": 4}\n",
      "tool params: {'operation': 'add', 'a': 2, 'b': 4}\n",
      "tool result: 6\n",
      "Executing task: task3 - Add the result from task2 with the third number identified in task1, 10, using the calculator tool\n",
      "LLM response to select tool calculator\n",
      "tool to use: calculator\n",
      "LLM response to selec tool {\"operation\": \"add\", \"a\": 6, \"b\": 10}\n",
      "tool params: {'operation': 'add', 'a': 6, 'b': 10}\n",
      "tool result: 16\n",
      "Executing task: task4 - Add the result from task3 with the final number from task1, 25, using the calculator tool\n",
      "LLM response to select tool calculator\n",
      "tool to use: calculator\n",
      "LLM response to selec tool {\"operation\": \"add\", \"a\": 16, \"b\": 43}\n",
      "tool params: {'operation': 'add', 'a': 16, 'b': 43}\n",
      "tool result: 59\n",
      "Executing task: task5 - Formulate the response to the user's query using the final calculated result\n",
      "LLM response to select tool none\n",
      "tool to use: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The sum of the numbers 2, 4, 10, and 25 is 41.'"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": null
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "should it ask to if plan needs adjustemnt after every item or in the end?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "You have a couple of options for when to call the `_adapt_plan` method:\n",
    "\n",
    "**After each task completes**:\n",
    "- Pros: More responsive and adaptive, can correct course earlier\n",
    "- Cons: More LLM calls (potentially higher cost and latency), might make unnecessary adjustments\n",
    "\n",
    "**After key milestones**:\n",
    "- Pros: Balance between adaptability and efficiency\n",
    "- Cons: Requires defining what constitutes a \"milestone\"\n",
    "\n",
    "**Only when encountering problems**:\n",
    "- Pros: Most efficient use of LLM calls\n",
    "- Cons: Reactive rather than proactive, might miss optimization opportunities\n",
    "\n",
    "For most cases, I'd recommend a balanced approach - check after completing logical groups of tasks or when you encounter unexpected results. This gives you the benefits of adaptation without excessive LLM calls.\n",
    "\n",
    "What do you think would work best for your use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "would this code need some more classes or functionality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "Based on your current implementation, here are some additional classes or functionality you might consider adding:\n",
    "\n",
    "1. **ToolRegistry** - A centralized registry for tools:\n",
    "   - Manage tool registration and discovery\n",
    "   - Handle tool versioning\n",
    "   - Provide tool recommendation based on task requirements\n",
    "\n",
    "2. **Memory/Knowledge Management**:\n",
    "   - Long-term storage for information learned during conversations\n",
    "   - Ability to retrieve relevant past information\n",
    "   - Structured knowledge representation\n",
    "\n",
    "3. **Error Handling and Recovery**:\n",
    "   - More sophisticated error detection\n",
    "   - Graceful fallbacks when tools fail\n",
    "   - Automatic retry mechanisms\n",
    "\n",
    "4. **User Interaction Manager**:\n",
    "   - Handle clarification questions\n",
    "   - Manage user preferences\n",
    "   - Support for multi-turn interactions within tasks\n",
    "\n",
    "5. **Evaluation System**:\n",
    "   - Track success/failure of different approaches\n",
    "   - Collect feedback on responses\n",
    "   - Improve planning over time\n",
    "\n",
    "Which of these areas would be most valuable for your specific use case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "User Interaction Manager:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "For a User Interaction Manager, you'd want to focus on handling the back-and-forth communication between the user and your agent system. Here's what this component would need:\n",
    "\n",
    "1. **Clarification Management**:\n",
    "   - Detect when user input is ambiguous or incomplete\n",
    "   - Generate appropriate clarification questions\n",
    "   - Track which clarifications have been asked and resolved\n",
    "\n",
    "2. **Conversation State Tracking**:\n",
    "   - Maintain the current state of the conversation\n",
    "   - Track which tasks require user input to proceed\n",
    "   - Handle conversation context switching\n",
    "\n",
    "3. **User Preference Storage**:\n",
    "   - Remember user preferences across sessions\n",
    "   - Apply preferences to response generation\n",
    "   - Allow users to update their preferences\n",
    "\n",
    "4. **Response Formatting**:\n",
    "   - Adjust verbosity based on user preferences\n",
    "   - Support different output formats (text, structured data, etc.)\n",
    "   - Ensure consistent response style\n",
    "\n",
    "Would you like me to sketch a basic structure for this User Interaction Manager class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "Here's a basic structure for a User Interaction Manager class:\n",
    "\n",
    "```python\n",
    "from enum import Enum\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "class InteractionState(Enum):\n",
    "    WAITING_FOR_QUERY = \"waiting_for_query\"\n",
    "    PROCESSING = \"processing\"\n",
    "    WAITING_FOR_CLARIFICATION = \"waiting_for_clarification\"\n",
    "    PRESENTING_RESULTS = \"presenting_results\"\n",
    "\n",
    "class UserInteractionManager:\n",
    "    \"\"\"Manages interactions between the user and the agent system.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_connector, orchestrator, default_preferences=None):\n",
    "        self.llm_connector = llm_connector\n",
    "        self.orchestrator = orchestrator\n",
    "        self.state = InteractionState.WAITING_FOR_QUERY\n",
    "        self.current_query = None\n",
    "        self.clarification_needed = None\n",
    "        self.clarification_responses = {}\n",
    "        self.user_preferences = default_preferences or {\n",
    "            \"verbosity\": \"medium\",  # \"low\", \"medium\", \"high\"\n",
    "            \"explanation_depth\": \"medium\",  # \"low\", \"medium\", \"high\"\n",
    "            \"format_preference\": \"text\"  # \"text\", \"structured\", \"visual\"\n",
    "        }\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def process_input(self, user_input: str) -> str:\n",
    "        \"\"\"Process user input based on current state.\n",
    "        \n",
    "        Args:\n",
    "            user_input: The text input from the user\n",
    "            \n",
    "        Returns:\n",
    "            str: Response to the user\n",
    "        \"\"\"\n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Handle input based on current state\n",
    "        if self.state == InteractionState.WAITING_FOR_QUERY:\n",
    "            return self._handle_new_query(user_input)\n",
    "            \n",
    "        elif self.state == InteractionState.WAITING_FOR_CLARIFICATION:\n",
    "            return self._handle_clarification_response(user_input)\n",
    "            \n",
    "        elif self.state == InteractionState.PROCESSING:\n",
    "            return \"I'm still working on your previous request. Please wait a moment.\"\n",
    "            \n",
    "        elif self.state == InteractionState.PRESENTING_RESULTS:\n",
    "            # Check if this is a follow-up question about the results\n",
    "            if self._is_followup_question(user_input):\n",
    "                return self._handle_followup(user_input)\n",
    "            else:\n",
    "                return self._handle_new_query(user_input)\n",
    "    \n",
    "    def _handle_new_query(self, query: str) -> str:\n",
    "        \"\"\"Handle a new query from the user.\"\"\"\n",
    "        self.current_query = query\n",
    "        \n",
    "        # Check if we need clarification before proceeding\n",
    "        clarification_needed = self._check_if_clarification_needed(query)\n",
    "        \n",
    "        if clarification_needed:\n",
    "            self.state = InteractionState.WAITING_FOR_CLARIFICATION\n",
    "            self.clarification_needed = clarification_needed\n",
    "            response = self._generate_clarification_question(clarification_needed)\n",
    "        else:\n",
    "            self.state = InteractionState.PROCESSING\n",
    "            # Process the query using the orchestrator\n",
    "            self.orchestrator.plan(query)\n",
    "            response = self.orchestrator.execute_plan()\n",
    "            self.state = InteractionState.PRESENTING_RESULTS\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return response\n",
    "    \n",
    "    def _handle_clarification_response(self, response: str) -> str:\n",
    "        \"\"\"Handle user's response to a clarification question.\"\"\"\n",
    "        # Store the clarification response\n",
    "        self.clarification_responses[self.clarification_needed] = response\n",
    "        \n",
    "        # Check if we need more clarifications\n",
    "        new_clarification = self._check_if_clarification_needed(\n",
    "            self.current_query, \n",
    "            self.clarification_responses\n",
    "        )\n",
    "        \n",
    "        if new_clarification:\n",
    "            self.clarification_needed = new_clarification\n",
    "            result = self._generate_clarification_question(new_clarification)\n",
    "        else:\n",
    "            self.state = InteractionState.PROCESSING\n",
    "            # Create enhanced query with clarifications\n",
    "            enhanced_query = self._enhance_query_with_clarifications(\n",
    "                self.current_query, \n",
    "                self.clarification_responses\n",
    "            )\n",
    "            \n",
    "            # Process the enhanced query\n",
    "            self.orchestrator.plan(enhanced_query)\n",
    "            result = self.orchestrator.execute_plan()\n",
    "            self.state = InteractionState.PRESENTING_RESULTS\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "    \n",
    "    def _check_if_clarification_needed(self, query: str, \n",
    "                                      existing_clarifications=None) -> Optional[str]:\n",
    "        \"\"\"Check if the query needs clarification before we can process it.\"\"\"\n",
    "        existing_clarifications = existing_clarifications or {}\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Analyze this user query and determine if any clarification is needed:\n",
    "        \n",
    "        USER QUERY: {query}\n",
    "        \n",
    "        EXISTING CLARIFICATIONS: {existing_clarifications}\n",
    "        \n",
    "        If clarification is needed, respond with a JSON object:\n",
    "        {{\n",
    "            \"clarification_needed\": \"specific_aspect_needing_clarification\",\n",
    "            \"importance\": \"high|medium|low\"\n",
    "        }}\n",
    "        \n",
    "        If no clarification is needed, respond with:\n",
    "        {{\n",
    "            \"clarification_needed\": null\n",
    "        }}\n",
    "        \n",
    "        Consider only clarifications that are NECESSARY to answer the query.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "        \n",
    "        try:\n",
    "            result = json.loads(response)\n",
    "            return result.get(\"clarification_needed\")\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    \n",
    "    def _generate_clarification_question(self, clarification_aspect: str) -> str:\n",
    "        \"\"\"Generate a clarification question for the user.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Generate a clear, concise question to ask the user to clarify this aspect:\n",
    "        \n",
    "        ASPECT NEEDING CLARIFICATION: {clarification_aspect}\n",
    "        \n",
    "        ORIGINAL QUERY: {self.current_query}\n",
    "        \n",
    "        The question should be polite and specific to the information needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "    \n",
    "    def _enhance_query_with_clarifications(self, query: str, \n",
    "                                         clarifications: Dict[str, str]) -> str:\n",
    "        \"\"\"Enhance the original query with clarification information.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Rewrite this query incorporating the clarification information:\n",
    "        \n",
    "        ORIGINAL QUERY: {query}\n",
    "        \n",
    "        CLARIFICATIONS:\n",
    "        {json.dumps(clarifications, indent=2)}\n",
    "        \n",
    "        Produce a comprehensive, detailed query that includes all the clarified information\n",
    "        while maintaining the original intent.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "    \n",
    "    def _is_followup_question(self, query: str) -> bool:\n",
    "        \"\"\"Determine if a query is a follow-up to the previous results.\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return False\n",
    "            \n",
    "        prompt = f\"\"\"\n",
    "        Determine if this query is a follow-up related to the previous conversation:\n",
    "        \n",
    "        PREVIOUS CONVERSATION:\n",
    "        {json.dumps(self.conversation_history[-4:], indent=2)}\n",
    "        \n",
    "        CURRENT QUERY: {query}\n",
    "        \n",
    "        Return only \"yes\" if this is clearly a follow-up question related to the \n",
    "        previous results, or \"no\" if it's a new query.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "        return response.strip().lower() == \"yes\"\n",
    "    \n",
    "    def _handle_followup(self, query: str) -> str:\n",
    "        \"\"\"Handle a follow-up question about previous results.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Answer this follow-up question based on the previous conversation:\n",
    "        \n",
    "        CONVERSATION HISTORY:\n",
    "        {json.dumps(self.conversation_history[-6:], indent=2)}\n",
    "        \n",
    "        FOLLOW-UP QUESTION: {query}\n",
    "        \n",
    "        Provide a clear, direct answer that addresses the follow-up question\n",
    "        using information from the conversation history.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return response\n",
    "    \n",
    "    def update_preference(self, preference_name: str, value: Any) -> str:\n",
    "        \"\"\"Update a user preference.\n",
    "        \n",
    "        Args:\n",
    "            preference_name: The name of the preference to update\n",
    "            value: The new value for the preference\n",
    "            \n",
    "        Returns:\n",
    "            str: Confirmation message\n",
    "        \"\"\"\n",
    "        if preference_name in self.user_preferences:\n",
    "            self.user_preferences[preference_name] = value\n",
    "            return f\"Your preference for {preference_name} has been updated to {value}.\"\n",
    "        else:\n",
    "            return f\"Unknown preference: {preference_name}. Available preferences are: {list(self.user_preferences.keys())}\"\n",
    "```\n",
    "\n",
    "This class handles:\n",
    "1. Managing conversation state\n",
    "2. Requesting clarifications when needed\n",
    "3. Enhancing queries with clarification information\n",
    "4. Handling follow-up questions\n",
    "5. Storing user preferences\n",
    "\n",
    "Would you like me to explain any specific part of this implementation in more detail?"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from enum import Enum\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "class InteractionState(Enum):\n",
    "    WAITING_FOR_QUERY = \"waiting_for_query\"\n",
    "    PROCESSING = \"processing\"\n",
    "    WAITING_FOR_CLARIFICATION = \"waiting_for_clarification\"\n",
    "    PRESENTING_RESULTS = \"presenting_results\"\n",
    "\n",
    "class UserInteractionManager:\n",
    "    \"\"\"Manages interactions between the user and the agent system.\"\"\"\n",
    "    \n",
    "    def __init__(self, llm_connector, orchestrator, default_preferences=None):\n",
    "        self.llm_connector = llm_connector\n",
    "        self.orchestrator = orchestrator\n",
    "        self.state = InteractionState.WAITING_FOR_QUERY\n",
    "        self.current_query = None\n",
    "        self.clarification_needed = None\n",
    "        self.clarification_responses = {}\n",
    "        self.user_preferences = default_preferences or {\n",
    "            \"verbosity\": \"medium\",  # \"low\", \"medium\", \"high\"\n",
    "            \"explanation_depth\": \"medium\",  # \"low\", \"medium\", \"high\"\n",
    "            \"format_preference\": \"text\"  # \"text\", \"structured\", \"visual\"\n",
    "        }\n",
    "        self.conversation_history = []\n",
    "        \n",
    "    def process_input(self, user_input: str) -> str:\n",
    "        \"\"\"Process user input based on current state.\n",
    "        \n",
    "        Args:\n",
    "            user_input: The text input from the user\n",
    "            \n",
    "        Returns:\n",
    "            str: Response to the user\n",
    "        \"\"\"\n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
    "        \n",
    "        # Handle input based on current state\n",
    "        if self.state == InteractionState.WAITING_FOR_QUERY:\n",
    "            return self._handle_new_query(user_input)\n",
    "            \n",
    "        elif self.state == InteractionState.WAITING_FOR_CLARIFICATION:\n",
    "            return self._handle_clarification_response(user_input)\n",
    "            \n",
    "        elif self.state == InteractionState.PROCESSING:\n",
    "            return \"I'm still working on your previous request. Please wait a moment.\"\n",
    "            \n",
    "        elif self.state == InteractionState.PRESENTING_RESULTS:\n",
    "            # Check if this is a follow-up question about the results\n",
    "            if self._is_followup_question(user_input):\n",
    "                return self._handle_followup(user_input)\n",
    "            else:\n",
    "                return self._handle_new_query(user_input)\n",
    "    \n",
    "    def _handle_new_query(self, query: str) -> str:\n",
    "        \"\"\"Handle a new query from the user.\"\"\"\n",
    "        self.current_query = query\n",
    "        \n",
    "        # Check if we need clarification before proceeding\n",
    "        clarification_needed = self._check_if_clarification_needed(query)\n",
    "        \n",
    "        if clarification_needed:\n",
    "            self.state = InteractionState.WAITING_FOR_CLARIFICATION\n",
    "            self.clarification_needed = clarification_needed\n",
    "            response = self._generate_clarification_question(clarification_needed)\n",
    "        else:\n",
    "            self.state = InteractionState.PROCESSING\n",
    "            # Process the query using the orchestrator\n",
    "            self.orchestrator.plan(query)\n",
    "            response = self.orchestrator.execute_plan()\n",
    "            self.state = InteractionState.PRESENTING_RESULTS\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return response\n",
    "    \n",
    "    def _handle_clarification_response(self, response: str) -> str:\n",
    "        \"\"\"Handle user's response to a clarification question.\"\"\"\n",
    "        # Store the clarification response\n",
    "        self.clarification_responses[self.clarification_needed] = response\n",
    "        \n",
    "        # Check if we need more clarifications\n",
    "        new_clarification = self._check_if_clarification_needed(\n",
    "            self.current_query, \n",
    "            self.clarification_responses\n",
    "        )\n",
    "        \n",
    "        if new_clarification:\n",
    "            self.clarification_needed = new_clarification\n",
    "            result = self._generate_clarification_question(new_clarification)\n",
    "        else:\n",
    "            self.state = InteractionState.PROCESSING\n",
    "            # Create enhanced query with clarifications\n",
    "            enhanced_query = self._enhance_query_with_clarifications(\n",
    "                self.current_query, \n",
    "                self.clarification_responses\n",
    "            )\n",
    "            \n",
    "            # Process the enhanced query\n",
    "            self.orchestrator.plan(enhanced_query)\n",
    "            result = self.orchestrator.execute_plan()\n",
    "            self.state = InteractionState.PRESENTING_RESULTS\n",
    "        \n",
    "        # Add to conversation history\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": result})\n",
    "        return result\n",
    "    \n",
    "    def _check_if_clarification_needed(self, query: str, \n",
    "                                      existing_clarifications=None) -> Optional[str]:\n",
    "        \"\"\"Check if the query needs clarification before we can process it.\"\"\"\n",
    "        existing_clarifications = existing_clarifications or {}\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "        Analyze this user query and determine if any clarification is needed:\n",
    "        \n",
    "        USER QUERY: {query}\n",
    "        \n",
    "        EXISTING CLARIFICATIONS: {existing_clarifications}\n",
    "        \n",
    "        If clarification is needed, respond with a JSON object:\n",
    "        {{\n",
    "            \"clarification_needed\": \"specific_aspect_needing_clarification\",\n",
    "            \"importance\": \"high|medium|low\"\n",
    "        }}\n",
    "        \n",
    "        If no clarification is needed, respond with:\n",
    "        {{\n",
    "            \"clarification_needed\": null\n",
    "        }}\n",
    "        \n",
    "        Consider only clarifications that are NECESSARY to answer the query.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "        \n",
    "        try:\n",
    "            result = json.loads(response)\n",
    "            return result.get(\"clarification_needed\")\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    \n",
    "    def _generate_clarification_question(self, clarification_aspect: str) -> str:\n",
    "        \"\"\"Generate a clarification question for the user.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Generate a clear, concise question to ask the user to clarify this aspect:\n",
    "        \n",
    "        ASPECT NEEDING CLARIFICATION: {clarification_aspect}\n",
    "        \n",
    "        ORIGINAL QUERY: {self.current_query}\n",
    "        \n",
    "        The question should be polite and specific to the information needed.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "    \n",
    "    def _enhance_query_with_clarifications(self, query: str, \n",
    "                                         clarifications: Dict[str, str]) -> str:\n",
    "        \"\"\"Enhance the original query with clarification information.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Rewrite this query incorporating the clarification information:\n",
    "        \n",
    "        ORIGINAL QUERY: {query}\n",
    "        \n",
    "        CLARIFICATIONS:\n",
    "        {json.dumps(clarifications, indent=2)}\n",
    "        \n",
    "        Produce a comprehensive, detailed query that includes all the clarified information\n",
    "        while maintaining the original intent.\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "    \n",
    "    def _is_followup_question(self, query: str) -> bool:\n",
    "        \"\"\"Determine if a query is a follow-up to the previous results.\"\"\"\n",
    "        if not self.conversation_history:\n",
    "            return False\n",
    "            \n",
    "        prompt = f\"\"\"\n",
    "        Determine if this query is a follow-up related to the previous conversation:\n",
    "        \n",
    "        PREVIOUS CONVERSATION:\n",
    "        {json.dumps(self.conversation_history[-4:], indent=2)}\n",
    "        \n",
    "        CURRENT QUERY: {query}\n",
    "        \n",
    "        Return only \"yes\" if this is clearly a follow-up question related to the \n",
    "        previous results, or \"no\" if it's a new query.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "        return response.strip().lower() == \"yes\"\n",
    "    \n",
    "    def _handle_followup(self, query: str) -> str:\n",
    "        \"\"\"Handle a follow-up question about previous results.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Answer this follow-up question based on the previous conversation:\n",
    "        \n",
    "        CONVERSATION HISTORY:\n",
    "        {json.dumps(self.conversation_history[-6:], indent=2)}\n",
    "        \n",
    "        FOLLOW-UP QUESTION: {query}\n",
    "        \n",
    "        Provide a clear, direct answer that addresses the follow-up question\n",
    "        using information from the conversation history.\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm_connector.generate_response(prompt, return_text_only=True)\n",
    "        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
    "        return response\n",
    "    \n",
    "    def update_preference(self, preference_name: str, value: Any) -> str:\n",
    "        \"\"\"Update a user preference.\n",
    "        \n",
    "        Args:\n",
    "            preference_name: The name of the preference to update\n",
    "            value: The new value for the preference\n",
    "            \n",
    "        Returns:\n",
    "            str: Confirmation message\n",
    "        \"\"\"\n",
    "        if preference_name in self.user_preferences:\n",
    "            self.user_preferences[preference_name] = value\n",
    "            return f\"Your preference for {preference_name} has been updated to {value}.\"\n",
    "        else:\n",
    "            return f\"Unknown preference: {preference_name}. Available preferences are: {list(self.user_preferences.keys())}\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "user_interactor=UserInteractionManager(oai_connector, orchestrator)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "user_interactor.process_input('what is sum of two digits')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Could you please specify which two digits you would like me to sum up?'"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": null
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "user_interactor.process_input('what is sum of 2 plus 67')"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The sum of 2 plus 67 is 69.'"
      ]
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": null
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Prompt**\n",
    "\n",
    "what was problem here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**AI Response**\n",
    "\n",
    "It looks like there were a couple of issues with the execution:\n",
    "\n",
    "1. **Parameter Extraction Failure**: The agent couldn't properly extract the parameters needed for the calculator tool from the query \"what is sum of 2 plus 67\". It failed to parse the operation as \"add\" and the numbers as 2 and 67.\n",
    "\n",
    "2. **Error Handling Issue**: When the tool execution failed, the system tried to adapt the plan, but there seems to be a problem with the plan adaptation logic, possibly creating circular dependencies.\n",
    "\n",
    "3. **JSON Parsing Error**: The LLM returned a detailed explanation rather than the expected JSON format when asked for parameters, causing a parsing error.\n",
    "\n",
    "4. **Task State Management**: The system appears to have gotten confused about which tasks were completed and which needed to be redone.\n",
    "\n",
    "To fix these issues, you could:\n",
    "\n",
    "1. Improve the parameter extraction prompt to better handle natural language queries\n",
    "2. Add better validation for the adapted plan to prevent circular dependencies\n",
    "3. Implement more robust error recovery mechanisms\n",
    "4. Enhance the tool selection logic to better identify when to use the calculator\n",
    "\n",
    "Would you like me to suggest specific improvements for any of these areas?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}