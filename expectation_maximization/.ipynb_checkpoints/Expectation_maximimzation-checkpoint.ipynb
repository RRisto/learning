{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://stats.stackexchange.com/questions/72774/numerical-example-to-understand-expectation-maximization Suppose we have two groups - red and blue. Specifically, each group contains a value drawn from a normal distribution with the following parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "np.random.seed(110) # for reproducible random results\n",
    "\n",
    "# set parameters\n",
    "red_mean = 3\n",
    "red_std = 0.8\n",
    "\n",
    "blue_mean = 7\n",
    "blue_std = 2\n",
    "\n",
    "# draw 20 samples from normal distributions with red/blue parameters\n",
    "red = np.random.normal(red_mean, red_std, size=20)\n",
    "blue = np.random.normal(blue_mean, blue_std, size=20)\n",
    "\n",
    "both_colours = np.sort(np.concatenate((red, blue))) # for later use..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we can see the colour of each point (i.e. which group it belongs to), it's very easy to estimate the mean and standard deviation for each each group. Just pass the red and blue values to the builtin functions in NumPy for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8132116984626867"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80011196723607392"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.9725538984676376"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(blue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0388437926931493"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if we can't see the colours of the points? That is, instead of red or blue, every point just looks purple to us.\n",
    "\n",
    "To try and recover the mean and standard deviation parameters for the red and blue groups, we can use Expectation Maximisation.\n",
    "\n",
    "Our first step (step 1 above) is to guess at the parameter values for each group's mean and standard deviation. We don't have to guess intelligently, we can pick any numbers we like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# estimates for the mean\n",
    "red_mean_guess = 1.1\n",
    "blue_mean_guess = 9\n",
    "\n",
    "# estimates for the standard deviation\n",
    "red_std_guess = 2\n",
    "blue_std_guess = 1.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are bad estimates: both means look far off any kind of \"middle\" for the groups of points, for instance. We want to improve these estimates.\n",
    "\n",
    "The next step (step 2) is to compute the likelihood of each data point appearing under the current parameter guesses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "likelihood_of_red = stats.norm(red_mean_guess, red_std_guess).pdf(both_colours)\n",
    "likelihood_of_blue = stats.norm(blue_mean_guess, blue_std_guess).pdf(both_colours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have simply put each data point into the probability density function with our current guesses at the mean and standard deviation for red and blue. This tells us, for example, that with our current guesses the data point at 1.761 is much more likely to be red (0.189) than blue (0.00003)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each data point, we can turn these two likelihood values into weights (step 3) so that they sum to 1 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "likelihood_total = likelihood_of_red + likelihood_of_blue\n",
    "\n",
    "red_weight = likelihood_of_red / likelihood_total\n",
    "blue_weight = likelihood_of_blue / likelihood_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our current estimates and our newly-computed weights, we can now compute new estimates for the mean and standard deviation of the red and blue groups (step 4).\n",
    "\n",
    "We twice compute the mean and standard deviation using all data points, but with the different weightings: once for the red weights and once for the blue weights.\n",
    "\n",
    "The key bit of intuition is that the greater the weight of a colour on a data point, the more the data point influences the next estimates for that colour's parameters. This has the effect of \"pulling\" the parameters in the right direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def estimate_mean(data, weight):\n",
    "    return np.sum(data * weight) / np.sum(weight)\n",
    "\n",
    "def estimate_std(data, weight, mean):\n",
    "    variance = np.sum(weight * (data - mean)**2) / np.sum(weight)\n",
    "    return np.sqrt(variance)\n",
    "\n",
    "# new estimates for standard deviation\n",
    "blue_std_guess = estimate_std(both_colours, blue_weight, blue_mean_guess)\n",
    "red_std_guess = estimate_std(both_colours, red_weight, red_mean_guess)\n",
    "\n",
    "# new estimates for mean\n",
    "red_mean_guess = estimate_mean(both_colours, red_weight)\n",
    "blue_mean_guess = estimate_mean(both_colours, blue_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have new estimates for the parameters. To improve them again, we can jump back to step 2 and repeat the process. We do this until the estimates converge, or after some number of iterations have been performed (step 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the means are already converging on some values, and the shapes of the curves (governed by the standard deviation) are also becoming more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EM process has converged to the following values, which turn out to very close to the actual values (where we can see the colours - no hidden variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    likelihood_of_red = stats.norm(red_mean_guess, red_std_guess).pdf(both_colours)\n",
    "    likelihood_of_blue = stats.norm(blue_mean_guess, blue_std_guess).pdf(both_colours)\n",
    "    likelihood_total = likelihood_of_red + likelihood_of_blue\n",
    "\n",
    "    red_weight = likelihood_of_red / likelihood_total\n",
    "    blue_weight = likelihood_of_blue / likelihood_total\n",
    "    \n",
    "    # new estimates for standard deviation\n",
    "    blue_std_guess = estimate_std(both_colours, blue_weight, blue_mean_guess)\n",
    "    red_std_guess = estimate_std(both_colours, red_weight, red_mean_guess)\n",
    "\n",
    "    # new estimates for mean\n",
    "    red_mean_guess = estimate_mean(both_colours, red_weight)\n",
    "    blue_mean_guess = estimate_mean(both_colours, blue_weight)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.9096103216068339"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_mean_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.8381294618993449"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blue_mean_guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
