{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import collections\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_excel('data/sentiment_data/training_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>The Da Vinci Code book is just awesome.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>positive</td>\n",
       "      <td>this was the first clive cussler i've ever rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>positive</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>i liked the Da Vinci Code a lot.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>I liked the Da Vinci Code but it ultimatly did...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y                                               text\n",
       "0  positive            The Da Vinci Code book is just awesome.\n",
       "1  positive  this was the first clive cussler i've ever rea...\n",
       "2  positive                   i liked the Da Vinci Code a lot.\n",
       "3  positive                   i liked the Da Vinci Code a lot.\n",
       "4  positive  I liked the Da Vinci Code but it ultimatly did..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " class TextCleaner(object):\n",
    "        '''class to clean text'''\n",
    "        \n",
    "        def __init__(self, stemmer=None, make_lower=True, custom_chars_to_remove=None):\n",
    "            self.stemmer=stemmer\n",
    "            self.make_lower=make_lower\n",
    "            self.custom_chars_to_remove=custom_chars_to_remove\n",
    "            \n",
    "        def remove_punctutation(self, text): \n",
    "            '''remove punctuation and custom characters if set'''\n",
    "            \n",
    "            text_clean = text.translate(str.maketrans(\"\", \"\", string.punctuation))\t\t\n",
    "            if self.custom_chars_to_remove is not None:\n",
    "                text_clean = text_clean.translate(str.maketrans(\"\", \"\",self.custom_chars_to_remove))\n",
    "            return text_clean\n",
    "            \n",
    "        def tokenize_text(self, text):\n",
    "            '''tokenize text into words, if stemmer, stems. returns list of words in order in text'''\n",
    "            \n",
    "            words = word_tokenize(text)\n",
    "            if self.stemmer is not None:\n",
    "                words_list=[]\n",
    "                for w in words:\n",
    "                    words_list.append(stemmer.stem(w))\n",
    "                    return words_list\n",
    "            return words\n",
    "            \n",
    "        def preporcess_text(self, text):\n",
    "            '''generic function to process data'''\n",
    "            if self.make_lower:\n",
    "                text=text.lower()\n",
    "            text=self.remove_punctutation(text)\n",
    "            text_tokens=self.tokenize_text(text)\n",
    "            return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "    '''generic class for network building. Exact implementation is in subclasses'''\n",
    "    \n",
    "    def __init__(self,x, y, nn_structure, test_size):\n",
    "        \"\"\"initiate NN with x and y texts, also clean data and turn it into matrix\n",
    "        \"\"\"\n",
    "        \n",
    "        self.X=x #matrix\n",
    "        self.y=y #matrix\n",
    "        self.nn_structure=nn_structure #list of nr of in each layer\n",
    "    \n",
    "        #split test and trainig data\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(self.X, self.y, test_size=test_size)\n",
    "        #vectorize y\n",
    "        self.y_v_train=self.convert_y_to_vect(self.y_train)\n",
    "        self.y_v_test=self.convert_y_to_vect(self.y_test)\n",
    "        self.y_pred=None\n",
    "        \n",
    "        self.avg_cost_func=[]\n",
    "        self.W=None\n",
    "        self.b=None\n",
    "        \n",
    "                 \n",
    "    def convert_y_to_vect(self, y):\n",
    "        y_vect = np.zeros((len(y), len(self.y_dictionary)))\n",
    "        for i in range(len(y)):\n",
    "            y_vect[i, y[i]] = 1\n",
    "        return y_vect\n",
    "\n",
    "    \n",
    "    def f(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "        \n",
    "        \n",
    "    def f_deriv(self,x):\n",
    "        return self.f(x) * (1 - self.f(x))\n",
    "    \n",
    "    \n",
    "    def setup_and_init_weights(self):\n",
    "        W = {}\n",
    "        b = {}\n",
    "        for l in range(1, len(self.nn_structure)):\n",
    "            W[l] = np.random.random_sample((self.nn_structure[l], self.nn_structure[l-1]))\n",
    "            b[l] = np.random.random_sample((self.nn_structure[l],))\n",
    "        return W, b\n",
    "    \n",
    "    \n",
    "    def init_tri_values(self):\n",
    "        tri_W = {}\n",
    "        tri_b = {}\n",
    "        for l in range(1, len(self.nn_structure)):\n",
    "            tri_W[l] = np.zeros((self.nn_structure[l], self.nn_structure[l-1]))\n",
    "            tri_b[l] = np.zeros((self.nn_structure[l],))\n",
    "        return tri_W, tri_b\n",
    "    \n",
    "    \n",
    "    def feed_forward(self, x, W, b):\n",
    "        h = {1: x}\n",
    "        z = {}\n",
    "        for l in range(1, len(W) + 1):\n",
    "            # if it is the first layer, then the input into the weights is x, otherwise, \n",
    "            # it is the output from the last layer\n",
    "            if l == 1:\n",
    "                node_in = x\n",
    "            else:\n",
    "                node_in = h[l]\n",
    "            z[l+1] = W[l].dot(node_in) + b[l] # z^(l+1) = W^(l)*h^(l) + b^(l)  \n",
    "            h[l+1] = self.f(z[l+1]) # h^(l) = f(z^(l)) \n",
    "        return h, z\n",
    "    \n",
    "    \n",
    "    def calculate_out_layer_delta(self, y, h_out, z_out):\n",
    "        # delta^(nl) = -(y_i - h_i^(nl)) * f'(z_i^(nl))\n",
    "        return -(y-h_out) * self.f_deriv(z_out)\n",
    "    \n",
    "    \n",
    "    def calculate_hidden_delta(self, delta_plus_1, w_l, z_l):\n",
    "        # delta^(l) = (transpose(W^(l)) * delta^(l+1)) * f'(z^(l))\n",
    "        return np.dot(np.transpose(w_l), delta_plus_1) * self.f_deriv(z_l)\n",
    "    \n",
    "    \n",
    "    def train_nn(self):\n",
    "        '''implemented in subclass'''\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "    def plot_avg_cost_func(self):\n",
    "        if len(self.avg_cost_func)==0:\n",
    "            print('Please train model before visalizing average cost')\n",
    "            return\n",
    "        plt.plot(self.avg_cost_func)\n",
    "        plt.ylabel('Average J')\n",
    "        plt.xlabel('Iteration number')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def predict_test_data(self):\n",
    "        m = self.X_test.shape[0]\n",
    "        y = np.zeros((m,))\n",
    "        for i in range(m):\n",
    "            h, z = self.feed_forward(self.X_test[i, :], self.W, self.b)\n",
    "            y[i] = np.argmax(h[len(self.nn_structure)])\n",
    "        self.y_pred=y\n",
    "        \n",
    "        \n",
    "    def get_test_accuracy(self):\n",
    "        self.predict_test_data()\n",
    "        return accuracy_score(self.y_test, self.y_pred)*100\n",
    "    \n",
    "    \n",
    "    def pickle(self, filename):\n",
    "        '''save model to file'''\n",
    "        f = open(filename, 'wb')\n",
    "        pickle.dump(self, f, pickle.HIGHEST_PROTOCOL)\n",
    "        f.close()\n",
    "        \n",
    "        \n",
    "    @staticmethod\n",
    "    def unpickle(filename):\n",
    "        '''read model from file'''\n",
    "        with open(filename, 'rb') as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NetworkMBGD(Network):\n",
    "    '''class for minibatch gradient descent'''\n",
    "    \n",
    "    def __init__(self,x, y, nn_structure, test_size):\n",
    "        Network.__init__(self,  x, y, nn_structure, test_size)\n",
    "        self.lamb=None\n",
    "        self.alpha=None\n",
    "        self.iter_num=None\n",
    "        \n",
    "        \n",
    "    def get_mini_batches(self, X, y, batch_size):\n",
    "        random_idxs = np.random.choice(len(y), len(y), replace=False)\n",
    "        X_shuffled = self.X_train[random_idxs,:]\n",
    "        y_shuffled = self.y_v_train[random_idxs]\n",
    "        mini_batches = [(X_shuffled[i:i+batch_size,:], y_shuffled[i:i+batch_size]) for\n",
    "                       i in range(0, len(self.y_v_train), batch_size)]\n",
    "        return mini_batches\n",
    "    \n",
    "    \n",
    "    def train_nn(self, iter_num=3000, bs=100, alpha=0.25, lamb=0.000):\n",
    "        self.lamb=lamb\n",
    "        self.alpha=alpha\n",
    "        self.iter_num=iter_num\n",
    "        #reset avg cost\n",
    "        if len(self.avg_cost_func)>0:\n",
    "            self.avg_cost_func=[]\n",
    "        \n",
    "        W, b = self.setup_and_init_weights()\n",
    "        cnt = 0\n",
    "        m = len(self.y_v_train)\n",
    "        print('Starting gradient descent for {} iterations'.format(iter_num))\n",
    "        while cnt < iter_num:\n",
    "            if cnt%1000 == 0:\n",
    "                print('Iteration {} of {}'.format(cnt, iter_num))\n",
    "            tri_W, tri_b = self.init_tri_values()\n",
    "            avg_cost = 0\n",
    "            mini_batches = self.get_mini_batches(self.X_train, self.y_v_train, bs)\n",
    "            for mb in mini_batches:\n",
    "                X_mb = mb[0]\n",
    "                y_mb = mb[1]\n",
    "                # pdb.set_trace()\n",
    "                for i in range(len(y_mb)):\n",
    "                    delta = {}\n",
    "                    # perform the feed forward pass and return the stored h and z values, \n",
    "                    # to be used in the gradient descent step\n",
    "                    h, z = self.feed_forward(X_mb[i, :], W, b)\n",
    "                    # loop from nl-1 to 1 backpropagating the errors\n",
    "                    for l in range(len(self.nn_structure), 0, -1):\n",
    "                        if l == len(self.nn_structure):\n",
    "                            delta[l] = self.calculate_out_layer_delta(y_mb[i,:], h[l], z[l])\n",
    "                            avg_cost += np.linalg.norm((y_mb[i,:]-h[l]))\n",
    "                        else:\n",
    "                            if l > 1:\n",
    "                                delta[l] = self.calculate_hidden_delta(delta[l+1], W[l], z[l])\n",
    "                            # triW^(l) = triW^(l) + delta^(l+1) * transpose(h^(l))\n",
    "                            tri_W[l] += np.dot(delta[l+1][:,np.newaxis], \n",
    "                                              np.transpose(h[l][:,np.newaxis])) \n",
    "                            # trib^(l) = trib^(l) + delta^(l+1)\n",
    "                            tri_b[l] += delta[l+1]\n",
    "                # perform the gradient descent step for the weights in each layer\n",
    "                for l in range(len(self.nn_structure) - 1, 0, -1):\n",
    "                    W[l] += -alpha * (1.0/bs * tri_W[l] + lamb * W[l])\n",
    "                    b[l] += -alpha * (1.0/bs * tri_b[l])\n",
    "            # complete the average cost calculation\n",
    "            avg_cost = 1.0/m * avg_cost\n",
    "            self.avg_cost_func.append(avg_cost)\n",
    "            cnt += 1\n",
    "        self.W=W\n",
    "        self.b=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextMBGD(NetworkMBGD):\n",
    "    '''wrapper class for NetworkMBGD to make textual data edible for it'''\n",
    "    \n",
    "    def __init__(self,x_texts, y_texts, nn_structure, test_size, n_words=10000, custom_chars_to_remove=None):\n",
    "            \n",
    "        self.x_texts=x_texts\n",
    "        self.y_texts=y_texts  \n",
    "        \n",
    "        #cleaning stuff\n",
    "        self.n_words=n_words\n",
    "        self.custom_chars_to_remove=custom_chars_to_remove\n",
    "        self.cleaner=TextCleaner(self.custom_chars_to_remove)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        #init X dataset\n",
    "        self.X_count=None\n",
    "        self.X_dictionary=None\n",
    "        self.X_reversed_dictionary=None\n",
    "        self.build_x_dataset()\n",
    "        \n",
    "        #init y dataset\n",
    "        self.y_dictionary=None\n",
    "        self.y_reversed_dictionary=None\n",
    "        self.build_y_dataset()\n",
    "        \n",
    "        NetworkMBGD.__init__(self, self.build_x_dataset(), self.build_y_dataset(), nn_structure, test_size)\n",
    "        \n",
    "        \n",
    "    def build_x_dataset(self):\n",
    "        '''build dataset for neural network. turns texts into matrix\n",
    "        INPUT: \n",
    "            - n_words: nr of top sequence words kept, others words are makerd as unknown\n",
    "            - texts_list_raw: list of texts to be build a dataset. each list element is 1 row\n",
    "        '''\n",
    "        \n",
    "        texts_list=[]\n",
    "        texts_tokenized=[]\n",
    "        words=[]\n",
    "        max_sentence_len=0\n",
    "\n",
    "        for sentence in self.x_texts:\n",
    "            text_words=self.cleaner.preporcess_text(sentence)\n",
    "            texts_tokenized.append(text_words)\n",
    "            words.extend(text_words)\n",
    "            if len(text_words)>max_sentence_len:\n",
    "                max_sentence_len=len(text_words)\n",
    "\n",
    "        #set normalised sentence len, other ideas?\n",
    "        normalized_sentence_len=max_sentence_len\n",
    "        #unknonown words count\n",
    "        count = [['UNK', -1]]\n",
    "\n",
    "        count.extend(collections.Counter(words).most_common(self.n_words - 1))\n",
    "        dictionary = dict()\n",
    "        for word, _ in count:\n",
    "            dictionary[word] = len(dictionary)\n",
    "        data = list()\n",
    "        unk_count = 0\n",
    "        for text in texts_tokenized:\n",
    "            text_data=[]\n",
    "            for word in text:\n",
    "                if word in dictionary:\n",
    "                    index = dictionary[word]\n",
    "                else:\n",
    "                    index = 0  # dictionary['UNK']\n",
    "                    unk_count += 1\n",
    "                text_data.append(index)\n",
    "            if len(text_data)<normalized_sentence_len:\n",
    "                text_data.extend([0] * (normalized_sentence_len-len(text_data)))\n",
    "\n",
    "            data.append(text_data)\n",
    "\n",
    "        count[0][1] = unk_count\n",
    "        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        \n",
    "        self.X_count=count\n",
    "        self.X_dictionary=dictionary\n",
    "        self.X_reversed_dictionary=reversed_dictionary\n",
    "        return np.array(self.scaler.fit_transform(data))\n",
    "        \n",
    "        \n",
    "    def build_y_dataset(self):\n",
    "        '''build y dataset, dataset_y will be one hot vector. turns text into matrix'''\n",
    "        dictionary={}\n",
    "        cnt=0\n",
    "        for word in set(self.y_texts):\n",
    "            dictionary[word]=cnt\n",
    "            cnt+=1\n",
    "\n",
    "        dataset=[]\n",
    "        for category in self.y_texts:\n",
    "            dataset.append([dictionary[category]])\n",
    "        reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "        \n",
    "        self.y_dictionary=dictionary\n",
    "        self.y_reversed_dictionary=reversed_dictionary\n",
    "        return np.array(dataset)\n",
    "        \n",
    "        \n",
    "    def text_to_vector(self, text):\n",
    "        '''turns text to vector based on input data dictionary, for random text vectorization'''\n",
    "        #make sep function for tokenization\n",
    "        texts_tokenized=[]\n",
    "        text_vector=[]\n",
    "        text_words=self.cleaner.preporcess_text(text)\n",
    "        texts_tokenized.extend(text_words)\n",
    "\n",
    "        for token in texts_tokenized:\n",
    "            if token in self.X_dictionary:\n",
    "                text_vector.extend([self.X_dictionary[token]])\n",
    "            else:\n",
    "                text_vector.extend([0])\n",
    "\n",
    "        return text_vector\n",
    "    \n",
    "    \n",
    "    def predict_text_label(self, text):\n",
    "        '''predicts label of text based on model'''\n",
    "        text_vector=self.text_to_vector(text)\n",
    "        #normalize lenght\n",
    "        if len(text_vector)<self.X_test.shape[1]:\n",
    "            text_vector.extend([0] * (self.X_test.shape[1]-len(text_vector)))\n",
    "\n",
    "        h, z = self.feed_forward(text_vector, self.W, self.b)\n",
    "        y = np.argmax(h[len(self.nn_structure)])\n",
    "        y_label=self.y_reversed_dictionary[y]\n",
    "        return y_label\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_MBGD=TextMBGD(df['text'].tolist(), df['y'].tolist(),[40, 30, 2], 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent for 120 iterations\n",
      "Iteration 0 of 120\n",
      "CPU times: user 41.7 s, sys: 0 ns, total: 41.7 s\n",
      "Wall time: 44.4 s\n"
     ]
    }
   ],
   "source": [
    "% time nn_MBGD.train_nn(iter_num=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f93583ad588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nn_MBGD.plot_avg_cost_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.160493827160494"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_MBGD.get_test_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
