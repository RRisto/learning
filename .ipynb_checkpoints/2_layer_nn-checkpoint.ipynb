{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-layer NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be making 2-layer NN: input and output layer, no hidden layer. Simple thing to startu but good to gain general understanding.\n",
    "\n",
    "First let's mark our input layer and output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's mark our output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([[0,0,1,1]]).T\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal of NN is to correctly guess values in output layer based on input layer values. To do that we in this example need to transfer input layer values through synapses to output layer. Places where transformation takes place is in acivation function which uses weights from synapses. Nice trick is to initially set those wieghts randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize weights randomly with mean 0\n",
    "syn0 = 2*np.random.random((3,1)) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can start passing input values to output layer. We will be using sigmoid activation function because of it's simplicity and but same time has nonlinear nature. Activation function is just a function which takes all the input to the node and transforms output out of it (it could be very simple: if input sum > 2: return 1, else: 0). \n",
    "\n",
    "We'll be using sigmoid function, which has following form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "Sigmoid = \\frac {1}{1+e ^{-net_i}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "net_i = \\frac {weight_i}{input_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "net refers to net input to layer. In our case we could calculate net input to output layer as following (just dot product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.40839638],\n",
       "       [ 0.2267457 ],\n",
       "       [-0.84471075],\n",
       "       [-0.20956867]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(X,syn0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could see how outut changes when we change values of net input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to calculate sigmoid output\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x7efe9095ecf8>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0kxJQUREmikpiIhIs/8P1eF+CvTXJ0gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efe9094dda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sigmoid=pd.DataFrame.from_dict({'x':[x for x in range(-10,10)]})\n",
    "df_sigmoid=df_sigmoid.set_index(df_sigmoid.x)\n",
    "df_sigmoid['sig']=sigmoid(df_sigmoid.x)\n",
    "plt_sig=df_sigmoid.sig.plot(title='Sigmoid function input vs output')\n",
    "plt_sig.set_xlabel('input')\n",
    "plt_sig.set_ylabel('output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot sigmoid function output is between 0 and 1. Also we see from the plot that output has most steepest growth between -2.5 and 2.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our matrix we could calculate our model output similarily:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.3992967 ],\n",
       "       [ 0.55644479],\n",
       "       [ 0.30054357],\n",
       "       [ 0.44779874]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output=sigmoid(np.dot(X, syn0))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have sigmoid output, we could calculate error. Let's calculate error, which could be done using formula:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "error_i = target_i-output_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3992967 ],\n",
       "       [-0.55644479],\n",
       "       [ 0.69945643],\n",
       "       [ 0.55220126]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error=y-output\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total error can be calulated by this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "error= \\sum \\frac 12 (target_i-output_i)^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to reduce error. To do that we need to change the weights. But how much? We could do it as a guess (change in weights in some way an observe if error is reducing). But of course it is not very efficient (especially with large NNs). This is a part where (partial) derivative comes in hand. If we take derivative of output layer, we'll get to know the slope of each point. \n",
    "\n",
    "What we need is to calculate following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac {\\partial error}{\\partial w_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It states we need to find each weights share in total error. Previous formula could be calculated like this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac {\\partial error}{\\partial w_i} = \\frac {\\partial error} {\\partial output_i} * \\frac {\\partial output_i} {\\partial net_i} * \\frac {\\partial net_i} {\\partial w_i}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find each component. First partial derivative of error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac {\\partial error} {\\partial output_i} = target_i -output_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then second component:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac {\\partial out_i} {\\partial net_i} = output_i * (1-output_i)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is derivative of sigmoid function which is not covered in this page, google it and find out how it works.\n",
    "\n",
    "And now third component:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac {\\partial net_i} {\\partial w_i} = input_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plug it all back in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "\\frac {\\partial error}{\\partial w_i} =  (target_i-output_i) * output_i*(1-output_i) * input_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we'll calculate:\n",
    " - size of the error (target if i - output of i)\n",
    " - input (input of i)\n",
    " - derivative sigmoid function. If slope is steep (see plot about sigmoid derivative), we want to adjust weights more because we are not very sure, but if we are more confident (for larger numbers sigmoid function has shallow gradient). Derivative achieves that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In python this look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of sigmoid function\n",
    "def sigmoid_der(x):\n",
    "    return x*(1-x)\n",
    "\n",
    "output_error = y - output\n",
    "\n",
    "# multiply how much we missed by the \n",
    "# slope of the sigmoid at the values in l1\n",
    "output_delta = output_error * sigmoid_der(l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn0 += np.dot(l0.T,output_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compact code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.16595599]\n",
      " [ 0.44064899]\n",
      " [-0.99977125]]\n",
      "Error:0.517208275438\n",
      "Error:0.00795484506673\n",
      "Error:0.0055978239634\n",
      "Error:0.00456086918013\n",
      "Error:0.00394482243339\n",
      "Error:0.00352530883742\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork():\n",
    "    def __init__(self):\n",
    "        # Seed to repeat results\n",
    "        np.random.seed(1)\n",
    "        #assign synaptic weights randomly with mean of 0\n",
    "        self.synaptic_weights = 2 * np.random.random((3, 1)) - 1\n",
    "\n",
    "    # The Sigmoid function, which describes an S shaped curve.\n",
    "    # We pass the weighted sum of the inputs through this function to\n",
    "    # normalise them between 0 and 1.\n",
    "    def __sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    # The derivative of the Sigmoid function.\n",
    "    # This is the gradient of the Sigmoid curve.\n",
    "    # It indicates how confident we are about the existing weight.\n",
    "    def __sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "\n",
    "    # We train the neural network through a process of trial and error.\n",
    "    # Adjusting the synaptic weights each time.\n",
    "    def train(self, training_set_inputs, training_set_outputs, number_of_training_iterations):\n",
    "        for iteration in range(number_of_training_iterations):\n",
    "            # Pass the training set through our neural network (a single neuron).\n",
    "            output = self.think(training_set_inputs)\n",
    "\n",
    "            # Calculate the error (The difference between the desired output\n",
    "            # and the predicted output).\n",
    "            error = training_set_outputs - output\n",
    "            \n",
    "            #print error\n",
    "            if (iteration% 10000) == 0:\n",
    "                print(\"Error:\" + str(np.mean(np.abs(error))))\n",
    "\n",
    "            # Multiply the error by the input and again by the gradient of the Sigmoid curve.\n",
    "            # This means less confident weights are adjusted more.\n",
    "            # This means inputs, which are zero, do not cause changes to the weights.\n",
    "            adjustment = np.dot(training_set_inputs.T, error * self.__sigmoid_derivative(output))\n",
    "\n",
    "            # Adjust the weights.\n",
    "            self.synaptic_weights += adjustment\n",
    "\n",
    "    # The neural network thinks.\n",
    "    def think(self, inputs):\n",
    "        # Pass inputs through our neural network (our single neuron).\n",
    "        return self.__sigmoid(np.dot(inputs, self.synaptic_weights))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #Intialise a single neuron neural network.\n",
    "    neural_network = NeuralNetwork()\n",
    "\n",
    "#     print(\"Random starting synaptic weights: \")\n",
    "#     print( neural_network.synaptic_weights)\n",
    "\n",
    "    # The training set. We have 4 examples, each consisting of 3 input values\n",
    "    # and 1 output value.\n",
    "    training_set_inputs = np.array([  [0,0,1],\n",
    "                [0,1,1],\n",
    "                [1,0,1],\n",
    "                [1,1,1] ])\n",
    "    training_set_outputs =  np.array([[0,0,1,1]]).T\n",
    "\n",
    "    # Train the neural network using a training set.\n",
    "    # Do it 10,000 times and make small adjustments each time.\n",
    "    neural_network.train(training_set_inputs, training_set_outputs, 60000)\n",
    "\n",
    "#     print(\"New synaptic weights after training: \")\n",
    "#     print(neural_network.synaptic_weights)\n",
    "\n",
    "#     # Test the neural network with a new situation.\n",
    "#     print(\"Considering new situation [1, 0, 0] -> ?: \")\n",
    "#     print(neural_network.think(np.array([1, 0, 0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
