{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Mixture Model with Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "from collections import namedtuple, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we receive some data that looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv(\"clusters.csv\", header=None)[1]\n",
    "_=data.hist(bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.002681\n",
       "1   -0.341619\n",
       "2   -0.352397\n",
       "3    0.682440\n",
       "4    0.504524\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995   -0.235853\n",
       "996    0.480309\n",
       "997    0.001624\n",
       "998   -0.629820\n",
       "999    0.637904\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1000.000000\n",
       "mean        0.169591\n",
       "std         0.455007\n",
       "min        -0.690683\n",
       "25%        -0.303794\n",
       "50%         0.176051\n",
       "75%         0.596546\n",
       "max         0.871537\n",
       "Name: 1, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that these data exist in three separate clusters. We want to develop a method for finding these _latent_ clusters. One way to start developing a method is to attempt to describe the process that may have generated these data.\n",
    "\n",
    "For simplicity and sanity, let's assume that each data point is generated independently of the other. Moreover, we will assume that within each cluster, the data points are identically distributed. In this case, we will assume each cluster is normally distributed and that each cluster has the same variance, $\\sigma^2$.\n",
    "\n",
    "Given these assumptions, our data could have been generated by the following process. For each data point, randomly select 1 of 3 clusters from the distribution $\\text{Discrete}(\\pi_1, \\pi_2, \\pi_3)$. Each cluster $k$ corresponds to a parameter $\\theta_k$ for that cluster, sample a data point from $\\mathcal{N}(\\theta_k, \\sigma^2)$.\n",
    "\n",
    "Equivalently, we could consider these data to be generated from a probability distribution with this probability density function:\n",
    "\n",
    "$$\n",
    "p(x_i \\,|\\, \\pi, \\theta_1, \\theta_2, \\theta_3, \\sigma)=\n",
    "    \\sum_{k=1}^3 \\pi_k\\cdot\n",
    "        \\frac{1}{\\sigma\\sqrt{2\\pi}}\n",
    "        \\text{exp}\\left\\{\n",
    "            \\frac{-(x_i-\\theta_k)^2}{2\\sigma^2}\n",
    "         \\right\\}\n",
    "$$\n",
    "\n",
    "where $\\pi$ is a 3-dimensional vector giving the _mixing proportions_. In other words, $\\pi_k$ describes the proportion of points that occur in cluster $k$.\n",
    "\n",
    "\n",
    "That is, _the probability distribution describing $x$ is a linear combination of normal distributions_.\n",
    "\n",
    "We want to use this _generative_ model to formulate an algorithm for determining the particular parameters that generated the dataset above. The $\\pi$ vector is unknown to us, as is each cluster mean $\\theta_k$. \n",
    "\n",
    "We would also like to know $z_i\\in\\{1, 2, 3\\}$, the latent cluster for each point. It turns out that introducing $z_i$ into our model will help us solve for the other values.\n",
    "\n",
    "The joint distribution of our observed data (`data`) along with the assignment variables is given by:\n",
    "\n",
    "\\begin{align}\n",
    "p(\\mathbf{x}, \\mathbf{z} \\,|\\, \\pi, \\theta_1, \\theta_2, \\theta_3, \\sigma)&=\n",
    "       p(\\mathbf{z} \\,|\\, \\pi)\n",
    "       p(\\mathbf{x} \\,|\\, \\mathbf{z}, \\theta_1, \\theta_2, \\theta_3, \\sigma)\\\\\n",
    "       &= \\prod_{i=1}^N p(z_i \\,|\\, \\pi)\n",
    "           \\prod_{i=1}^N p(x_i \\,|\\, z_i, \\theta_1, \\theta_2, \\theta_3, \\sigma) \\\\\n",
    "       &= \\prod_{i=1}^N \\pi_{z_i}\n",
    "           \\prod_{i=1}^N \n",
    "           \\frac{1}{\\sigma\\sqrt{2\\pi}}\n",
    "        \\text{exp}\\left\\{\n",
    "            \\frac{-(x_i-\\theta_{z_i})^2}{2\\sigma^2}\n",
    "         \\right\\}\\\\\n",
    "       &= \\prod_{i=1}^N \n",
    "           \\left(\n",
    "           \\pi_{z_i}\n",
    "           \\frac{1}{\\sigma\\sqrt{2\\pi}}\n",
    "        \\text{exp}\\left\\{\n",
    "            \\frac{-(x_i-\\theta_{z_i})^2}{2\\sigma^2}\n",
    "         \\right\\}\n",
    "         \\right)\\\\\n",
    "       &=\n",
    "           \\prod_i^n\n",
    "           \\prod_k^K\n",
    "           \\left(\n",
    "               \\pi_k \n",
    "               \\frac{1}{\\sigma\\sqrt{2\\pi}}\n",
    "        \\text{exp}\\left\\{\n",
    "            \\frac{-(x_i-\\theta_k)^2}{2\\sigma^2}\n",
    "         \\right\\}\n",
    "           \\right)^{\\delta(z_i, k)}\n",
    "\\end{align}\n",
    "\n",
    "### Keeping Everything Straight\n",
    "\n",
    "Before moving on, we need to devise a way to keep all our data and parameters straight. Following ideas suggested by [Keith Bonawitz](http://people.csail.mit.edu/bonawitz/Composable%20Probabilistic%20Inference%20with%20Blaise%20-%20Keith%20Bonawitz%20PhD%20Thesis.pdf), let's define a \"state\" object to store all of this data. \n",
    "\n",
    "It won't yet be clear why we are defining some components of `state`, however we will use each part eventually! As an attempt at clarity, I am using a trailing underscore in the names of members that are fixed. We will update the other parameters as we try to fit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SuffStat = namedtuple('SuffStat', 'theta N')\n",
    "\n",
    "def update_suffstats(state):\n",
    "    for cluster_id, N in Counter(state['assignment']).items():\n",
    "        points_in_cluster = [x \n",
    "            for x, cid in zip(state['data_'], state['assignment'])\n",
    "            if cid == cluster_id\n",
    "        ]\n",
    "        mean = np.array(points_in_cluster).mean()\n",
    "        \n",
    "        state['suffstats'][cluster_id] = SuffStat(mean, N)\n",
    "\n",
    "def initial_state():\n",
    "    num_clusters = 3\n",
    "    alpha = 1.0\n",
    "    cluster_ids = range(num_clusters)\n",
    "\n",
    "    state = {\n",
    "        'cluster_ids_': cluster_ids,\n",
    "        'data_': data,\n",
    "        'num_clusters_': num_clusters,\n",
    "        'cluster_variance_': .01,\n",
    "        'alpha_': alpha,\n",
    "        'hyperparameters_': {\n",
    "            \"mean\": 0,\n",
    "            \"variance\": 1,\n",
    "        },\n",
    "        'suffstats': [None, None, None],\n",
    "        'assignment': [random.choice(cluster_ids) for _ in data],\n",
    "        'pi': [alpha / num_clusters for _ in cluster_ids],\n",
    "        'cluster_means': [-1, 0, 1]\n",
    "    }\n",
    "    update_suffstats(state)\n",
    "    return state\n",
    "\n",
    "state = initial_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_\n",
      "assignment\n",
      "cluster_ids_\n",
      "cluster_means\n",
      "cluster_variance_\n",
      "data_\n",
      "hyperparameters_\n",
      "num_clusters_\n",
      "pi\n",
      "suffstats\n"
     ]
    }
   ],
   "source": [
    "for k, v in sorted(state.items()):\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling\n",
    "\n",
    "The [theory of Gibbs sampling](https://en.wikipedia.org/wiki/Gibbs_sampling) tells us that given some data $\\bf y$ and a probability distribution $p$ parameterized by $\\gamma_1, \\ldots, \\gamma_d$, we can successively draw samples from the distribution by sampling from\n",
    "\n",
    "$$\\gamma_j^{(t)}\\sim p(\\gamma_j \\,|\\, \\gamma_{\\neg j}^{(t-1)})$$\n",
    "    \n",
    "where $\\gamma_{\\neg j}^{(t-1)}$ is all current values of $\\gamma_i$ except for $\\gamma_j$. If we sample long enough, these $\\gamma_j$ values will be random samples from $p$. \n",
    "\n",
    "In deriving a Gibbs sampler, it is often helpful to observe that \n",
    "\n",
    "$$\n",
    "    p(\\gamma_j \\,|\\, \\gamma_{\\neg j})\n",
    "        = \\frac{\n",
    "            p(\\gamma_1,\\ldots,\\gamma_d)\n",
    "        }{\n",
    "            p(\\gamma_{\\neg j})\n",
    "        } \\propto p(\\gamma_1,\\ldots,\\gamma_d).\n",
    "$$\n",
    "\n",
    "The conditional distribution is proportional to the joint distribution. We will get a lot of mileage from this simple observation by dropping constant terms from the joint distribution (relative to the parameters we are conditioned on).\n",
    "\n",
    "The $\\gamma$ values in our model are each of the $\\theta_k$ values, the $z_i$ values, and the $\\pi_k$ values. Thus, we need to derive the conditional distributions for each of these.\n",
    "\n",
    "Many derivation of Gibbs samplers that I have seen rely on a lot of handwaving and casual appeals to conjugacy. I have tried to add more mathematical details here. I would gladly accept feedback on how to more clearly present the derivations! I have also tried to make the derivations more concrete by immediately providing code to do the computations in this specific case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Distribution of Assignment\n",
    "\n",
    "For berevity, we will use\n",
    "\n",
    "$$\n",
    "p(z_i=k \\,|\\, \\cdot)=\n",
    "p(z_i=k \\,|\\, \n",
    "        z_{\\neg i}, \\pi,\n",
    "        \\theta_1, \\theta_2, \\theta_3, \\sigma, \\bf x\n",
    "        ).\n",
    "        $$\n",
    "        \n",
    "Because cluster assignements are conditionally independent given the cluster weights and paramters,\n",
    "\n",
    "\\begin{align}\n",
    "    p(z_i=k \\,|\\, \\cdot) \n",
    "        &\\propto\n",
    "           \\prod_i^n\n",
    "           \\prod_k^K\n",
    "           \\left(\n",
    "               \\pi_k \n",
    "               \\frac{1}{\\sigma\\sqrt{2\\pi}}\n",
    "        \\text{exp}\\left\\{\n",
    "            \\frac{-(x_i-\\theta_k)^2}{2\\sigma^2}\n",
    "         \\right\\}\n",
    "           \\right)^{\\delta(z_i, k)} \\\\\n",
    "        &\\propto\n",
    "            \\pi_k \\cdot\n",
    "            \\frac{1}{\\sigma\\sqrt{2\\pi}} \n",
    "            \\text{exp}\\left\\{\n",
    "                \\frac{-(x_i-\\theta_k)^2}{2\\sigma^2}\n",
    "             \\right\\}\n",
    "\\end{align}\n",
    "\n",
    "This equation intuitively makes sense: point $i$ is more likely to be in cluster $k$ if $k$ is itself probable ($\\pi_k\\gg 0$) and $x_i$ is close to the mean of the cluster $\\theta_k$.\n",
    "\n",
    "For each data point $i$, we can compute $p(z_i=k \\,|\\, \\cdot)$ for each of cluster $k$. These values are the unnormalized parameters to a discrete distribution from which we can sample assignments.\n",
    "\n",
    "Below, we define functions for doing this sampling. `sample_assignment` will generate a sample from the posterior assignment distribution for the specified data point. `update_assignment` will sample from the posterior assignment for each data point and update the `state` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_assignment_score(data_id, cluster_id, state):\n",
    "    \"\"\"log p(z_i=k \\,|\\, \\cdot) \n",
    "    \n",
    "    We compute these scores in log space for numerical stability.\n",
    "    \"\"\"\n",
    "    x = state['data_'][data_id]\n",
    "    theta = state['cluster_means'][cluster_id]\n",
    "    var = state['cluster_variance_']\n",
    "    log_pi = np.log(state['pi'][cluster_id])\n",
    "    return log_pi + stats.norm.logpdf(x, theta, var)\n",
    "\n",
    "\n",
    "def assigment_probs(data_id, state):\n",
    "    \"\"\"p(z_i=cid \\,|\\, \\cdot) for cid in cluster_ids\n",
    "    \"\"\"\n",
    "    scores = [log_assignment_score(data_id, cid, state) for cid in state['cluster_ids_']]\n",
    "\n",
    "    scores = np.exp(np.array(scores))\n",
    "    return scores / scores.sum()\n",
    "\n",
    "\n",
    "def sample_assignment(data_id, state):\n",
    "    \"\"\"Sample cluster assignment for data_id given current state\n",
    "    \n",
    "    cf Step 1 of Algorithm 2.1 in Sudderth 2006 http://cs.brown.edu/~sudderth/papers/sudderthPhD.pdf\n",
    "    \"\"\"\n",
    "    try:\n",
    "        p = assigment_probs(data_id, state)\n",
    "        return np.random.choice(state['cluster_ids_'], p=p)\n",
    "    except:\n",
    "        #hack, sometimes all p-s are zero, then return cluster 0\n",
    "        return 0\n",
    "\n",
    "\n",
    "def update_assignment(state):\n",
    "    \"\"\"Update cluster assignment for each data point given current state \n",
    "    \n",
    "    cf Step 1 of Algorithm 2.1 in Sudderth 2006\n",
    "    \"\"\"\n",
    "    for data_id, x in enumerate(state['data_']):\n",
    "        state['assignment'][data_id] = sample_assignment(data_id, state)\n",
    "    update_suffstats(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Distribution of Mixture Weights\n",
    "\n",
    "We can similarly derive the conditional distributions of mixture weights by an application of Bayes theorem. Instead of updating each component of $\\pi$ separately, we update them together (this is called blocked Gibbs).\n",
    "\n",
    "\\begin{align}\n",
    "p(\\pi \\,|\\, \\cdot)&=\n",
    "p(\\pi \\,|\\, \n",
    "        \\bf{z}, \n",
    "        \\theta_1, \\theta_2, \\theta_3,\n",
    "        \\sigma, \\mathbf{x}, \\alpha\n",
    "        )\\\\\n",
    "&\\propto\n",
    "p(\\pi \\,|\\, \n",
    "        \\mathbf{x}, \n",
    "        \\theta_1, \\theta_2, \\theta_3,\n",
    "        \\sigma, \\alpha\n",
    "        )\n",
    "p(\\bf{z}\\ \\,|\\, \n",
    "        \\mathbf{x}, \n",
    "        \\theta_1, \\theta_2, \\theta_3,\n",
    "        \\sigma, \\pi, \\alpha\n",
    "        )\\\\\n",
    "&=\n",
    "p(\\pi \\,|\\, \n",
    "        \\alpha\n",
    "        )\n",
    "p(\\bf{z}\\ \\,|\\, \n",
    "        \\mathbf{x}, \n",
    "        \\theta_1, \\theta_2, \\theta_3,\n",
    "        \\sigma, \\pi, \\alpha\n",
    "        )\\\\\n",
    "&=\n",
    "\\prod_{i=1}^K \\pi_k^{\\alpha/K - 1}\n",
    "\\prod_{i=1}^K \\pi_k^{\\sum_{i=1}^N \\delta(z_i, k)} \\\\\n",
    "&=\\prod_{k=1}^3 \\pi_k^{\\alpha/K+\\sum_{i=1}^N \\delta(z_i, k)-1}\\\\\n",
    "&\\propto \\text{Dir}\\left(\n",
    "    \\sum_{i=1}^N \\delta(z_i, 1)+\\alpha/K, \n",
    "    \\sum_{i=1}^N \\delta(z_i, 2)+\\alpha/K,\n",
    "    \\sum_{i=1}^N \\delta(z_i, 3)+\\alpha/K\n",
    "    \\right)\n",
    "\\end{align}\n",
    "\n",
    "Here are Python functions to sample from the mixture weights given the current `state` and to update the mixture weights in the `state` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mixture_weights(state):\n",
    "    \"\"\"Sample new mixture weights from current state according to \n",
    "    a Dirichlet distribution \n",
    "    \n",
    "    cf Step 2 of Algorithm 2.1 in Sudderth 2006\n",
    "    \"\"\"\n",
    "    ss = state['suffstats']\n",
    "    alpha = [ss[cid].N + state['alpha_'] / state['num_clusters_'] \n",
    "             for cid in state['cluster_ids_']]\n",
    "    return stats.dirichlet(alpha).rvs(size=1).flatten()\n",
    "\n",
    "def update_mixture_weights(state):\n",
    "    \"\"\"Update state with new mixture weights from current state\n",
    "    sampled according to a Dirichlet distribution \n",
    "    \n",
    "    cf Step 2 of Algorithm 2.1 in Sudderth 2006\n",
    "    \"\"\"\n",
    "    state['pi'] = sample_mixture_weights(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Distribution of Cluster Means\n",
    "\n",
    "Finally, we need to compute the conditional distribution for the cluster means.\n",
    "\n",
    "We assume the unknown cluster means are distributed according to a normal distribution with hyperparameter mean $\\lambda_1$ and variance $\\lambda_2^2$. The final step in this derivation comes from the normal-normal conjugacy. For more information see [section 2.3 of this](http://www.cs.ubc.ca/~murphyk/Papers/bayesGauss.pdf) and [section 6.2 this](https://web.archive.org/web/20160304125731/http://fisher.osu.edu/~schroeder.9/AMIS900/ech6.pdf).)\n",
    "\n",
    "\\begin{align}\n",
    "p(\\theta_k \\,|\\, \\cdot)&=\n",
    "p(\\theta_k \\,|\\, \n",
    "        \\bf{z}, \\pi,\n",
    "        \\theta_{\\neg k},\n",
    "        \\sigma, \\bf x, \\lambda_1, \\lambda_2\n",
    "        ) \\\\\n",
    "&\\propto p(\\left\\{x_i \\,|\\, z_i=k\\right\\} \\,|\\, \\bf{z}, \\pi,\n",
    "        \\theta_1, \\theta_2, \\theta_3,\n",
    "        \\sigma, \\lambda_1, \\lambda_2) \\cdot\\\\\n",
    "    &\\phantom{==}p(\\theta_k \\,|\\, \\bf{z}, \\pi,\n",
    "        \\theta_{\\neg k},\n",
    "        \\sigma, \\lambda_1, \\lambda_2)\\\\\n",
    "&\\propto p(\\left\\{x_i \\,|\\, z_i=k\\right\\} \\,|\\, \\mathbf{z},\n",
    "        \\theta_k, \\sigma)\n",
    "        p(\\theta_k \\,|\\,  \\lambda_1, \\lambda_2)\\\\\n",
    "&= \\mathcal{N}(\\theta_k \\,|\\, \\mu_n, \\sigma_n)\\\\\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "$$ \\sigma_n^2 = \\frac{1}{\n",
    "    \\frac{1}{\\lambda_2^2} + \\frac{N_k}{\\sigma^2}\n",
    "    } $$\n",
    "    \n",
    "and \n",
    "\n",
    "$$\\mu_n = \\sigma_n^2 \n",
    "    \\left(\n",
    "        \\frac{\\lambda_1}{\\lambda_2^2} + \n",
    "        \\frac{n\\bar{x_k}}{\\sigma^2}\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "Here is the code for sampling those means and for updating our state accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_cluster_mean(cluster_id, state):\n",
    "    cluster_var = state['cluster_variance_']\n",
    "    hp_mean = state['hyperparameters_']['mean']\n",
    "    hp_var = state['hyperparameters_']['variance']\n",
    "    ss = state['suffstats'][cluster_id]\n",
    "    \n",
    "    numerator = hp_mean / hp_var + ss.theta * ss.N / cluster_var\n",
    "    denominator = (1.0 / hp_var + ss.N / cluster_var)\n",
    "    posterior_mu = numerator / denominator\n",
    "    posterior_var = 1.0 / denominator\n",
    "    \n",
    "    return stats.norm(posterior_mu, np.sqrt(posterior_var)).rvs()\n",
    "\n",
    "\n",
    "def update_cluster_means(state):\n",
    "    state['cluster_means'] = [sample_cluster_mean(cid, state)\n",
    "                              for cid in state['cluster_ids_']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing each of these three updates in sequence makes a complete _Gibbs step_ for our mixture model. Here is a function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gibbs_step(state):\n",
    "    update_assignment(state)\n",
    "    update_mixture_weights(state)\n",
    "    update_cluster_means(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we assigned each data point to a random cluster. We can see this by plotting a histogram of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_clusters(state):\n",
    "    gby = pd.DataFrame({\n",
    "            'data': state['data_'], \n",
    "            'assignment': state['assignment']}\n",
    "        ).groupby(by='assignment')['data']\n",
    "    hist_data = [gby.get_group(cid).tolist() \n",
    "                 for cid in gby.groups.keys()]\n",
    "    plt.hist(hist_data, \n",
    "             bins=20,\n",
    "             histtype='stepfilled', alpha=.2 )\n",
    "    \n",
    "plot_clusters(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time we run `gibbs_step`, our `state` is updated with newly sampled assignments. Look what happens to our histogram after 5 steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    gibbs_step(state)\n",
    "plot_clusters(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suddenly, we are seeing clusters that appear very similar to what we would intuitively expect: three Gaussian clusters.\n",
    "\n",
    "Another way to see the progress made by the Gibbs sampler is to plot the change in the model's log-likelihood after each step. The log likehlihood is given by:\n",
    "\n",
    "$$\n",
    "\\log p(\\mathbf{x} \\,|\\, \\pi, \\theta_1, \\theta_2, \\theta_3)\n",
    "\\propto \\sum_x \\log \\left(\n",
    "    \\sum_{k=1}^3 \\pi_k \\exp \n",
    "    \\left\\{ \n",
    "        -(x-\\theta_k)^2 / (2\\sigma^2)\n",
    "    \\right\\}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "We can define this as a function of our `state` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(state):\n",
    "    \"\"\"Data log-likeliehood\n",
    "    \n",
    "    Equation 2.153 in Sudderth\n",
    "    \"\"\"\n",
    "    \n",
    "    ll = 0 \n",
    "    for x in state['data_']:\n",
    "        pi = state['pi']\n",
    "        mean = state['cluster_means']\n",
    "        sd = np.sqrt(state['cluster_variance_'])\n",
    "        ll += np.log(np.dot(pi, stats.norm(mean, sd).pdf(x)))\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = initial_state()\n",
    "ll = [log_likelihood(state)]\n",
    "for _ in range(20):\n",
    "    gibbs_step(state)\n",
    "    ll.append(log_likelihood(state))\n",
    "pd.Series(ll).plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See that the log likelihood improves with iterations of the Gibbs sampler. This is what we should expect: the Gibbs sampler finds state configurations that make the data we have seem \"likely\". However, the likelihood isn't strictly monotonic: it jitters up and down. Though it behaves similarly, the Gibbs sampler isn't optimizing the likelihood function. In its steady state, it is sampling from the posterior distribution. The `state` after each step of the Gibbs sampler is a sample from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(ll).plot(ylim=[-150, -100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[In another post](/collapsed-gibbs/), I show how we can \"collapse\" the Gibbs sampler and sampling the assignment parameter without sampling the $\\pi$ and $\\theta$ values. This collapsed sampler can also be extended to the model with a Dirichet process prior that allows the number of clusters to be a parameter fit by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Notation Helper\n",
    "\n",
    "* $N_k$, `state['suffstat'][k].N`: Number of points in cluster $k$.\n",
    "\n",
    "* $\\theta_k$, `state['suffstat'][k].theta`: Mean of cluster $k$.\n",
    "* $\\lambda_1$, `state['hyperparameters_']['mean']`: Mean of prior distribution over cluster means.\n",
    "* $\\lambda_2^2$, `state['hyperparameters_']['variance']` Variance of prior distribution over cluster means.\n",
    "* $\\sigma^2$, `state[cluster_variance_]`: Known, fixed variance of clusters. \n",
    "\n",
    "The superscript $(t)$ on $\\theta_k$, $pi_k$, and $z_i$ indicates the value of that variable at step $t$ of the Gibbs sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state['pi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nikola": {
   "slug": "mixture-model",
   "title": "Fitting a Mixture Model with Gibbs Sampling"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
