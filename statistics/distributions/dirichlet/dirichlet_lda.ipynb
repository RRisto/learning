{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "625454f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\risto\\Anaconda3\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\risto\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "C:\\Users\\risto\\Anaconda3\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import dirichlet, poisson\n",
    "from numpy import round\n",
    "from collections import defaultdict\n",
    "from random import choice as stl_choice\n",
    "from collections import Counter\n",
    "from scipy.stats import beta\n",
    "from numpy.random import choice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643fc691",
   "metadata": {},
   "source": [
    "source: https://dp.tdhopper.com/nonparametric-lda/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399af162",
   "metadata": {},
   "source": [
    "## Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b74744",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = ['see', 'spot', 'run']\n",
    "num_terms = len(vocabulary)\n",
    "num_topics = 2 # K\n",
    "num_documents = 5 # M\n",
    "mean_document_length = 5 # xi\n",
    "term_dirichlet_parameter = 1 # beta\n",
    "topic_dirichlet_parameter = 1 # alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af3e73",
   "metadata": {},
   "source": [
    "## init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de2cd2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.41002688 0.01880712 0.571166  ]\n",
      " [0.37652229 0.35930123 0.26417647]]\n"
     ]
    }
   ],
   "source": [
    "term_dirichlet_vector = num_terms * [term_dirichlet_parameter]\n",
    "term_distributions = dirichlet(term_dirichlet_vector, 2).rvs(size=num_topics)\n",
    "print(term_distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91049bfe",
   "metadata": {},
   "source": [
    "Each document corresponds to a categorical distribution across this distribution of topics (in this case, a 2-dimensional categorical distribution). This categorical distribution is a distribution of distributions; we could look at it as a Dirichlet process!\n",
    "\n",
    "The base base distribution of our Dirichlet process is a uniform distribution of topics (remember, topics are term distributions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6803df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 5020 topic: [0.38, 0.36, 0.26]\n",
      "count: 4980 topic: [0.41, 0.02, 0.57]\n"
     ]
    }
   ],
   "source": [
    "base_distribution = lambda: stl_choice(term_distributions)\n",
    "# A sample from base_distribution is a distribution over terms\n",
    "# Each of our two topics has equal probability\n",
    "for topic, count in Counter([tuple(base_distribution()) for _ in range(10000)]).most_common():\n",
    "    print(\"count:\", count, \"topic:\", [round(prob, 2) for prob in topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72a15cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.37652229, 0.35930123, 0.26417647])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a3688a",
   "metadata": {},
   "source": [
    "Recall that a sample from a Dirichlet process is a distribution that approximates (but varies from) the base distribution. In this case, a sample from the Dirichlet process will be a distribution over topics that varies from the uniform distribution we provided as a base. If we use the stick-breaking metaphor, we are effectively breaking a stick one time and the size of each portion corresponds to the proportion of a topic in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3977811",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirichletProcessSample():\n",
    "    def __init__(self, base_measure, alpha):\n",
    "        self.base_measure = base_measure\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        self.cache = []\n",
    "        self.weights = []\n",
    "        self.total_stick_used = 0.\n",
    "\n",
    "    def __call__(self):\n",
    "        remaining = 1.0 - self.total_stick_used\n",
    "        i = DirichletProcessSample.roll_die(self.weights + [remaining])\n",
    "        if i is not None and i < len(self.weights) :\n",
    "            return self.cache[i]\n",
    "        else:\n",
    "            stick_piece = beta(1, self.alpha).rvs() * remaining\n",
    "            self.total_stick_used += stick_piece\n",
    "            self.weights.append(stick_piece)\n",
    "            new_value = self.base_measure()\n",
    "            self.cache.append(new_value)\n",
    "            return new_value\n",
    "      \n",
    "    @staticmethod \n",
    "    def roll_die(weights):\n",
    "        if weights:\n",
    "            return choice(range(len(weights)), p=weights)\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230b6fc2",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d845c5b",
   "metadata": {},
   "source": [
    "For each document, we will draw a topic distribution from the Dirichlet process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74985917",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distribution = DirichletProcessSample(base_measure=base_distribution, \n",
    "                                            alpha=topic_dirichlet_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deb2ec",
   "metadata": {},
   "source": [
    "A sample from this topic distribution is a distribution over terms. However, unlike our base distribution which returns each term distribution with equal probability, the topics will be unevenly weighted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22535f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: 6838 topic: [0.38, 0.36, 0.26]\n",
      "count: 3162 topic: [0.41, 0.02, 0.57]\n"
     ]
    }
   ],
   "source": [
    "for topic, count in Counter([tuple(topic_distribution()) for _ in range(10000)]).most_common():\n",
    "    print(\"count:\", count, \"topic:\", [round(prob, 2) for prob in topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da670de",
   "metadata": {},
   "source": [
    "To generate each word in the document, we draw a sample topic from the topic distribution, and then a term from the term distribution (topic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d53c7d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_index = defaultdict(list)\n",
    "documents = defaultdict(list)\n",
    "\n",
    "for doc in range(num_documents):\n",
    "    topic_distribution_rvs = DirichletProcessSample(base_measure=base_distribution, \n",
    "                                                    alpha=topic_dirichlet_parameter)\n",
    "    document_length = poisson(mean_document_length).rvs()\n",
    "    for word in range(document_length):\n",
    "        topic_distribution = topic_distribution_rvs()\n",
    "        topic_index[doc].append(tuple(topic_distribution))\n",
    "        documents[doc].append(choice(vocabulary, p=topic_distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b080963d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "287fb924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['see', 'spot', 'run', 'spot', 'see']\n",
      "['run', 'run', 'run', 'spot', 'see', 'run']\n",
      "['spot', 'run', 'run', 'spot', 'run', 'see']\n",
      "['run', 'see', 'see', 'see', 'see']\n",
      "['run', 'see', 'see', 'spot', 'spot']\n"
     ]
    }
   ],
   "source": [
    "for doc in documents.values():\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8642e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: 0\n",
      "      count: 5 topic: [0.38, 0.36, 0.26]\n",
      "Doc: 1\n",
      "      count: 6 topic: [0.41, 0.02, 0.57]\n",
      "Doc: 2\n",
      "      count: 3 topic: [0.38, 0.36, 0.26]\n",
      "      count: 3 topic: [0.41, 0.02, 0.57]\n",
      "Doc: 3\n",
      "      count: 5 topic: [0.41, 0.02, 0.57]\n",
      "Doc: 4\n",
      "      count: 3 topic: [0.38, 0.36, 0.26]\n",
      "      count: 2 topic: [0.41, 0.02, 0.57]\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(Counter(term_dist).most_common() for term_dist in topic_index.values()):\n",
    "    print(\"Doc:\", i)\n",
    "    for topic, count in doc:\n",
    "        print(5*\" \", \"count:\", count, \"topic:\", [round(prob, 2) for prob in topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6cc6ce",
   "metadata": {},
   "source": [
    "To recap: for each document we draw a sample from a Dirichlet Process. The base distribution for the Dirichlet process is a categorical distribution over term distributions; we can think of the base distribution as an n-sided die where n is the number of topics and each side of the die is a distribution over terms for that topic. By sampling from the Dirichlet process, we are effectively reweighting the sides of the die (changing the distribution of the topics).\n",
    "\n",
    "For each word in the document, we draw a sample (a term distribution) from the distribution (over term distributions) sampled from the Dirichlet process (with a distribution over term distributions as its base measure). Each term distribution uniquely identifies the topic for the word. We can sample from this term distribution to get the word.\n",
    "\n",
    "Given this formulation, we might ask if we can roll an infinite sided die to draw from an unbounded number of topics (term distributions). We can do exactly this with a Hierarchical Dirichlet process. Instead of the base distribution of our Dirichlet process being a finite distribution over topics (term distributions) we will instead make it an infinite Distribution over topics (term distributions) by using yet another Dirichlet process! This base Dirichlet process will have as its base distribution a Dirichlet distribution over terms.\n",
    "\n",
    "We will again draw a sample from a Dirichlet Process for each document. The base distribution for the Dirichlet process is itself a Dirichlet process whose base distribution is a Dirichlet distribution over terms. (Try saying that 5-times fast.) We can think of this as a countably infinite die each side of the die is a distribution over terms for that topic. The sample we draw is a topic (distribution over terms).\n",
    "\n",
    "For each word in the document, we will draw a sample (a term distribution) from the distribution (over term distributions) sampled from the Dirichlet process (with a distribution over term distributions as its base measure). Each term distribution uniquely identifies the topic for the word. We can sample from this term distribution to get the word.\n",
    "\n",
    "These last few paragraphs are confusing! Let's illustrate with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edda56ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_dirichlet_vector = num_terms * [term_dirichlet_parameter]\n",
    "base_distribution = lambda: dirichlet(term_dirichlet_vector).rvs(size=1)[0]\n",
    "\n",
    "base_dp_parameter = 10\n",
    "base_dp = DirichletProcessSample(base_distribution, alpha=base_dp_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8e21c4",
   "metadata": {},
   "source": [
    "This sample from the base Dirichlet process is our infinite sided die. It is a probability distribution over a countable infinite number of topics.\n",
    "\n",
    "The fact that our die is countably infinite is important. The sampler base_distribution draws topics (term-distributions) from an uncountable set. If we used this as the base distribution of the Dirichlet process below each document would be constructed from a completely unique set of topics. By feeding base_distribution into a Dirichlet Process (stochastic memoizer), we allow the topics to be shared across documents.\n",
    "\n",
    "In other words, base_distribution will never return the same topic twice; however, every topic sampled from base_dp would be sampled an infinite number of times (if we sampled from base_dp forever). At the same time, base_dp will also return an infinite number of topics. In our formulation of the the LDA sampler above, our base distribution only ever returned a finite number of topics (num_topics); there is no num_topics parameter here.\n",
    "\n",
    "Given this setup, we can generate documents from the hierarchical Dirichlet process with an algorithm that is essentially identical to that of the original latent Dirichlet allocation generative sampler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f154c6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_dp_parameter = 10\n",
    "\n",
    "topic_index = defaultdict(list)\n",
    "documents = defaultdict(list)\n",
    "\n",
    "for doc in range(num_documents):\n",
    "    topic_distribution_rvs = DirichletProcessSample(base_measure=base_dp, \n",
    "                                                    alpha=nested_dp_parameter)\n",
    "    document_length = poisson(mean_document_length).rvs()\n",
    "    for word in range(document_length):\n",
    "        topic_distribution = topic_distribution_rvs()\n",
    "        topic_index[doc].append(tuple(topic_distribution))\n",
    "        documents[doc].append(choice(vocabulary, p=topic_distribution))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb7cf91",
   "metadata": {},
   "source": [
    "Here are the documents we generated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7fa7183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'see', 'run', 'see', 'see']\n",
      "['run', 'run', 'spot']\n",
      "['spot', 'spot', 'see', 'see']\n",
      "['spot', 'run', 'spot', 'see']\n",
      "['spot', 'spot', 'run', 'spot', 'spot', 'spot']\n"
     ]
    }
   ],
   "source": [
    "for doc in documents.values():\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f48a96",
   "metadata": {},
   "source": [
    "And here are the latent topics used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "230c3dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: 0\n",
      "      count: 2 topic: [0.19, 0.66, 0.14]\n",
      "      count: 2 topic: [0.75, 0.14, 0.11]\n",
      "      count: 1 topic: [0.19, 0.64, 0.17]\n",
      "Doc: 1\n",
      "      count: 1 topic: [0.22, 0.48, 0.3]\n",
      "      count: 1 topic: [0.75, 0.14, 0.11]\n",
      "      count: 1 topic: [0.19, 0.64, 0.17]\n",
      "Doc: 2\n",
      "      count: 2 topic: [0.68, 0.25, 0.07]\n",
      "      count: 1 topic: [0.02, 0.8, 0.18]\n",
      "      count: 1 topic: [0.21, 0.31, 0.48]\n",
      "Doc: 3\n",
      "      count: 2 topic: [0.19, 0.64, 0.17]\n",
      "      count: 1 topic: [0.02, 0.8, 0.18]\n",
      "      count: 1 topic: [0.75, 0.14, 0.11]\n",
      "Doc: 4\n",
      "      count: 2 topic: [0.02, 0.8, 0.18]\n",
      "      count: 2 topic: [0.19, 0.64, 0.17]\n",
      "      count: 1 topic: [0.56, 0.4, 0.05]\n",
      "      count: 1 topic: [0.01, 0.69, 0.3]\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(Counter(term_dist).most_common() for term_dist in topic_index.values()):\n",
    "    print(\"Doc:\", i)\n",
    "    for topic, count in doc:\n",
    "        print(5*\" \", \"count:\", count, \"topic:\", [round(prob, 2) for prob in topic])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d94298",
   "metadata": {},
   "source": [
    "Our documents were generated by an unspecified number of topics, and yet the topics were shared across the 5 documents. This is the power of the hierarchical Dirichlet process!\n",
    "\n",
    "This non-parametric formulation of Latent Dirichlet Allocation was first published by Yee Whye Teh et al.\n",
    "\n",
    "Unfortunately, forward sampling is the easy part. Fitting the model on data requires complex MCMC or variational inference. There are a limited number of implementations of HDP-LDA available, and none of them are great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba069b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
