{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "slugify() got an unexpected keyword argument 'separator'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-285cbc5e01b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# from minhash.dedup import Dedup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdedup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDedup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdedup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_duplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is a test'\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# returns False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdedup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_duplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is a test'\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# returns True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdedup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_duplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'This is a test2'\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m# returns True most of the time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-754e74ec897a>\u001b[0m in \u001b[0;36mis_duplicate\u001b[0;34m(self, text, dt)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleanup_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mfootprint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minhash\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhasher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhashes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfootprint\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_minhash_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0minc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'duplicates'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-812bf5bb3dbc>\u001b[0m in \u001b[0;36mhashes\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mreturns\u001b[0m \u001b[0mall\u001b[0m \u001b[0mhashes\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0marray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \"\"\"\n\u001b[0;32m--> 158\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m         return [\n\u001b[1;32m    160\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhash_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-812bf5bb3dbc>\u001b[0m in \u001b[0;36mfeatures\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mreturns\u001b[0m \u001b[0mset\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \"\"\"\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gram_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-812bf5bb3dbc>\u001b[0m in \u001b[0;36mclean\u001b[0;34m(self, s)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_re_concat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\1_'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Slugify, and return\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mslugify\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslugify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseparator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mensure_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: slugify() got an unexpected keyword argument 'separator'"
     ]
    }
   ],
   "source": [
    "# from minhash.dedup import Dedup \n",
    "dedup = Dedup()\n",
    "res = dedup.is_duplicate('This is a test')      # returns False\n",
    "res = dedup.is_duplicate('This is a test')      # returns True\n",
    "res = dedup.is_duplicate('This is a test2')     # returns True most of the time\n",
    "res = dedup.is_duplicate('Something different') # returns False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class MinHash(object):\n",
    "\n",
    "    def __init__(self, hasher=TextHasher()):\n",
    "        self.hasher = hasher\n",
    "\n",
    "    def similarity(self, a, b):\n",
    "        \"\"\"\n",
    "        MinHash similarity\n",
    "        a: string\n",
    "        b: string\n",
    "        returns similarity as float between 0 and 1\n",
    "        \"\"\"\n",
    "        hashesA = self.hasher.hashes(a)\n",
    "        hashesB = self.hasher.hashes(b)\n",
    "        similar = 0\n",
    "        for i in range(0, self.hasher.number_of_signatures):\n",
    "            if hashesA[i] == hashesB[i]:\n",
    "                similar += 1\n",
    "        return pow(\n",
    "            float(similar) / self.hasher.number_of_signatures,\n",
    "            1.0 / self.hasher.signature_length\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Jaccard(object):\n",
    "\n",
    "    def __init__(self, hasher=TextHasher()):\n",
    "        self.hasher = hasher\n",
    "\n",
    "    def similarity(self, a, b):\n",
    "        \"\"\"\n",
    "        Jaccard similarity\n",
    "        a: string\n",
    "        b: string\n",
    "        returns similarity as float between 0 and 1\n",
    "        \"\"\"\n",
    "        setA = self.hasher.features(a)\n",
    "        setB = self.hasher.features(b)\n",
    "        union = setA | setB\n",
    "        intersection = setA & setB\n",
    "        return float(len(intersection)) / len(union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "# from .minhash import MinHash\n",
    "from datetime import timedelta, datetime\n",
    "import logging\n",
    "import json\n",
    "\n",
    "\n",
    "def inc(d, k):\n",
    "    if k not in d:\n",
    "        d[k] = 0\n",
    "    d[k] = d[k] + 1\n",
    "\n",
    "\n",
    "class Dedup:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        minhash=MinHash(),\n",
    "        max_age=timedelta(hours=1), \n",
    "        cleanup_interval=timedelta(minutes=1)\n",
    "    ):\n",
    "        self._max_age = max_age\n",
    "        self._cleanup_interval = cleanup_interval\n",
    "\n",
    "        self._minhash_dict = {}\n",
    "        self._last_cleanup = datetime.utcnow() - cleanup_interval\n",
    "        self._minhash = minhash\n",
    "\n",
    "        self.cleanup()\n",
    "        self._stats = {}\n",
    "\n",
    "        logging.debug('Number of unique hashes: %s' % len(self._minhash_dict))\n",
    "\n",
    "    def load(self):\n",
    "        if os.path.isfile(self._filename):\n",
    "            with open(filename, 'r') as f:\n",
    "                self._minhash_dict = json.load(f)\n",
    "\n",
    "    def save(self):\n",
    "        with open(self._filename, 'w') as f:\n",
    "            json.dump(self._minhash_dict, f)\n",
    "\n",
    "    def is_duplicate(self, text, dt=datetime.utcnow()):\n",
    "        self.cleanup_if_needed()\n",
    "\n",
    "        footprint = tuple(self._minhash.hasher.hashes(text))\n",
    "        if footprint in self._minhash_dict:\n",
    "            inc(self._stats, 'duplicates')\n",
    "            return True\n",
    "\n",
    "        self._minhash_dict[footprint] = dt\n",
    "        return False\n",
    "\n",
    "    def get_similars(self, text, dt):\n",
    "        self.cleanup_if_needed()\n",
    "        ret = []\n",
    "        footprint = tuple(self._minhash.hashes(text))\n",
    "        if footprint in self._minhash_dict:\n",
    "            inc(self._stats, 'duplicates')\n",
    "            ret.append(self._minhash_dict[footprint])\n",
    "\n",
    "        self._minhash_dict[footprint] = dt\n",
    "        return []\n",
    "\n",
    "    def cleanup(self):\n",
    "        min_dt = datetime.utcnow() - self._max_age\n",
    "        for footprint, dt in self._minhash_dict.items():\n",
    "            if dt < min_dt:\n",
    "                del self._minhash_dict[footprint]\n",
    "\n",
    "    def cleanup_if_needed(self):\n",
    "        if datetime.utcnow() > self._last_cleanup + self._cleanup_interval:\n",
    "            self.cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import slugify\n",
    "import re\n",
    "\n",
    "from random import shuffle\n",
    "from math import pow\n",
    "\n",
    "_re_concat = re.compile(r'(\\w+)(-)(?=\\w+)')\n",
    "_re_ellipsis = re.compile(r'#*\\w+(\\.{3}|\\xe2\\x80\\xa6)')\n",
    "_re_http = re.compile(r'http\\S*')\n",
    "_re_mention = re.compile(r'@\\w+')\n",
    "_re_word = re.compile(r'\\w+')\n",
    "\n",
    "# Overwrite slugify regexp to allow for #, -, and _\n",
    "slugify.REPLACE2_REXP = re.compile(r'[^-#_a-z0-9]+')\n",
    "\n",
    "\n",
    "class TextHasher(object):\n",
    "\n",
    "    def __init__(\n",
    "        self, n_gram_length=3, signature_length=2, number_of_signatures=1,\n",
    "        alphabet='abcdefghijklmnopqrstuvwxyz0123456789#_ '\n",
    "    ):\n",
    "        self.n_gram_length = n_gram_length\n",
    "        self.signature_length = signature_length\n",
    "        self.number_of_signatures = number_of_signatures\n",
    "        self.alphabet = alphabet\n",
    "        self.build_feature_space()\n",
    "        self.build_hash_dicts()\n",
    "\n",
    "    def set_charachter_alphabet(self, chars):\n",
    "        \"\"\"\n",
    "        Set the character alphabet\n",
    "        \"\"\"\n",
    "        self.alphabet = chars\n",
    "        self.build_feature_space()\n",
    "        self.build_hash_dicts()\n",
    "\n",
    "    def build_feature_space(self):\n",
    "        \"\"\"\n",
    "        Build the feature space, a list of all possible features.\n",
    "        \"\"\"\n",
    "        self.feature_space = [\n",
    "            ''.join(tup) \n",
    "            for tup\n",
    "            in itertools.product(self.alphabet, repeat=self.n_gram_length)\n",
    "        ]\n",
    "\n",
    "    def build_hash_dicts(self):\n",
    "        \"\"\"\n",
    "        Build the hash dictionaries.\n",
    "        For each signature a different ordering of the features is required.\n",
    "        \"\"\"\n",
    "        self.hash_dicts = []\n",
    "        for i in range(0, self.number_of_signatures):\n",
    "            self.hash_dicts.append({self.feature_space[idx]: idx\n",
    "                                   for idx in range(len(self.feature_space))})\n",
    "            shuffle(self.feature_space)\n",
    "\n",
    "    def slide(self, s, w):\n",
    "        \"\"\"\n",
    "        slide over string\n",
    "        s : Input string\n",
    "        w : Feature length\n",
    "        returns array with all fixed length strings found\n",
    "        \"\"\"\n",
    "        if len(s) < w:\n",
    "            raise Exception('String \"%s\" is shorter than %s' % (s, w))\n",
    "        return [s[i: i + w] for i in range(len(s) - w + 1)]\n",
    "\n",
    "    def clean(self, s):\n",
    "        \"\"\"\n",
    "        Clean up string\n",
    "        \"\"\"\n",
    "        # Remove ellipsis, encode to utf-8, convert to lowercase\n",
    "#         s = _re_ellipsis.sub(lambda x: '', s.encode('utf-8').lower())\n",
    "        s = _re_ellipsis.sub(lambda x: '', s.lower())\n",
    "        # Remove urls\n",
    "        s = _re_http.sub(lambda x: '', s)\n",
    "        # Remove user mentions (i.e., @)\n",
    "        s = _re_mention.sub(lambda x: '', s)\n",
    "        # Replace concatenation dash with underscore\n",
    "        s = _re_concat.sub(r'\\1_', s)\n",
    "        # Slugify, and return\n",
    "#         return slugify.slugify(str(s), separator=' ')\n",
    "        return slugify.slugify(str(s))\n",
    "\n",
    "    def ensure_length(self, s, l):\n",
    "        \"\"\"\n",
    "        make sure string has a length of size l\n",
    "        s: string\n",
    "        l: feature length\n",
    "        returns string\n",
    "        \"\"\"\n",
    "        return s + ''.join(' ' for i in range(l - len(s)))\n",
    "\n",
    "    def fill(self, s):\n",
    "        \"\"\"\n",
    "        prepend and append whitespace\n",
    "        s: string\n",
    "        returns longer string\n",
    "        \"\"\"\n",
    "        n = self.n_gram_length - 1\n",
    "        s = ''.join(' ' for i in range(n)) + s + ''.join(' ' for i in range(n))\n",
    "        return s\n",
    "\n",
    "    def features(self, s):\n",
    "        \"\"\"\n",
    "        get features for a string\n",
    "        s: string\n",
    "        n: feature length\n",
    "        returns set with features\n",
    "        \"\"\"\n",
    "        s = self.clean(s)\n",
    "        s = self.fill(s)\n",
    "        s = self.ensure_length(s, self.n_gram_length)\n",
    "        return set(self.slide(s, self.n_gram_length))\n",
    "\n",
    "    def hash_feature_index(self, feature, i):\n",
    "        \"\"\"\n",
    "        get hash index for a feature\n",
    "        features: feature list\n",
    "        i: hashDict index\n",
    "        returns index of feature\n",
    "        \"\"\"\n",
    "        if not feature in self.hash_dicts[i]:\n",
    "            raise Exception(\n",
    "                'Feature %s does not exist in feature space' %\n",
    "                feature)\n",
    "        return self.hash_dicts[i][feature]\n",
    "\n",
    "\n",
    "    def hash_features(self, features, i=0):\n",
    "        \"\"\"\n",
    "        get hash for a feature list\n",
    "        features: feature list\n",
    "        i: hash dict index\n",
    "        returns hash as tuple\n",
    "        \"\"\"\n",
    "        hashes = sorted([self.hash_feature_index(feature, i)\n",
    "                         for feature in features])\n",
    "        return tuple(hashes[:self.signature_length])\n",
    "\n",
    "    def hash(self, s, i=0):\n",
    "        \"\"\"\n",
    "        get hash for a string\n",
    "        s: string\n",
    "        returns hash as tuple\n",
    "        \"\"\"\n",
    "        features = self.features(s)\n",
    "        return self.hash_features(features, i)\n",
    "\n",
    "    def hashes(self, s):\n",
    "        \"\"\"\n",
    "        get hashes for a string\n",
    "        s: string\n",
    "        returns all hashes as array of tuples\n",
    "        \"\"\"\n",
    "        features = self.features(s)\n",
    "        return [\n",
    "            self.hash_features(features, i)\n",
    "            for i in range(0, self.number_of_signatures)\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
