{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleSummarizer:\n",
    "    \"\"\"\n",
    "    A summarizer based on the algorithm found in Classifier4J by Nick Lothan.\n",
    "    In order to summarize a document this algorithm first determines the\n",
    "    frequencies of the words in the document.  It then splits the document\n",
    "    into a series of sentences.  Then it creates a summary by including the\n",
    "    first sentence that includes each of the most frequent words.  Finally\n",
    "    summary's sentences are reordered to reflect that of those in the original\n",
    "    document.\n",
    "    \"\"\"\n",
    "\n",
    "    def reorder_sentences( self, output_sentences, input ):\n",
    "        output_sentences=[x for _,x in sorted(zip(input,output_sentences))]\n",
    "        return output_sentences\n",
    "\n",
    "    def get_summarized(self, input, num_sentences ):\n",
    "        # TODO: allow the caller to specify the tokenizer they want\n",
    "        # TODO: allow the user to specify the sentence tokenizer they want\n",
    "\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "\n",
    "        # get the frequency of each word in the input\n",
    "        base_words = [word.lower() for word in tokenizer.tokenize(input)]\n",
    "        words = [word for word in base_words if word not in stopwords.words()]\n",
    "        word_frequencies = FreqDist(words)\n",
    "        \n",
    "            # now create a set of the most frequent words\n",
    "        most_frequent_words = [pair[0] for pair in list(word_frequencies.items())[:100]]\n",
    "        \n",
    "        # break the input up into sentences.  working_sentences is used\n",
    "        # for the analysis, but actual_sentences is used in the results\n",
    "        # so capitalization will be correct.\n",
    "        \n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        actual_sentences = sent_detector.tokenize(input)\n",
    "        working_sentences = [sentence.lower() for sentence in actual_sentences]\n",
    "\n",
    "        # iterate over the most frequent words, and add the first sentence\n",
    "        # that inclues each word to the result.\n",
    "        output_sentences = []\n",
    "        \n",
    "        for word in most_frequent_words:\n",
    "            for i in range(0, len(working_sentences)):\n",
    "                if (word in working_sentences[i] and actual_sentences[i] not in output_sentences):\n",
    "                    output_sentences.append(actual_sentences[i])\n",
    "                    break\n",
    "                if len(output_sentences) >= num_sentences:\n",
    "                    break\n",
    "            if len(output_sentences) >= num_sentences:\n",
    "                break\n",
    "\n",
    "        output_sentences=[x for _,x in sorted(zip(input,output_sentences))]\n",
    "        return output_sentences\n",
    "\n",
    "\n",
    "    def summarize(self, input, num_sentences):\n",
    "        return \" \".join(self.get_summarized(input, num_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summarize is a package that uses NLTK to create summaries. NLTK is a python library for working human-written text.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SimpleSummarizer()\n",
    "input = \"NLTK is a python library for working human-written text. Summarize is a package that uses NLTK to create summaries.\"\n",
    "ss.summarize(input, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
