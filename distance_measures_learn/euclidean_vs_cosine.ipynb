{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([[6.6, 6.2, 1],\n",
    "              [9.7, 9.9, 2],\n",
    "              [8.0, 8.3, 2],\n",
    "              [6.3, 5.4, 1],\n",
    "              [1.3, 2.7, 0],\n",
    "              [2.3, 3.1, 0],\n",
    "              [6.6, 6.0, 1],\n",
    "              [6.5, 6.4, 1],\n",
    "              [6.3, 5.8, 1],\n",
    "              [9.5, 9.9, 2],\n",
    "              [8.9, 8.9, 2],\n",
    "              [8.7, 9.5, 2],\n",
    "              [2.5, 3.8, 0],\n",
    "              [2.0, 3.1, 0],\n",
    "              [1.3, 1.3, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>weight</th>\n",
       "      <th>length</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.6</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.7</td>\n",
       "      <td>9.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.0</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.3</td>\n",
       "      <td>5.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.3</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.5</td>\n",
       "      <td>6.4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.3</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.5</td>\n",
       "      <td>9.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.9</td>\n",
       "      <td>8.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8.7</td>\n",
       "      <td>9.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.5</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2.0</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.3</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    weight  length  label\n",
       "0      6.6     6.2    1.0\n",
       "1      9.7     9.9    2.0\n",
       "2      8.0     8.3    2.0\n",
       "3      6.3     5.4    1.0\n",
       "4      1.3     2.7    0.0\n",
       "5      2.3     3.1    0.0\n",
       "6      6.6     6.0    1.0\n",
       "7      6.5     6.4    1.0\n",
       "8      6.3     5.8    1.0\n",
       "9      9.5     9.9    2.0\n",
       "10     8.9     8.9    2.0\n",
       "11     8.7     9.5    2.0\n",
       "12     2.5     3.8    0.0\n",
       "13     2.0     3.1    0.0\n",
       "14     1.3     1.3    0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(X, columns=['weight', 'length', 'label'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2bbb297048>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGQSSVHEGgSRV3P8HQKbJSbkdjwQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2bbb2866d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "ax = df[df['label'] == 0].plot.scatter(x='weight', y='length', c='blue', label='young')\n",
    "ax = df[df['label'] == 1].plot.scatter(x='weight', y='length', c='orange', label='mid', ax=ax)\n",
    "ax = df[df['label'] == 2].plot.scatter(x='weight', y='length', c='red', label='adult', ax=ax)\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking our Metric\n",
    "Considering instance #0, #1, and #4 to be our known instances, we assume that we don’t know the label of #14. Plotting this will look as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2bb9234ba8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBYGISOAUBCIigVMQiIgETkEgIhK4/w8dCLT9p16ULgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2bb91cc2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df2 = pd.DataFrame([df.iloc[0], df.iloc[1], df.iloc[4]], columns=['weight', 'length', 'label'])\n",
    "df3 = pd.DataFrame([df.iloc[14]], columns=['weight', 'length', 'label'])\n",
    "\n",
    "ax = df2[df2['label'] == 0].plot.scatter(x='weight', y='length', c='blue', label='young')\n",
    "ax = df2[df2['label'] == 1].plot.scatter(x='weight', y='length', c='orange', label='mid', ax=ax)\n",
    "ax = df2[df2['label'] == 2].plot.scatter(x='weight', y='length', c='red', label='adult', ax=ax)\n",
    "ax = df3.plot.scatter(x='weight', y='length', c='gray', label='?', ax=ax)\n",
    "ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euclidean_distance(x, y):   \n",
    "    return np.sqrt(np.sum((x - y) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x0: [ 6.6  6.2] \n",
      " x1: [ 9.7  9.9] \n",
      " x4: [ 1.3  2.7] \n",
      "x14: [ 1.3  1.3]\n"
     ]
    }
   ],
   "source": [
    "x0 = X[0][:-1]\n",
    "x1 = X[1][:-1]\n",
    "x4 = X[4][:-1]\n",
    "x14 = X[14][:-1]\n",
    "print(\" x0:\", x0, \"\\n x1:\", x1, \"\\n x4:\", x4, \"\\nx14:\", x14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x14 and x0: 7.21803297305 \n",
      " x14 and x1: 12.0216471417 \n",
      " x14 and x4: 1.4\n"
     ]
    }
   ],
   "source": [
    "print(\" x14 and x0:\", euclidean_distance(x14, x0), \"\\n\",\n",
    "      \"x14 and x1:\", euclidean_distance(x14, x1), \"\\n\",\n",
    "      \"x14 and x4:\", euclidean_distance(x14, x4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to euclidean distance, instance #14 is closest to #4. Our 4th instance had the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3,  2.7,  0. ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 = young, which is what we would visually also deem the correct label for this instance.\n",
    "\n",
    "Now let’s see what happens when we use Cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.sqrt(np.dot(x, x)) * np.sqrt(np.dot(y, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " x14 and x0: 0.999512076087 \n",
      " x14 and x1: 0.999947942424 \n",
      " x14 and x4: 0.943858356366\n"
     ]
    }
   ],
   "source": [
    "print(\" x14 and x0:\", cosine_similarity(x14, x0), \"\\n\",\n",
    "      \"x14 and x1:\", cosine_similarity(x14, x1), \"\\n\",\n",
    "      \"x14 and x4:\", cosine_similarity(x14, x4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to cosine similarity, instance #14 is closest to #1. However, our 1st instance had the label:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.7,  9.9,  2. ])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 = adult, which is definitely NOT what we would deem the correct label!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to Use Cosine?\n",
    "Cosine similarity is generally used as a metric for measuring distance when the magnitude of the vectors does not matter. This happens for example when working with text data represented by word counts. We could assume that when a word (e.g. science) occurs more frequent in document 1 than it does in document 2, that document 1 is more related to the topic of science. However, it could also be the case that we are working with documents of uneven lengths (Wikipedia articles for example). Then, science probably occurred more in document 1 just because it was way longer than document 2. Cosine similarity corrects for this.\n",
    "\n",
    "Text data is the most typical example for when to use this metric. However, you might also want to apply cosine similarity for other cases where some properties of the instances make so that the weights might be larger without meaning anything different. Sensor values that were captured in various lengths (in time) between instances could be such an example.\n",
    "\n",
    "## How do Euclidean Distance and Cosine Similarity Relate?\n",
    "Let’s consider two of our vectors, their euclidean distance, as well as their cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors \t [ 6.6  6.2] [ 9.7  9.9] \n",
      "euclidean \t 4.82700735446 \n",
      "cosine \t\t 0.99914133854\n"
     ]
    }
   ],
   "source": [
    "print(\"vectors \\t\", x0, x1, \"\\n\"\n",
    "      \"euclidean \\t\", euclidean_distance(x0, x1), \"\\n\"\n",
    "      \"cosine \\t\\t\", cosine_similarity(x0, x1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine similarity takes a unit length vector to calculate dot products. However, what happens if we do the same for the vectors we’re calculating the euclidian distance for (i.e. normalize them)? For this, we can for example use the L1L1 norm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l1_normalize(v):\n",
    "    norm = np.sum(v)\n",
    "    return v / norm\n",
    "\n",
    "def l2_normalize(v):\n",
    "    norm = np.sqrt(np.sum(np.square(v)))\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We divide the values of our vector by these norms to get a normalized vector.\n",
    "\n",
    "Applying the L1L1 norm to our vectors will make them sum up to 1 respectively, as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.515625  0.484375] [ 0.49489796  0.50510204]\n"
     ]
    }
   ],
   "source": [
    "x0_n = l1_normalize(x0)\n",
    "x1_n = l1_normalize(x1)\n",
    "print(x0_n, x1_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s compare the result we had before against these normalized vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors \t [ 0.515625  0.484375] [ 0.49489796  0.50510204] \n",
      "euclidean \t 0.0293124622303 \n",
      "cosine \t\t 0.99914133854\n"
     ]
    }
   ],
   "source": [
    "print(\"vectors \\t\", x0_n, x1_n, \"\\n\"\n",
    "      \"euclidean \\t\", euclidean_distance(x0_n, x1_n), \"\\n\"\n",
    "      \"cosine \\t\\t\", cosine_similarity(x0_n, x1_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, before, the distance was pretty big, but the cosine similarity very high. Now that we normalized our vectors, it turns out that the distance is now very small. The same pattern occurs when we compare it against vector 4. Unnormalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors \t [ 6.6  6.2] [ 1.3  2.7] \n",
      "euclidean \t 6.35137780328 \n",
      "cosine \t\t 0.933079411589\n"
     ]
    }
   ],
   "source": [
    "print(\"vectors \\t\", x0, x4, \"\\n\"\n",
    "      \"euclidean \\t\", euclidean_distance(x0, x4), \"\\n\"\n",
    "      \"cosine \\t\\t\", cosine_similarity(x0, x4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors \t [ 0.515625  0.484375] [ 0.325  0.675] \n",
      "euclidean \t 0.269584460327 \n",
      "cosine \t\t 0.933079411589\n"
     ]
    }
   ],
   "source": [
    "x4_n = l1_normalize(x4)\n",
    "\n",
    "print(\"vectors \\t\", x0_n, x4_n, \"\\n\"\n",
    "      \"euclidean \\t\", euclidean_distance(x0_n, x4_n), \"\\n\"\n",
    "      \"cosine \\t\\t\", cosine_similarity(x0_n, x4_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that because the cosine similarity is a bit lower between x0 and x4 than it was for x0 and x1, the euclidean distance is now also a bit larger. To take this point home, let’s construct a vector that is almost evenly distant in our euclidean space, but where the cosine similarity is much lower (because the angle is larger):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors \t [ 6.6  6.2] [ 0.1  6. ] \n",
      "euclidean \t 6.50307619516 \n",
      "cosine \t\t 0.696726168728\n"
     ]
    }
   ],
   "source": [
    "x00 = np.array([0.1, 6])\n",
    "\n",
    "print(\"vectors \\t\", x0, x00, \"\\n\"\n",
    "      \"euclidean \\t\", euclidean_distance(x0, x00), \"\\n\"\n",
    "      \"cosine \\t\\t\", cosine_similarity(x0, x00))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we normalize this, we should see the same behaviour from our euclidean distance (i.e. it should be larger than for x0 and x4). As follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors \t [ 0.515625  0.484375] [ 0.01639344  0.98360656] \n",
      "euclidean \t 0.706020039207 \n",
      "cosine \t\t 0.696726168728\n"
     ]
    }
   ],
   "source": [
    "x00_n = l1_normalize(x00)\n",
    "\n",
    "print(\"vectors \\t\", x0_n, x00_n, \"\\n\"\n",
    "      \"euclidean \\t\", euclidean_distance(x0_n, x00_n), \"\\n\"\n",
    "      \"cosine \\t\\t\", cosine_similarity(x0_n, x00_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine in Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So when is cosine handy? Let’s consider four articles from Wikipedia. We use the Wikipedia API to extract them, after which we can access their text with the .content method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "\n",
    "q1 = wikipedia.page('Machine Learning')\n",
    "q2 = wikipedia.page('Artifical Intelligence')\n",
    "q3 = wikipedia.page('Soccer')\n",
    "q4 = wikipedia.page('Tennis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent these by their frequency vectors. Each instance is a document, and each word will be a feature. The feature values will then represent how many times a word occurs in a certain document. So the feature ball, will probably be 0 for both machine learning and AI, but definitely not 0 for soccer and tennis. For this example I’ll use sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X = np.array(cv.fit_transform([q1.content, q2.content, q3.content, q4.content]).todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer by default splits up the text into words using white spaces. We can do the same to see how many words are in each article. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML \t 3850 \n",
      "AI \t 11657 \n",
      "soccer \t 6422 \n",
      "tennis \t 9731\n"
     ]
    }
   ],
   "source": [
    "print(\"ML \\t\", len(q1.content.split()), \"\\n\"\n",
    "      \"AI \\t\", len(q2.content.split()), \"\\n\"\n",
    "      \"soccer \\t\", len(q3.content.split()), \"\\n\"\n",
    "      \"tennis \\t\", len(q4.content.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI is a much larger article than Machine Learning (ML). This would mean that if we do not normalize our vectors, AI will be much further away from ML just because it has many more words. ML will probably be closer to an article with less words. Let’s try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML - AI \t 710.584266643 \n",
      "ML - soccer \t 478.882031402 \n",
      "ML - tennis \t 793.951509854\n"
     ]
    }
   ],
   "source": [
    "print(\"ML - AI \\t\", euclidean_distance(X[0], X[1]), \"\\n\"\n",
    "      \"ML - soccer \\t\", euclidean_distance(X[0], X[2]), \"\\n\"\n",
    "      \"ML - tennis \\t\", euclidean_distance(X[0], X[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see pretty clearly that our prior assumptions have been confirmed. ML seems to be closest to soccer, which doesn’t make a lot of sense intuitively. So, what happens if we look at cosine similairty (thus normalising our vectors)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ML - AI \t 0.886051907186 \n",
      "ML - soccer \t 0.787512523401 \n",
      "ML - tennis \t 0.797782610605\n"
     ]
    }
   ],
   "source": [
    "print(\"ML - AI \\t\", cosine_similarity(X[0], X[1]), \"\\n\"\n",
    "      \"ML - soccer \\t\", cosine_similarity(X[0], X[2]), \"\\n\"\n",
    "      \"ML - tennis \\t\", cosine_similarity(X[0], X[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML is closer to AI! Granted, it still seems pretty close to soccer an tennis judging from these scores, but please note that word frequency is not that great of a representation for texts with such rich content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorize a Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, just for fun, let’s see how this plays out for the following tweet by OpenAI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_tweet = \"New research release: overcoming many of Reinforcement Learning's limitations with Evolution Strategies.\"\n",
    "x = np.array(cv.transform([ml_tweet]).todense())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we represent this tweet as a word vector, and we try to measure the distance between the tweet and our four wikipedia documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet - ML \t 360.449719101 \n",
      "tweet - AI \t 1010.03465287 \n",
      "tweet - soccer \t 708.098863154 \n",
      "tweet - tennis \t 1051.18742382\n"
     ]
    }
   ],
   "source": [
    "print(\"tweet - ML \\t\", euclidean_distance(x[0], X[0]), \"\\n\"\n",
    "      \"tweet - AI \\t\", euclidean_distance(x[0], X[1]), \"\\n\"\n",
    "      \"tweet - soccer \\t\", euclidean_distance(x[0], X[2]), \"\\n\"\n",
    "      \"tweet - tennis \\t\", euclidean_distance(x[0], X[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that worked out pretty well at first glance, it’s closest to ML. However, see how it’s also closer to tennis than AI? There’s so many dimensions that come into play here that it’s hard to say why this is the case. However, tennis being our second smallest document might have something to do with it. Now we’ll do the same for cosine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet - ML \t 0.292298494946 \n",
      "tweet - AI \t 0.210024511291 \n",
      "tweet - soccer \t 0.13381225929 \n",
      "tweet - tennis \t 0.129826142966\n"
     ]
    }
   ],
   "source": [
    "print(\"tweet - ML \\t\", cosine_similarity(x, X[0]), \"\\n\"\n",
    "      \"tweet - AI \\t\", cosine_similarity(x, X[1]), \"\\n\"\n",
    "      \"tweet - soccer \\t\", cosine_similarity(x, X[2]), \"\\n\"\n",
    "      \"tweet - tennis \\t\", cosine_similarity(x, X[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! This seems definitely more in line with our intuitions. So, remember how euclidean distance in this example seemed to slightly relate to the length of the document? Let’s try the same for a soccer tweet, by Manchester United:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "so_tweet = \"#LegendsDownUnder The Reds are out for the warm up at the @nibStadium. Not long now until kick-off in Perth.\"\n",
    "x2 = np.array(cv.transform([so_tweet]).todense())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same trick, different tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet - ML \t 358.429909466 \n",
      "tweet - AI \t 1007.86606253 \n",
      "tweet - soccer \t 705.233294733 \n",
      "tweet - tennis \t 1048.39448682\n"
     ]
    }
   ],
   "source": [
    "print(\"tweet - ML \\t\", euclidean_distance(x2, X[0]), \"\\n\"\n",
    "      \"tweet - AI \\t\", euclidean_distance(x2, X[1]), \"\\n\"\n",
    "      \"tweet - soccer \\t\", euclidean_distance(x2, X[2]), \"\\n\"\n",
    "      \"tweet - tennis \\t\", euclidean_distance(x2, X[3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how awfully similar these distances are to that of our previous tweet, even though there’s very little overlap? Now let’s try the same with cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tweet - ML \t 0.43592467861 \n",
      "tweet - AI \t 0.464170584577 \n",
      "tweet - soccer \t 0.613017025005 \n",
      "tweet - tennis \t 0.596896132902\n"
     ]
    }
   ],
   "source": [
    "print(\"tweet - ML \\t\", cosine_similarity(x2, X[0]), \"\\n\"\n",
    "      \"tweet - AI \\t\", cosine_similarity(x2, X[1]), \"\\n\"\n",
    "      \"tweet - soccer \\t\", cosine_similarity(x2, X[2]), \"\\n\"\n",
    "      \"tweet - tennis \\t\", cosine_similarity(x2, X[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
